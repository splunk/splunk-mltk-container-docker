{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM with Function Calling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example Usage:\n",
    "\n",
    "| makeresults\n",
    "| fit MLTKContainer algo=llm_rag_function_calling prompt=\"What indexes are there in my Splunk\" llm_service=azure_openai _time into app:llm_rag_function_calling as RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 0 - import libraries\n",
    "At stage 0 we define all imports necessary to run our subsequent code depending on various libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "deletable": false,
    "name": "mltkc_import"
   },
   "outputs": [],
   "source": [
    "# this definition exposes all python module imports that should be available in all subsequent commands\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from typing import Dict, List, Any, Optional, Union\n",
    "import pymilvus\n",
    "from pymilvus import (\n",
    "    connections,\n",
    "    utility,\n",
    "    FieldSchema, CollectionSchema, DataType,\n",
    "    Collection,\n",
    ")\n",
    "import llama_index\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Document, StorageContext, ServiceContext\n",
    "from llama_index.vector_stores.milvus import MilvusVectorStore\n",
    "import textwrap\n",
    "from llama_index.core.llms import ChatMessage, MessageRole\n",
    "from llama_index.core import ChatPromptTemplate\n",
    "from llama_index.core.agent import FunctionCallingAgentWorker\n",
    "from typing import Sequence, List\n",
    "from llama_index.core.tools import BaseTool, FunctionTool\n",
    "from llama_index.core.agent import AgentRunner, ReActAgentWorker\n",
    "from pydantic import Field\n",
    "from app.model.llm_utils import create_llm, create_embedding_model\n",
    "import splunklib.client\n",
    "# ...\n",
    "# global constants\n",
    "MODEL_DIRECTORY = \"/srv/app/model/data/\"\n",
    "\n",
    "## Acknowledgement: The example tools are following the Splunk MCP at https://github.com/livehybrid/splunk-mcp\n",
    "\n",
    "def get_splunk_connection() -> splunklib.client.Service:\n",
    "    \"\"\"\n",
    "    Get a connection to the Splunk service.\n",
    "    \n",
    "    Returns:\n",
    "        splunklib.client.Service: Connected Splunk service\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"🔌 Connecting to Splunk\")\n",
    "        \n",
    "        # Connect to Splunk\n",
    "        service = splunklib.client.connect(\n",
    "            host=os.environ[\"splunk_access_host\"],\n",
    "            port=os.environ[\"splunk_access_port\"],\n",
    "            token=os.environ[\"splunk_access_token\"],\n",
    "            scheme=\"https\",\n",
    "            verify=False\n",
    "        )\n",
    "        \n",
    "        print(f\"Connected to Splunk successfully\")\n",
    "        return service\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to connect to Splunk: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def search_splunk(search_query: str, earliest_time: str = \"-24h\", latest_time: str = \"now\", max_results: int = 100) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Execute a Splunk search query and return the results.\n",
    "    \n",
    "    Args:\n",
    "        search_query: The search query to execute\n",
    "        earliest_time: Start time for the search (default: 24 hours ago)\n",
    "        latest_time: End time for the search (default: now)\n",
    "        max_results: Maximum number of results to return (default: 100)\n",
    "        \n",
    "    Returns:\n",
    "        List of search results\n",
    "    \"\"\"\n",
    "    if not search_query:\n",
    "        raise ValueError(\"Search query cannot be empty\")\n",
    "        \n",
    "    try:\n",
    "        service = get_splunk_connection()\n",
    "        print(f\"🔍 Executing search: {search_query}\")\n",
    "        \n",
    "        # Create the search job\n",
    "        kwargs_search = {\n",
    "            \"earliest_time\": earliest_time,\n",
    "            \"latest_time\": latest_time,\n",
    "            \"preview\": False,\n",
    "            \"exec_mode\": \"blocking\"\n",
    "        }\n",
    "        \n",
    "        job = service.jobs.create(search_query, **kwargs_search)\n",
    "        \n",
    "        # Get the results\n",
    "        result_stream = job.results(output_mode='json', count=max_results)\n",
    "        results_data = json.loads(result_stream.read().decode('utf-8'))\n",
    "        \n",
    "        return results_data.get(\"results\", [])\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Search failed: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def list_indexes() -> Dict[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Get a list of all available Splunk indexes.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing list of indexes\n",
    "    \"\"\"\n",
    "    try:\n",
    "        service = get_splunk_connection()\n",
    "        indexes = [index.name for index in service.indexes]\n",
    "        print(f\"📊 Found {len(indexes)} indexes\")\n",
    "        return {\"indexes\": indexes}\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to list indexes: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def get_index_info(index_name: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Get metadata for a specific Splunk index.\n",
    "    \n",
    "    Args:\n",
    "        index_name: Name of the index to get metadata for\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing index metadata\n",
    "    \"\"\"\n",
    "    try:\n",
    "        service = get_splunk_connection()\n",
    "        index = service.indexes[index_name]\n",
    "        \n",
    "        return {\n",
    "            \"name\": index_name,\n",
    "            \"total_event_count\": str(index[\"totalEventCount\"]),\n",
    "            \"current_size\": str(index[\"currentDBSizeMB\"]),\n",
    "            \"max_size\": str(index[\"maxTotalDataSizeMB\"]),\n",
    "            \"min_time\": str(index[\"minTime\"]),\n",
    "            \"max_time\": str(index[\"maxTime\"])\n",
    "        }\n",
    "    except KeyError:\n",
    "        print(f\"❌ Index not found: {index_name}\")\n",
    "        raise ValueError(f\"Index not found: {index_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to get index info: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def list_saved_searches() -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    List all saved searches in Splunk\n",
    "    \n",
    "    Returns:\n",
    "        List of saved searches with their names, descriptions, and search queries\n",
    "    \"\"\"\n",
    "    try:\n",
    "        service = get_splunk_connection()\n",
    "        saved_searches = []\n",
    "        \n",
    "        for saved_search in service.saved_searches:\n",
    "            try:\n",
    "                saved_searches.append({\n",
    "                    \"name\": saved_search.name,\n",
    "                    \"description\": saved_search.description or \"\",\n",
    "                    \"search\": saved_search.search\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Error processing saved search: {str(e)}\")\n",
    "                continue\n",
    "            \n",
    "        return saved_searches\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to list saved searches: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def list_users() -> List[Dict[str, Any]]:\n",
    "    \"\"\"List all Splunk users (requires admin privileges)\"\"\"\n",
    "    try:\n",
    "        service = get_splunk_connection()\n",
    "        print(\"👥 Fetching Splunk users...\")\n",
    "                \n",
    "        users = []\n",
    "        for user in service.users:\n",
    "            try:\n",
    "                if hasattr(user, 'content'):\n",
    "                    # Ensure roles is a list\n",
    "                    roles = user.content.get('roles', [])\n",
    "                    if roles is None:\n",
    "                        roles = []\n",
    "                    elif isinstance(roles, str):\n",
    "                        roles = [roles]\n",
    "                    \n",
    "                    # Ensure capabilities is a list\n",
    "                    capabilities = user.content.get('capabilities', [])\n",
    "                    if capabilities is None:\n",
    "                        capabilities = []\n",
    "                    elif isinstance(capabilities, str):\n",
    "                        capabilities = [capabilities]\n",
    "                    \n",
    "                    user_info = {\n",
    "                        \"username\": user.name,\n",
    "                        \"real_name\": user.content.get('realname', \"N/A\") or \"N/A\",\n",
    "                        \"email\": user.content.get('email', \"N/A\") or \"N/A\",\n",
    "                        \"roles\": roles,\n",
    "                        \"capabilities\": capabilities,\n",
    "                        \"default_app\": user.content.get('defaultApp', \"search\") or \"search\",\n",
    "                        \"type\": user.content.get('type', \"user\") or \"user\"\n",
    "                    }\n",
    "                    users.append(user_info)\n",
    "                    print(f\"✅ Successfully processed user: {user.name}\")\n",
    "                else:\n",
    "                    # Handle users without content\n",
    "                    user_info = {\n",
    "                        \"username\": user.name,\n",
    "                        \"real_name\": \"N/A\",\n",
    "                        \"email\": \"N/A\",\n",
    "                        \"roles\": [],\n",
    "                        \"capabilities\": [],\n",
    "                        \"default_app\": \"search\",\n",
    "                        \"type\": \"user\"\n",
    "                    }\n",
    "                    users.append(user_info)\n",
    "                    print(f\"⚠️ User {user.name} has no content, using default values\")\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Error processing user {user.name}: {str(e)}\")\n",
    "                continue\n",
    "            \n",
    "        print(f\"✅ Found {len(users)} users\")\n",
    "        return users\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error listing users: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def get_indexes_and_sourcetypes() -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Get a list of all indexes and their sourcetypes.\n",
    "    \n",
    "    This endpoint performs a search to gather:\n",
    "    - All available indexes\n",
    "    - All sourcetypes within each index\n",
    "    - Event counts for each sourcetype\n",
    "    - Time range information\n",
    "    \n",
    "    Returns:\n",
    "        Dict[str, Any]: Dictionary containing:\n",
    "            - indexes: List of all accessible indexes\n",
    "            - sourcetypes: Dictionary mapping indexes to their sourcetypes\n",
    "            - metadata: Additional information about the search\n",
    "    \"\"\"\n",
    "    try:\n",
    "        service = get_splunk_connection()\n",
    "        print(\"📊 Fetching indexes and sourcetypes...\")\n",
    "        \n",
    "        # Get list of indexes\n",
    "        indexes = [index.name for index in service.indexes]\n",
    "        print(f\"Found {len(indexes)} indexes\")\n",
    "        \n",
    "        # Search for sourcetypes across all indexes\n",
    "        search_query = \"\"\"\n",
    "        | tstats count WHERE index=* BY index, sourcetype\n",
    "        | stats count BY index, sourcetype\n",
    "        | sort - count\n",
    "        \"\"\"\n",
    "        \n",
    "        kwargs_search = {\n",
    "            \"earliest_time\": \"-24h\",\n",
    "            \"latest_time\": \"now\",\n",
    "            \"preview\": False,\n",
    "            \"exec_mode\": \"blocking\"\n",
    "        }\n",
    "        \n",
    "        print(\"🔍 Executing search for sourcetypes...\")\n",
    "        job = service.jobs.create(search_query, **kwargs_search)\n",
    "        \n",
    "        # Get the results\n",
    "        result_stream = job.results(output_mode='json')\n",
    "        results_data = json.loads(result_stream.read().decode('utf-8'))\n",
    "        \n",
    "        # Process results\n",
    "        sourcetypes_by_index = {}\n",
    "        for result in results_data.get('results', []):\n",
    "            index = result.get('index', '')\n",
    "            sourcetype = result.get('sourcetype', '')\n",
    "            count = result.get('count', '0')\n",
    "            \n",
    "            if index not in sourcetypes_by_index:\n",
    "                sourcetypes_by_index[index] = []\n",
    "            \n",
    "            sourcetypes_by_index[index].append({\n",
    "                'sourcetype': sourcetype,\n",
    "                'count': count\n",
    "            })\n",
    "        \n",
    "        response = {\n",
    "            'indexes': indexes,\n",
    "            'sourcetypes': sourcetypes_by_index,\n",
    "            'metadata': {\n",
    "                'total_indexes': len(indexes),\n",
    "                'total_sourcetypes': sum(len(st) for st in sourcetypes_by_index.values()),\n",
    "                'search_time_range': '24 hours'\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        print(f\"✅ Successfully retrieved indexes and sourcetypes\")\n",
    "        return response\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error getting indexes and sourcetypes: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def health_check() -> Dict[str, Any]:\n",
    "    \"\"\"Get basic Splunk connection information and list available apps\"\"\"\n",
    "    try:\n",
    "        service = get_splunk_connection()\n",
    "        print(\"🏥 Performing health check...\")\n",
    "        \n",
    "        # List available apps\n",
    "        apps = []\n",
    "        for app in service.apps:\n",
    "            try:\n",
    "                app_info = {\n",
    "                    \"name\": app['name'],\n",
    "                    \"label\": app['label'],\n",
    "                    \"version\": app['version']\n",
    "                }\n",
    "                apps.append(app_info)\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Error getting info for app {app['name']}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        response = {\n",
    "            \"status\": \"healthy\",\n",
    "            \"connection\": {\n",
    "                \"host\": os.environ[\"splunk_access_host\"],\n",
    "                \"port\": os.environ[\"splunk_access_port\"],\n",
    "                \"scheme\": \"https\",\n",
    "                \"username\": \"admin\",\n",
    "                \"ssl_verify\": False\n",
    "            },\n",
    "            \"apps_count\": len(apps),\n",
    "            \"apps\": apps\n",
    "        }\n",
    "        \n",
    "        print(f\"✅ Health check successful. Found {len(apps)} apps\")\n",
    "        return response\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Health check failed: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "        \n",
    "search_splunk_tool = FunctionTool.from_defaults(fn=search_splunk)\n",
    "list_indexes_tool = FunctionTool.from_defaults(fn=list_indexes)\n",
    "get_index_info_tool = FunctionTool.from_defaults(fn=get_index_info)\n",
    "list_saved_searches_tool = FunctionTool.from_defaults(fn=list_saved_searches)\n",
    "list_users_tool = FunctionTool.from_defaults(fn=list_users)\n",
    "get_indexes_and_sourcetypes_tool = FunctionTool.from_defaults(fn=get_indexes_and_sourcetypes)\n",
    "health_check_tool = FunctionTool.from_defaults(fn=health_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some logging settings \n",
    "import logging\n",
    "import sys\n",
    "import llama_index.core\n",
    "from llama_index.core.callbacks import (\n",
    "    CallbackManager,\n",
    "    LlamaDebugHandler,\n",
    "    CBEventType,\n",
    ")\n",
    "\n",
    "llama_index.core.set_global_handler(\"simple\")\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "llama_debug = LlamaDebugHandler(print_trace_on_end=True)\n",
    "callback_manager = CallbackManager([llama_debug])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "name": "mltkc_stage"
   },
   "outputs": [],
   "source": [
    "# this cell is not executed from MLTK and should only be used for staging data into the notebook environment\n",
    "def stage(name):\n",
    "    with open(\"data/\"+name+\".csv\", 'r') as f:\n",
    "        df = pd.read_csv(f)\n",
    "    with open(\"data/\"+name+\".json\", 'r') as f:\n",
    "        param = json.load(f)\n",
    "    return df, param"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 2 - create and initialize a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "name": "mltkc_init"
   },
   "outputs": [],
   "source": [
    "# initialize your model\n",
    "# available inputs: data and parameters\n",
    "# returns the model object which will be used as a reference to call fit, apply and summary subsequently\n",
    "def init(df,param):\n",
    "    model = {}\n",
    "    model['hyperparameter'] = 42.0\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 3 - fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "name": "mltkc_fit"
   },
   "outputs": [],
   "source": [
    "# train your model\n",
    "# returns a fit info json object and may modify the model object\n",
    "def fit(model,df,param):\n",
    "    info = {\"message\": \"model trained\"}\n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'message': 'model trained'}\n"
     ]
    }
   ],
   "source": [
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",
    "print(fit(model,df,param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 4 - apply the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "name": "mltkc_apply"
   },
   "outputs": [],
   "source": [
    "def apply(model,df,param):\n",
    "    try:\n",
    "        query = param['options']['params']['prompt'].strip('\\\"')\n",
    "    except:\n",
    "        result = pd.DataFrame({'Message': \"ERROR: Please input a parameter \\'prompt\\'.\"})\n",
    "        return result\n",
    "        \n",
    "    tool_list = [\n",
    "        search_splunk_tool,\n",
    "        list_indexes_tool,\n",
    "        get_index_info_tool,\n",
    "        list_saved_searches_tool,\n",
    "        list_users_tool,\n",
    "        get_indexes_and_sourcetypes_tool,\n",
    "        health_check_tool\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        service = param['options']['params']['llm_service'].strip(\"\\\"\")\n",
    "        print(f\"Using {service} LLM service.\")\n",
    "    except:\n",
    "        service = \"ollama\"\n",
    "        print(\"Using default Ollama LLM service.\")\n",
    "\n",
    "    try:\n",
    "        model_name = param['options']['params']['model_name'].strip(\"\\\"\")\n",
    "    except:\n",
    "        model_name = None\n",
    "        print(\"No model name specified\")\n",
    "        \n",
    "    llm, m = create_llm(service=service, model=model_name)\n",
    "\n",
    "    if llm is None:\n",
    "        cols={'Message': [m]}\n",
    "        returns=pd.DataFrame(data=cols)\n",
    "        return returns\n",
    "\n",
    "    \n",
    "    worker = ReActAgentWorker.from_tools(tool_list, llm=llm)\n",
    "    agent = AgentRunner(worker)     \n",
    "    response = agent.chat(query)\n",
    "    \n",
    "    cols = {\"Response\": [response.response]}\n",
    "    for i in range(len(response.sources)):\n",
    "        if response.sources[i].tool_name != \"unknown\":\n",
    "            cols[response.sources[i].tool_name] = [response.sources[i].content]\n",
    "    result = pd.DataFrame(data=cols)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",
    "print(apply(model, df, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 5 - save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "name": "mltkc_save"
   },
   "outputs": [],
   "source": [
    "# save model to name in expected convention \"<algo_name>_<model_name>\"\n",
    "def save(model,name):\n",
    "    with open(MODEL_DIRECTORY + name + \".json\", 'w') as file:\n",
    "        json.dump(model, file)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 6 - load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "name": "mltkc_load"
   },
   "outputs": [],
   "source": [
    "# load model from name in expected convention \"<algo_name>_<model_name>\"\n",
    "def load(name):\n",
    "    model = {}\n",
    "    with open(MODEL_DIRECTORY + name + \".json\", 'r') as file:\n",
    "        model = json.load(file)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 7 - provide a summary of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "name": "mltkc_summary"
   },
   "outputs": [],
   "source": [
    "# return a model summary\n",
    "def summary(model=None):\n",
    "    returns = {\"version\": {\"numpy\": np.__version__, \"pandas\": pd.__version__} }\n",
    "    return returns\n",
    "\n",
    "def compute(model,df,param):\n",
    "    try:\n",
    "        query = param['options']['params']['prompt'].strip('\\\"')\n",
    "    except:\n",
    "        result = pd.DataFrame({'Message': \"ERROR: Please input a parameter \\'prompt\\'.\"})\n",
    "        return result\n",
    "    # Case of only two functions. Please customize for your own functions\n",
    "    try:\n",
    "        func1 = int(param['options']['params']['func1'])\n",
    "        func2 = int(param['options']['params']['func2'])\n",
    "    except:\n",
    "        func1 = 1\n",
    "        func2 = 1\n",
    "        \n",
    "    tool_list = []\n",
    "\n",
    "    if func1:\n",
    "        tool_list.append(search_splunk_tool)\n",
    "    if func2:\n",
    "        tool_list.append(search_record_from_vector_db_tool)\n",
    "\n",
    "    try:\n",
    "        service = param['options']['params']['llm_service'].strip(\"\\\"\")\n",
    "        print(f\"Using {service} LLM service.\")\n",
    "    except:\n",
    "        service = \"ollama\"\n",
    "        print(\"Using default Ollama LLM service.\")\n",
    "\n",
    "    try:\n",
    "        model_name = param['options']['params']['model_name'].strip(\"\\\"\")\n",
    "    except:\n",
    "        model_name = None\n",
    "        print(\"No model name specified\")\n",
    "        \n",
    "    llm, m = create_llm(service=service, model=model_name)\n",
    "\n",
    "    if llm is None:\n",
    "        cols={'Message': [m]}\n",
    "        returns=pd.DataFrame(data=cols)\n",
    "        return returns\n",
    "\n",
    "    \n",
    "    worker = ReActAgentWorker.from_tools(tool_list, llm=llm)\n",
    "    agent = AgentRunner(worker)     \n",
    "    response = agent.chat(query)\n",
    "    \n",
    "    cols = {\"Response\": [response.response]}\n",
    "    for i in range(len(response.sources)):\n",
    "        if response.sources[i].tool_name != \"unknown\":\n",
    "            cols[response.sources[i].tool_name] = [response.sources[i].content]\n",
    "    result = pd.DataFrame(data=cols)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After implementing your fit, apply, save and load you can train your model:<br>\n",
    "| makeresults count=10<br>\n",
    "| streamstats c as i<br>\n",
    "| eval s = i%3<br>\n",
    "| eval feature_{s}=0<br>\n",
    "| foreach feature_* [eval &lt;&lt;FIELD&gt;&gt;=random()/pow(2,31)]<br>\n",
    "| fit MLTKContainer algo=barebone s from feature_* into app:barebone_model<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or apply your model:<br>\n",
    "| makeresults count=10<br>\n",
    "| streamstats c as i<br>\n",
    "| eval s = i%3<br>\n",
    "| eval feature_{s}=0<br>\n",
    "| foreach feature_* [eval &lt;&lt;FIELD&gt;&gt;=random()/pow(2,31)]<br>\n",
    "| apply barebone_model as the_meaning_of_life"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Send data back to Splunk HEC\n",
    "When you configured the Splunk HEC Settings in the DSDL app you can easily send back data to an index with [Splunk's HTTP Event Collector (HEC)](https://docs.splunk.com/Documentation/Splunk/latest/Data/UsetheHTTPEventCollector). Read more about data formats and options in the [documentation](https://docs.splunk.com/Documentation/Splunk/latest/Data/FormateventsforHTTPEventCollector#Event_metadata).\n",
    "\n",
    "### Use cases\n",
    "- you want to offload longer running, possibly distributed computations that need to deliver results asynchroneously back into Splunk. \n",
    "- you might not want to present results back into the search pipeline after your `| fit` or `| apply` command. \n",
    "- you can easily utilize this approach for any logging purposes or other profiling tasks in your ML code so you can actively monitor and analyze your processes.\n",
    "\n",
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dsdlsupport import SplunkHEC as SplunkHEC\n",
    "hec = SplunkHEC.SplunkHEC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example to send 10 hello world events\n",
    "response = hec.send_hello_world(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HEC endpoint http://host.docker.internal:8088/services/collector/event \n",
      "returned with status code 200 \n",
      "and response message: {\"text\":\"Success\",\"code\":0}\n"
     ]
    }
   ],
   "source": [
    "print(\"HEC endpoint %s \\nreturned with status code %s \\nand response message: %s\" % (response.url, response.status_code, response.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example to send a JSON object, e.g. to log some data\n",
    "from datetime import datetime\n",
    "response = hec.send({'event': {'message': 'operation done', 'log_level': 'INFO' }, 'time': datetime.now().timestamp()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HEC endpoint http://host.docker.internal:8088/services/collector/event \n",
      "returned with status code 200 \n",
      "and response message: {\"text\":\"Success\",\"code\":0}\n"
     ]
    }
   ],
   "source": [
    "print(\"HEC endpoint %s \\nreturned with status code %s \\nand response message: %s\" % (response.url, response.status_code, response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of Stages\n",
    "All subsequent cells are not tagged and can be used for further freeform code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

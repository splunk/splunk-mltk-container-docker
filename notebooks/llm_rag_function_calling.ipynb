{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM with Function Calling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example Usage: | makeresults\n",
    "| fit MLTKContainer algo=llm_rag_function_calling prompt=\"Search Splunk for index _internal and sourcetype splunkd for events containing keyword error from 60 minutes ago to 30 minutes ago. Tell me how many events occurred\" model_name=\"llama3\" func1=1 func2=0  _time into app:llm_rag_function_calling as RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 0 - import libraries\n",
    "At stage 0 we define all imports necessary to run our subsequent code depending on various libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "name": "mltkc_import"
   },
   "outputs": [],
   "source": [
    "# this definition exposes all python module imports that should be available in all subsequent commands\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pymilvus\n",
    "from pymilvus import (\n",
    "    connections,\n",
    "    utility,\n",
    "    FieldSchema, CollectionSchema, DataType,\n",
    "    Collection,\n",
    ")\n",
    "import llama_index\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Document, StorageContext, ServiceContext\n",
    "from llama_index.vector_stores.milvus import MilvusVectorStore\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "import textwrap\n",
    "from llama_index.core.llms import ChatMessage, MessageRole\n",
    "from llama_index.core import ChatPromptTemplate\n",
    "from llama_index.core.agent import FunctionCallingAgentWorker\n",
    "from typing import Sequence, List\n",
    "from llama_index.core.tools import BaseTool, FunctionTool\n",
    "from llama_index.core.agent import AgentRunner, ReActAgentWorker\n",
    "from pydantic import Field\n",
    "# ...\n",
    "# global constants\n",
    "MODEL_DIRECTORY = \"/srv/app/model/data/\"\n",
    "\n",
    "def search_splunk_events(\n",
    "    index: str, \n",
    "    sourcetype: str, \n",
    "    earliest_time: str, \n",
    "    latest_time: str, \n",
    "    source: str = None, \n",
    "    keyword: str =None\n",
    "):\n",
    "    '''\n",
    "    Description on input fields\n",
    "    earliest_time: Time specifier for earliest event to search, formatted like '[+|-]<time_integer><time_unit>@<time_unit>'. For example, '-12h@h' for the past 12 hours, '-5m@m' for the last 5 minutes and '-40s@s' for the last 40 seconds\n",
    "    latest_time: Time specifier for latest event search, formatted like '[+|-]<time_integer><time_unit>@<time_unit>'. For example, '-12h@h' for the past 12 hours, '-5m@m' for the last 5 minutes and '-40s@s' for the last 40 seconds. For searching events up to now, set this field to 'now'\n",
    "    '''\n",
    "    # Imports\n",
    "    import splunklib.client as splunk_client\n",
    "    import splunklib.results as splunk_results\n",
    "    import time \n",
    "    import pandas as pd\n",
    "    # Load Splunk server info and create service\n",
    "    token = os.environ[\"splunk_access_token\"]\n",
    "    host = os.environ[\"splunk_access_host\"]\n",
    "    port = os.environ[\"splunk_access_port\"]\n",
    "    service = splunk_client.connect(host=host, port=port, token=token)\n",
    "    if index is not None:\n",
    "        index = index\n",
    "    else:\n",
    "        index= ' *'\n",
    "    if source is not None:\n",
    "        source = source\n",
    "    else:\n",
    "        source = ' *'\n",
    "    if sourcetype is not None:\n",
    "        sourcetype = sourcetype\n",
    "    else:\n",
    "        sourcetype = ' *'\n",
    "    if keyword is not None:\n",
    "        keyword = keyword\n",
    "    else:\n",
    "        keyword = ' *'\n",
    "    if earliest_time is not None:\n",
    "        earliest = earliest_time\n",
    "    else:\n",
    "        earliest = '-24h@h'\n",
    "    if latest_time is not None:\n",
    "        latest = latest_time\n",
    "    else:\n",
    "        latest = \"now\"\n",
    "\n",
    "    query = f\"index={index} sourcetype={sourcetype} source={source} {keyword} earliest={earliest} latest={latest}\"\n",
    "    query_cleaned = query.strip()\n",
    "    # add search keyword before the SPL\n",
    "    query_cleaned=\"search \"+query_cleaned\n",
    "    \n",
    "    job = service.jobs.create(\n",
    "        query_cleaned,\n",
    "        earliest_time=earliest, \n",
    "        latest_time=latest, \n",
    "        adhoc_search_level=\"smart\",\n",
    "        search_mode=\"normal\")\n",
    "    while not job.is_done():\n",
    "        time.sleep(0.1)\n",
    "    resultCount = int(job.resultCount)\n",
    "    diagnostic_messages = []\n",
    "    resultset = []\n",
    "    processed = 0\n",
    "    offset = 0\n",
    "    while processed < resultCount:\n",
    "        for event in splunk_results.JSONResultsReader(job.results(output_mode='json', offset=offset, count=0)):\n",
    "            if isinstance(event, splunk_results.Message):\n",
    "                # Diagnostic messages may be returned in the results\n",
    "                diagnostic_messages.append(event.message)\n",
    "                #print('%s: %s' % (event.type, event.message))\n",
    "            elif isinstance(event, dict):\n",
    "                # Normal events are returned as dicts\n",
    "                resultset.append(event['_raw'])\n",
    "                #print(result)\n",
    "            processed += 1\n",
    "        offset = processed   \n",
    "    results = f'The list of events searched from Splunk is {str(resultset)}'\n",
    "    return results\n",
    "\n",
    "# Milvus search function\n",
    "def search_record_from_vector_db(log_message: str, collection_name: str):\n",
    "    from pymilvus import connections, Collection\n",
    "    from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "    transformer_embedder = HuggingFaceEmbedding(model_name='all-MiniLM-L6-v2')\n",
    "    connections.connect(\"default\", host=\"milvus-standalone\", port=\"19530\")\n",
    "    collection = Collection(collection_name)\n",
    "    search_params = {\n",
    "        \"metric_type\": \"L2\",\n",
    "        \"params\": {\"nprobe\": 10},\n",
    "    }\n",
    "    log_message = transformer_embedder.get_text_embedding(log_message)\n",
    "    results = collection.search(data=[log_message], anns_field=\"embeddings\", param=search_params, limit=1, output_fields=[\"_key\",\"label\"])\n",
    "    l = []\n",
    "    for result in results:\n",
    "        t = \"\"\n",
    "        for r in result:\n",
    "            t += f\"For the log message {log_message}, the recorded similar log message is: {r.entity.get('label')}.\"\n",
    "        l.append(t)\n",
    "    return l[0]\n",
    "\n",
    "search_splunk_tool = FunctionTool.from_defaults(fn=search_splunk_events)\n",
    "search_record_from_vector_db_tool = FunctionTool.from_defaults(fn=search_record_from_vector_db)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some logging settings \n",
    "import logging\n",
    "import sys\n",
    "import llama_index.core\n",
    "from llama_index.core.callbacks import (\n",
    "    CallbackManager,\n",
    "    LlamaDebugHandler,\n",
    "    CBEventType,\n",
    ")\n",
    "\n",
    "llama_index.core.set_global_handler(\"simple\")\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "llama_debug = LlamaDebugHandler(print_trace_on_end=True)\n",
    "callback_manager = CallbackManager([llama_debug])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "name": "mltkc_stage"
   },
   "outputs": [],
   "source": [
    "# this cell is not executed from MLTK and should only be used for staging data into the notebook environment\n",
    "def stage(name):\n",
    "    with open(\"data/\"+name+\".csv\", 'r') as f:\n",
    "        df = pd.read_csv(f)\n",
    "    with open(\"data/\"+name+\".json\", 'r') as f:\n",
    "        param = json.load(f)\n",
    "    return df, param"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 2 - create and initialize a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "name": "mltkc_init"
   },
   "outputs": [],
   "source": [
    "# initialize your model\n",
    "# available inputs: data and parameters\n",
    "# returns the model object which will be used as a reference to call fit, apply and summary subsequently\n",
    "def init(df,param):\n",
    "    model = {}\n",
    "    model['hyperparameter'] = 42.0\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 3 - fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "name": "mltkc_fit"
   },
   "outputs": [],
   "source": [
    "# train your model\n",
    "# returns a fit info json object and may modify the model object\n",
    "def fit(model,df,param):\n",
    "    info = {\"message\": \"model trained\"}\n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'message': 'model trained'}\n"
     ]
    }
   ],
   "source": [
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",
    "print(fit(model,df,param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 4 - apply the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "name": "mltkc_apply"
   },
   "outputs": [],
   "source": [
    "# apply your model\n",
    "# returns the calculated results\n",
    "def apply(model,df,param):\n",
    "    # Example: 'all-MiniLM-L6-v2'\n",
    "    query = param['options']['params']['prompt'].strip('\\\"')\n",
    "    # Case of only two functions\n",
    "    try:\n",
    "        func1 = int(param['options']['params']['func1'])\n",
    "        func2 = int(param['options']['params']['func2'])\n",
    "    except:\n",
    "        func1 = 1\n",
    "        func2 = 1\n",
    "    tool_list = []\n",
    "\n",
    "    if func1:\n",
    "        tool_list.append(search_splunk_tool)\n",
    "    if func2:\n",
    "        tool_list.append(search_record_from_vector_db_tool)\n",
    "    \n",
    "    try:\n",
    "        model = param['options']['params']['model_name'].strip('\\\"')\n",
    "    except:\n",
    "        model=\"mistral\"\n",
    "    \n",
    "    url = \"http://ollama:11434\"\n",
    "    llm = Ollama(model=model, base_url=url, request_timeout=6000.0)\n",
    "\n",
    "    \n",
    "    worker = ReActAgentWorker.from_tools(tool_list, llm=llm)\n",
    "    agent = AgentRunner(worker)     \n",
    "    response = agent.chat(query)\n",
    "    \n",
    "    cols = {\"Response\": [response.response]}\n",
    "    for i in range(len(response.sources)):\n",
    "        if response.sources[i].tool_name != \"unknown\":\n",
    "            cols[response.sources[i].tool_name] = [response.sources[i].content]\n",
    "    result = pd.DataFrame(data=cols)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:httpx:load_ssl_context verify=True cert=None trust_env=True http2=False\n",
      "load_ssl_context verify=True cert=None trust_env=True http2=False\n",
      "DEBUG:httpx:load_verify_locations cafile='/usr/local/lib/python3.9/site-packages/certifi/cacert.pem'\n",
      "load_verify_locations cafile='/usr/local/lib/python3.9/site-packages/certifi/cacert.pem'\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='ollama' port=11434 local_address=None timeout=6000.0 socket_options=None\n",
      "connect_tcp.started host='ollama' port=11434 local_address=None timeout=6000.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7f1f2793b490>\n",
      "connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7f1f2793b490>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Tue, 04 Jun 2024 02:09:36 GMT'), (b'Content-Length', b'823')])\n",
      "receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Tue, 04 Jun 2024 02:09:36 GMT'), (b'Content-Length', b'823')])\n",
      "INFO:httpx:HTTP Request: POST http://ollama:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://ollama:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "response_closed.complete\n",
      "DEBUG:httpcore.connection:close.started\n",
      "close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "close.complete\n",
      "** Messages: **\n",
      "system: You are designed to help with a variety of tasks, from answering questions to providing summaries to other types of analyses.\n",
      "\n",
      "## Tools\n",
      "\n",
      "You have access to a wide variety of tools. You are responsible for using the tools in any sequence you deem appropriate to complete the task at hand.\n",
      "This may require breaking the task into subtasks and using different tools to complete each subtask.\n",
      "\n",
      "You have access to the following tools:\n",
      "> Tool Name: search_record_from_vector_db\n",
      "Tool Description: search_record_from_vector_db(log_message: str)\n",
      "None\n",
      "Tool Args: {\"type\": \"object\", \"properties\": {\"log_message\": {\"title\": \"Log Message\", \"type\": \"string\"}}, \"required\": [\"log_message\"]}\n",
      "\n",
      "> Tool Name: splunk_search_tool\n",
      "Tool Description: splunk_search_tool(index: str, sourcetype: str, hours_ago: int, source: str = None, keyword: str = None)\n",
      "None\n",
      "Tool Args: {\"type\": \"object\", \"properties\": {\"index\": {\"title\": \"Index\", \"type\": \"string\"}, \"sourcetype\": {\"title\": \"Sourcetype\", \"type\": \"string\"}, \"hours_ago\": {\"title\": \"Hours Ago\", \"type\": \"integer\"}, \"source\": {\"title\": \"Source\", \"type\": \"string\"}, \"keyword\": {\"title\": \"Keyword\", \"type\": \"string\"}}, \"required\": [\"index\", \"sourcetype\", \"hours_ago\"]}\n",
      "\n",
      "\n",
      "\n",
      "## Output Format\n",
      "\n",
      "Please answer in the same language as the question and use the following format:\n",
      "\n",
      "```\n",
      "Thought: The current language of the user is: (user's language). I need to use a tool to help me answer the question.\n",
      "Action: tool name (one of search_record_from_vector_db, splunk_search_tool) if using a tool.\n",
      "Action Input: the input to the tool, in a JSON format representing the kwargs (e.g. {\"input\": \"hello world\", \"num_beams\": 5})\n",
      "```\n",
      "\n",
      "Please ALWAYS start with a Thought.\n",
      "\n",
      "Please use a valid JSON format for the Action Input. Do NOT do this {'input': 'hello world', 'num_beams': 5}.\n",
      "\n",
      "If this format is used, the user will respond in the following format:\n",
      "\n",
      "```\n",
      "Observation: tool response\n",
      "```\n",
      "\n",
      "You should keep repeating the above format till you have enough information to answer the question without using any more tools. At that point, you MUST respond in the one of the following two formats:\n",
      "\n",
      "```\n",
      "Thought: I can answer without using any more tools. I'll use the user's language to answer\n",
      "Answer: [your answer here (In the same language as the user's question)]\n",
      "```\n",
      "\n",
      "```\n",
      "Thought: I cannot answer the question with the provided tools.\n",
      "Answer: [your answer here (In the same language as the user's question)]\n",
      "```\n",
      "\n",
      "## Current Conversation\n",
      "\n",
      "Below is the current conversation consisting of interleaving human and assistant messages.\n",
      "\n",
      "user: Search Splunk for index _internal and sourcetype splunkd for events containing keyword error pipe in the last two hours. Tell me how many events occurred\n",
      "**************************************************\n",
      "** Response: **\n",
      "assistant:  Thought: The current language of the user is: English. I need to use a tool to help me answer the question.\n",
      "Action: splunk_search_tool\n",
      "Action Input: {\"index\": \"_internal\", \"sourcetype\": \"splunkd\", \"hours_ago\": 2, \"keyword\": \"error\"}\n",
      "\n",
      "Thought: I can answer without using any more tools. I'll use the user's language to answer\n",
      "Observation: tool response (number of events found)\n",
      "Answer: [The number of events found in the last two hours for index _internal and sourcetype splunkd containing keyword error]\n",
      "**************************************************\n",
      "\n",
      "\n",
      "search index=_internal sourcetype=splunkd source= * error earliest=-2h@h\n",
      "DEBUG:splunklib.binding:GET request to https://57.180.12.62:8089/services/server/info (body: {})\n",
      "GET request to https://57.180.12.62:8089/services/server/info (body: {})\n",
      "DEBUG:splunklib.binding:Operation took 0:00:00.010684\n",
      "Operation took 0:00:00.010684\n",
      "DEBUG:splunklib.binding:GET request to https://57.180.12.62:8089/services/server/info (body: {})\n",
      "GET request to https://57.180.12.62:8089/services/server/info (body: {})\n",
      "DEBUG:splunklib.binding:Operation took 0:00:00.006356\n",
      "Operation took 0:00:00.006356\n",
      "DEBUG:splunklib.binding:POST request to https://57.180.12.62:8089/services/search/v2/jobs/ (body: {'search': 'search index=_internal sourcetype=splunkd source= * error earliest=-2h@h', 'earliest_time': '-2h@h', 'latest_time': 'now', 'adhoc_search_level': 'smart', 'search_mode': 'normal'})\n",
      "POST request to https://57.180.12.62:8089/services/search/v2/jobs/ (body: {'search': 'search index=_internal sourcetype=splunkd source= * error earliest=-2h@h', 'earliest_time': '-2h@h', 'latest_time': 'now', 'adhoc_search_level': 'smart', 'search_mode': 'normal'})\n",
      "DEBUG:splunklib.binding:Operation took 0:00:00.012534\n",
      "Operation took 0:00:00.012534\n",
      "DEBUG:splunklib.binding:GET request to https://57.180.12.62:8089/services/search/v2/jobs/1717466976.41025 (body: {})\n",
      "GET request to https://57.180.12.62:8089/services/search/v2/jobs/1717466976.41025 (body: {})\n",
      "DEBUG:splunklib.binding:Operation took 0:00:00.007320\n",
      "Operation took 0:00:00.007320\n",
      "DEBUG:splunklib.binding:GET request to https://57.180.12.62:8089/servicesNS/nobody/search/search/v2/jobs/1717466976.41025 (body: {})\n",
      "GET request to https://57.180.12.62:8089/servicesNS/nobody/search/search/v2/jobs/1717466976.41025 (body: {})\n",
      "DEBUG:splunklib.binding:Operation took 0:00:00.008425\n",
      "Operation took 0:00:00.008425\n",
      "DEBUG:splunklib.binding:GET request to https://57.180.12.62:8089/servicesNS/nobody/search/search/v2/jobs/1717466976.41025 (body: {})\n",
      "GET request to https://57.180.12.62:8089/servicesNS/nobody/search/search/v2/jobs/1717466976.41025 (body: {})\n",
      "DEBUG:splunklib.binding:Operation took 0:00:00.009025\n",
      "Operation took 0:00:00.009025\n",
      "DEBUG:splunklib.binding:GET request to https://57.180.12.62:8089/servicesNS/nobody/search/search/v2/jobs/1717466976.41025 (body: {})\n",
      "GET request to https://57.180.12.62:8089/servicesNS/nobody/search/search/v2/jobs/1717466976.41025 (body: {})\n",
      "DEBUG:splunklib.binding:Operation took 0:00:00.008375\n",
      "Operation took 0:00:00.008375\n",
      "DEBUG:splunklib.binding:POST request to https://57.180.12.62:8089/servicesNS/nobody/search/search/v2/jobs/1717466976.41025/results (body: {'output_mode': 'json', 'offset': 0, 'count': 0, 'segmentation': 'none'})\n",
      "POST request to https://57.180.12.62:8089/servicesNS/nobody/search/search/v2/jobs/1717466976.41025/results (body: {'output_mode': 'json', 'offset': 0, 'count': 0, 'segmentation': 'none'})\n",
      "DEBUG:splunklib.binding:Operation took 0:00:00.007791\n",
      "Operation took 0:00:00.007791\n",
      "DEBUG:httpx:load_ssl_context verify=True cert=None trust_env=True http2=False\n",
      "load_ssl_context verify=True cert=None trust_env=True http2=False\n",
      "DEBUG:httpx:load_verify_locations cafile='/usr/local/lib/python3.9/site-packages/certifi/cacert.pem'\n",
      "load_verify_locations cafile='/usr/local/lib/python3.9/site-packages/certifi/cacert.pem'\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='ollama' port=11434 local_address=None timeout=6000.0 socket_options=None\n",
      "connect_tcp.started host='ollama' port=11434 local_address=None timeout=6000.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7f1f272403d0>\n",
      "connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7f1f272403d0>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Tue, 04 Jun 2024 02:09:41 GMT'), (b'Content-Length', b'471')])\n",
      "receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Tue, 04 Jun 2024 02:09:41 GMT'), (b'Content-Length', b'471')])\n",
      "INFO:httpx:HTTP Request: POST http://ollama:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://ollama:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "response_closed.complete\n",
      "DEBUG:httpcore.connection:close.started\n",
      "close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "close.complete\n",
      "** Messages: **\n",
      "system: You are designed to help with a variety of tasks, from answering questions to providing summaries to other types of analyses.\n",
      "\n",
      "## Tools\n",
      "\n",
      "You have access to a wide variety of tools. You are responsible for using the tools in any sequence you deem appropriate to complete the task at hand.\n",
      "This may require breaking the task into subtasks and using different tools to complete each subtask.\n",
      "\n",
      "You have access to the following tools:\n",
      "> Tool Name: search_record_from_vector_db\n",
      "Tool Description: search_record_from_vector_db(log_message: str)\n",
      "None\n",
      "Tool Args: {\"type\": \"object\", \"properties\": {\"log_message\": {\"title\": \"Log Message\", \"type\": \"string\"}}, \"required\": [\"log_message\"]}\n",
      "\n",
      "> Tool Name: splunk_search_tool\n",
      "Tool Description: splunk_search_tool(index: str, sourcetype: str, hours_ago: int, source: str = None, keyword: str = None)\n",
      "None\n",
      "Tool Args: {\"type\": \"object\", \"properties\": {\"index\": {\"title\": \"Index\", \"type\": \"string\"}, \"sourcetype\": {\"title\": \"Sourcetype\", \"type\": \"string\"}, \"hours_ago\": {\"title\": \"Hours Ago\", \"type\": \"integer\"}, \"source\": {\"title\": \"Source\", \"type\": \"string\"}, \"keyword\": {\"title\": \"Keyword\", \"type\": \"string\"}}, \"required\": [\"index\", \"sourcetype\", \"hours_ago\"]}\n",
      "\n",
      "\n",
      "\n",
      "## Output Format\n",
      "\n",
      "Please answer in the same language as the question and use the following format:\n",
      "\n",
      "```\n",
      "Thought: The current language of the user is: (user's language). I need to use a tool to help me answer the question.\n",
      "Action: tool name (one of search_record_from_vector_db, splunk_search_tool) if using a tool.\n",
      "Action Input: the input to the tool, in a JSON format representing the kwargs (e.g. {\"input\": \"hello world\", \"num_beams\": 5})\n",
      "```\n",
      "\n",
      "Please ALWAYS start with a Thought.\n",
      "\n",
      "Please use a valid JSON format for the Action Input. Do NOT do this {'input': 'hello world', 'num_beams': 5}.\n",
      "\n",
      "If this format is used, the user will respond in the following format:\n",
      "\n",
      "```\n",
      "Observation: tool response\n",
      "```\n",
      "\n",
      "You should keep repeating the above format till you have enough information to answer the question without using any more tools. At that point, you MUST respond in the one of the following two formats:\n",
      "\n",
      "```\n",
      "Thought: I can answer without using any more tools. I'll use the user's language to answer\n",
      "Answer: [your answer here (In the same language as the user's question)]\n",
      "```\n",
      "\n",
      "```\n",
      "Thought: I cannot answer the question with the provided tools.\n",
      "Answer: [your answer here (In the same language as the user's question)]\n",
      "```\n",
      "\n",
      "## Current Conversation\n",
      "\n",
      "Below is the current conversation consisting of interleaving human and assistant messages.\n",
      "\n",
      "user: Search Splunk for index _internal and sourcetype splunkd for events containing keyword error pipe in the last two hours. Tell me how many events occurred\n",
      "assistant: Thought: The current language of the user is: English. I need to use a tool to help me answer the question.\n",
      "Action: splunk_search_tool\n",
      "Action Input: {'index': '_internal', 'sourcetype': 'splunkd', 'hours_ago': 2, 'keyword': 'error'}\n",
      "user: Observation: [['06-04-2024 00:44:00.151 +0000 WARN  HttpListener [3992207 HTTPDispatch] - Socket error from 127.0.0.1:40098 while accessing /services/mltk-container/sync: Broken pipe', '06-04-2024 00:42:33.027 +0000 ERROR X509 [3621916 ApplicationUpdateThread] - X509 certificate (CN=splunkbase.splunk.com,O=Splunk Inc.,L=San Francisco,ST=California,C=US) common name (splunkbase.splunk.com) did not match any allowed names (apps.splunk.com,cdn.apps.splunk.com)', '06-04-2024 00:42:00.146 +0000 WARN  HttpListener [3992207 HTTPDispatch] - Socket error from 127.0.0.1:36490 while accessing /services/mltk-container/sync: Broken pipe', '06-04-2024 00:40:00.129 +0000 WARN  HttpListener [3992207 HTTPDispatch] - Socket error from 127.0.0.1:43914 while accessing /services/mltk-container/sync: Broken pipe', '06-04-2024 00:38:00.127 +0000 WARN  HttpListener [3992207 HTTPDispatch] - Socket error from 127.0.0.1:50112 while accessing /services/mltk-container/sync: Broken pipe', '06-04-2024 00:36:00.098 +0000 WARN  HttpListener [3992207 HTTPDispatch] - Socket error from 127.0.0.1:51220 while accessing /services/mltk-container/sync: Broken pipe', '06-04-2024 00:34:00.072 +0000 WARN  HttpListener [3992207 HTTPDispatch] - Socket error from 127.0.0.1:39796 while accessing /services/mltk-container/sync: Broken pipe', '06-04-2024 00:32:00.059 +0000 WARN  HttpListener [3992207 HTTPDispatch] - Socket error from 127.0.0.1:34036 while accessing /services/mltk-container/sync: Broken pipe', '06-04-2024 00:30:01.046 +0000 WARN  HttpListener [3992207 HTTPDispatch] - Socket error from 127.0.0.1:60640 while accessing /services/mltk-container/sync: Broken pipe', '06-04-2024 00:28:01.029 +0000 WARN  HttpListener [3992207 HTTPDispatch] - Socket error from 127.0.0.1:54240 while accessing /services/mltk-container/sync: Broken pipe', '06-04-2024 00:26:01.017 +0000 WARN  HttpListener [3992207 HTTPDispatch] - Socket error from 127.0.0.1:50466 while accessing /services/mltk-container/sync: Broken pipe', '06-04-2024 00:24:01.002 +0000 WARN  HttpListener [3992207 HTTPDispatch] - Socket error from 127.0.0.1:45594 while accessing /services/mltk-container/sync: Broken pipe', '06-04-2024 00:22:00.985 +0000 WARN  HttpListener [3992207 HTTPDispatch] - Socket error from 127.0.0.1:42018 while accessing /services/mltk-container/sync: Broken pipe', '06-04-2024 00:20:00.971 +0000 WARN  HttpListener [3992207 HTTPDispatch] - Socket error from 127.0.0.1:56858 while accessing /services/mltk-container/sync: Broken pipe', '06-04-2024 00:18:00.955 +0000 WARN  HttpListener [3992207 HTTPDispatch] - Socket error from 127.0.0.1:44056 while accessing /services/mltk-container/sync: Broken pipe', '06-04-2024 00:16:00.935 +0000 WARN  HttpListener [3992207 HTTPDispatch] - Socket error from 127.0.0.1:50268 while accessing /services/mltk-container/sync: Broken pipe', '06-04-2024 00:14:00.921 +0000 WARN  HttpListener [3992207 HTTPDispatch] - Socket error from 127.0.0.1:32864 while accessing /services/mltk-container/sync: Broken pipe', '06-04-2024 00:12:00.906 +0000 WARN  HttpListener [3992207 HTTPDispatch] - Socket error from 127.0.0.1:46714 while accessing /services/mltk-container/sync: Broken pipe', '06-04-2024 00:10:00.891 +0000 WARN  HttpListener [3992207 HTTPDispatch] - Socket error from 127.0.0.1:48196 while accessing /services/mltk-container/sync: Broken pipe', '06-04-2024 00:08:00.876 +0000 WARN  HttpListener [3992207 HTTPDispatch] - Socket error from 127.0.0.1:45306 while accessing /services/mltk-container/sync: Broken pipe', '06-04-2024 00:06:00.862 +0000 WARN  HttpListener [3992207 HTTPDispatch] - Socket error from 127.0.0.1:57688 while accessing /services/mltk-container/sync: Broken pipe', '06-04-2024 00:04:00.850 +0000 WARN  HttpListener [3992207 HTTPDispatch] - Socket error from 127.0.0.1:42810 while accessing /services/mltk-container/sync: Broken pipe', '06-04-2024 00:02:00.839 +0000 WARN  HttpListener [3992207 HTTPDispatch] - Socket error from 127.0.0.1:56866 while accessing /services/mltk-container/sync: Broken pipe', '06-04-2024 00:00:00.823 +0000 WARN  HttpListener [3992207 HTTPDispatch] - Socket error from 127.0.0.1:49438 while accessing /services/mltk-container/sync: Broken pipe']]\n",
      "**************************************************\n",
      "** Response: **\n",
      "assistant:  Thought: I can answer without using any more tools. I'll use the user's language to answer\n",
      "Answer: There were 26 events containing the keyword 'error' in the last two hours.\n",
      "**************************************************\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",
    "query = 'Search Splunk for index _internal and sourcetype splunkd for events containing keyword error pipe in the last two hours. Tell me how many events occurred'\n",
    "r = apply(None,None,query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Response': [\"It seems there was a similar error related to a broken pipe that occurred on 05-12-2024 in the DSDL app sync process. The possible solution provided for this issue is to check the docker host connectivity. However, I don't have information about whether this issue has been resolved in your current situation or not. You should verify if the problem persists and whether checking docker host connectivity resolves it in your case.\"],\n",
       " 'splunk_search_tool': [\"['06-03-2024 03:45:09.104 +0000 WARN  HttpListener [3992795 webui] - Socket error from 106.72.0.128:5842 while accessing /en-US/static/@d95b3299fa65/build/api/layout-dark.js: Broken pipe']\"],\n",
       " 'search_record_from_vector_db': ['The recorded log message is: 05-12-2024 02:56:00.950 +0000 WARN  HttpListener [3906 HTTPDispatch] - Socket error from 127.0.0.1:33540 while accessing /services/mltk-container/sync: Broken pipe. The recorded explanation of this message is Socket error in DSDL app sync.. The recorded solution of this log message is Check docker host connectivity.. The past issue has been Resolved. DISTANCE: 1.027364730834961']}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = {\"Response\": [r.response]}\n",
    "for i in range(len(r.sources)):\n",
    "    cols[r.sources[i].tool_name] = [r.sources[i].content]\n",
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Response</th>\n",
       "      <th>splunk_search_tool</th>\n",
       "      <th>search_record_from_vector_db</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>It seems there was a similar error related to ...</td>\n",
       "      <td>['06-03-2024 03:45:09.104 +0000 WARN  HttpList...</td>\n",
       "      <td>The recorded log message is: 05-12-2024 02:56:...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Response  \\\n",
       "0  It seems there was a similar error related to ...   \n",
       "\n",
       "                                  splunk_search_tool  \\\n",
       "0  ['06-03-2024 03:45:09.104 +0000 WARN  HttpList...   \n",
       "\n",
       "                        search_record_from_vector_db  \n",
       "0  The recorded log message is: 05-12-2024 02:56:...  "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(data=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"It seems there was a similar error related to a broken pipe that occurred on 05-12-2024 in the DSDL app sync process. The possible solution provided for this issue is to check the docker host connectivity. However, I don't have information about whether this issue has been resolved in your current situation or not. You should verify if the problem persists and whether checking docker host connectivity resolves it in your case.\""
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'splunk_search_tool'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.sources[0].tool_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The recorded log message is: 05-12-2024 02:56:00.950 +0000 WARN  HttpListener [3906 HTTPDispatch] - Socket error from 127.0.0.1:33540 while accessing /services/mltk-container/sync: Broken pipe. The recorded explanation of this message is Socket error in DSDL app sync.. The recorded solution of this log message is Check docker host connectivity.. The past issue has been Resolved. DISTANCE: 1.027364730834961'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.sources[1].content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 5 - save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "name": "mltkc_save"
   },
   "outputs": [],
   "source": [
    "# save model to name in expected convention \"<algo_name>_<model_name>\"\n",
    "def save(model,name):\n",
    "    with open(MODEL_DIRECTORY + name + \".json\", 'w') as file:\n",
    "        json.dump(model, file)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 6 - load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "name": "mltkc_load"
   },
   "outputs": [],
   "source": [
    "# load model from name in expected convention \"<algo_name>_<model_name>\"\n",
    "def load(name):\n",
    "    model = {}\n",
    "    with open(MODEL_DIRECTORY + name + \".json\", 'r') as file:\n",
    "        model = json.load(file)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 7 - provide a summary of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "name": "mltkc_summary"
   },
   "outputs": [],
   "source": [
    "# return a model summary\n",
    "def summary(model=None):\n",
    "    returns = {\"version\": {\"numpy\": np.__version__, \"pandas\": pd.__version__} }\n",
    "    return returns\n",
    "\n",
    "def compute(model,df,param):\n",
    "    # Example: 'all-MiniLM-L6-v2'\n",
    "    query = param['params']['prompt'].strip('\\\"')\n",
    "    # Case of only two functions\n",
    "    try:\n",
    "        func1 = int(param['params']['func1'])\n",
    "        func2 = int(param['params']['func2'])\n",
    "    except:\n",
    "        func1 = 1\n",
    "        func2 = 1\n",
    "    tool_list = []\n",
    "\n",
    "    if func1:\n",
    "        tool_list.append(search_splunk_tool)\n",
    "    if func2:\n",
    "        tool_list.append(search_record_from_vector_db_tool)\n",
    "    \n",
    "    try:\n",
    "        model = param['params']['model_name'].strip('\\\"')\n",
    "    except:\n",
    "        model=\"mistral\"\n",
    "    \n",
    "    url = \"http://ollama:11434\"\n",
    "    llm = Ollama(model=model, base_url=url, request_timeout=6000.0)\n",
    "\n",
    "    \n",
    "    worker = ReActAgentWorker.from_tools(tool_list, llm=llm)\n",
    "    agent = AgentRunner(worker)     \n",
    "    response = agent.chat(query)\n",
    "    \n",
    "    cols = {\"Response\": response.response}\n",
    "    for i in range(len(response.sources)):\n",
    "        if response.sources[i].tool_name != \"unknown\":\n",
    "            cols[response.sources[i].tool_name] = response.sources[i].content\n",
    "    result = [cols]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After implementing your fit, apply, save and load you can train your model:<br>\n",
    "| makeresults count=10<br>\n",
    "| streamstats c as i<br>\n",
    "| eval s = i%3<br>\n",
    "| eval feature_{s}=0<br>\n",
    "| foreach feature_* [eval &lt;&lt;FIELD&gt;&gt;=random()/pow(2,31)]<br>\n",
    "| fit MLTKContainer algo=barebone s from feature_* into app:barebone_model<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or apply your model:<br>\n",
    "| makeresults count=10<br>\n",
    "| streamstats c as i<br>\n",
    "| eval s = i%3<br>\n",
    "| eval feature_{s}=0<br>\n",
    "| foreach feature_* [eval &lt;&lt;FIELD&gt;&gt;=random()/pow(2,31)]<br>\n",
    "| apply barebone_model as the_meaning_of_life"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Send data back to Splunk HEC\n",
    "When you configured the Splunk HEC Settings in the DSDL app you can easily send back data to an index with [Splunk's HTTP Event Collector (HEC)](https://docs.splunk.com/Documentation/Splunk/latest/Data/UsetheHTTPEventCollector). Read more about data formats and options in the [documentation](https://docs.splunk.com/Documentation/Splunk/latest/Data/FormateventsforHTTPEventCollector#Event_metadata).\n",
    "\n",
    "### Use cases\n",
    "- you want to offload longer running, possibly distributed computations that need to deliver results asynchroneously back into Splunk. \n",
    "- you might not want to present results back into the search pipeline after your `| fit` or `| apply` command. \n",
    "- you can easily utilize this approach for any logging purposes or other profiling tasks in your ML code so you can actively monitor and analyze your processes.\n",
    "\n",
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dsdlsupport import SplunkHEC as SplunkHEC\n",
    "hec = SplunkHEC.SplunkHEC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example to send 10 hello world events\n",
    "response = hec.send_hello_world(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HEC endpoint http://host.docker.internal:8088/services/collector/event \n",
      "returned with status code 200 \n",
      "and response message: {\"text\":\"Success\",\"code\":0}\n"
     ]
    }
   ],
   "source": [
    "print(\"HEC endpoint %s \\nreturned with status code %s \\nand response message: %s\" % (response.url, response.status_code, response.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example to send a JSON object, e.g. to log some data\n",
    "from datetime import datetime\n",
    "response = hec.send({'event': {'message': 'operation done', 'log_level': 'INFO' }, 'time': datetime.now().timestamp()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HEC endpoint http://host.docker.internal:8088/services/collector/event \n",
      "returned with status code 200 \n",
      "and response message: {\"text\":\"Success\",\"code\":0}\n"
     ]
    }
   ],
   "source": [
    "print(\"HEC endpoint %s \\nreturned with status code %s \\nand response message: %s\" % (response.url, response.status_code, response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of Stages\n",
    "All subsequent cells are not tagged and can be used for further freeform code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

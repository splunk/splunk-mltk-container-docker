{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text classification fine-tuning script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script contains the source code for fine-tuning the BERT model on **End-to-End text classification task** in both English and Japanese.\n",
    "\n",
    "## Sample usage of SPL for fine-tuning:\n",
    "\n",
    "| inputlookup classification_en\n",
    "| fields - TITLE ID\n",
    "| rename ABSTRACT as text\n",
    "| head 100\n",
    "| fit MLTKContainer algo=transformers_classification max_epochs=1 lang=en base_model=bert_classification_en text \"Computer Science\" Mathematics Physics \"Quantitative Biology\" \"Quantitative Finance\" Statistics into app:bert_classification_en_finetuned_final as score\n",
    "\n",
    "## Sample usage for applying:\n",
    "\n",
    "| inputlookup classification_en\n",
    "| fields - TITLE ID\n",
    "| rename ABSTRACT as text\n",
    "| head 10\n",
    "| apply bert_classification_en_finetuned_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 0 - import libraries\n",
    "At stage 0 we define all imports necessary to run our subsequent code depending on various libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "name": "mltkc_import"
   },
   "outputs": [],
   "source": [
    "# this definition exposes all python module imports that should be available in all subsequent commands\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import re\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import tarfile\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "# tensorboard related\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import tensorboard\n",
    "import datetime\n",
    "import logging\n",
    "import sys\n",
    "import io\n",
    "import os\n",
    "import psutil\n",
    "import shutil\n",
    "\n",
    "# Fine-tune parameters initialization\n",
    "MODEL_NAME = \"/srv/app/model/data\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "max_length_src = 500\n",
    "max_length_target = 200\n",
    "\n",
    "batch_size_train = 4\n",
    "batch_size_valid = 4\n",
    "\n",
    "epochs = 100\n",
    "patience = 20\n",
    "\n",
    "MODEL_DIRECTORY = \"/\"\n",
    "\n",
    "class BertClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "        Bert Model for classification Tasks.\n",
    "    \"\"\"\n",
    "    def __init__(self, MODEL_NAME, D_out, freeze_bert=False):\n",
    "        super(BertClassifier,self).__init__()\n",
    "        D_in, H, D_out = 768, 60, D_out\n",
    "        self.bert = BertModel.from_pretrained(MODEL_NAME, local_files_only=True)\n",
    "        self.classifier = nn.Sequential(\n",
    "                            nn.Linear(D_in, H),\n",
    "                            nn.ReLU(),\n",
    "                            nn.Linear(H, D_out))\n",
    "        # Freeze the Bert Model\n",
    "        if freeze_bert:\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "                \n",
    "    def forward(self,input_ids,attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids,\n",
    "                           attention_mask = attention_mask)\n",
    "        last_hidden_state_cls = outputs[0][:,0,:]\n",
    "        logit = self.classifier(last_hidden_state_cls)\n",
    "        \n",
    "        return logit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1 - get a data sample from Splunk\n",
    "In Splunk run a search to pipe a prepared dataset into this environment. (internal testing only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| inputlookup classification_en\n",
    "| fields - TITLE ID\n",
    "| rename ABSTRACT as text\n",
    "| head 100\n",
    "| fit MLTKContainer algo=transformers_classification mode=stage \"Computer Science\" max_epochs=1 lang=en base_model=bert_classification_en text \"Mathematics\" \"Physics\" \"Quantitative Biology\" \"Quantitative Finance\" \"Statistics\"  into app:bert_classification_en_finetuned_test as score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "name": "mltkc_stage"
   },
   "outputs": [],
   "source": [
    "# this cell is not executed from MLTK and should only be used for staging data into the notebook environment\n",
    "def stage(name):\n",
    "    print(\"DEBUG stage call\")\n",
    "    print(\"DEBUG\" + name)\n",
    "    with open(\"/srv/notebooks/data/\"+name+\".csv\", 'r') as f:\n",
    "        df = pd.read_csv(f)\n",
    "    with open(\"/srv/notebooks/data/\"+name+\".json\", 'r') as f:\n",
    "        param = json.load(f) \n",
    "    return df, param"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 2 - create and initialize a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "name": "mltkc_init",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def init(df,param):\n",
    "    tag = \"-- process=fine_tuning_progress model={} max_epoch={} -- \".format(param['options']['model_name'], param['options']['params']['max_epochs'])\n",
    "\n",
    "    print(tag + \"Training data loaded with shape: \" + str(df.shape))\n",
    "    print(tag + \"Input parameters: \", param['options']['params'])\n",
    "    print(tag + \"Epoch number: \" + param['options']['params']['max_epochs'])\n",
    "    print(tag + \"Base model: \" + param['options']['params']['base_model'])\n",
    "    print(tag + \"Model Initialization: started\")\n",
    "    l = len(list(df.columns)) - 1\n",
    "    MODEL_NAME = \"/srv/app/model/data/classification\"\n",
    "    if param['options']['params']['base_model'] == \"bert_classification_en\" or param['options']['params']['base_model'] == \"bert_classification_jp\":\n",
    "        MODEL_NAME = os.path.join(MODEL_NAME, param['options']['params']['lang'], param['options']['params']['base_model'])\n",
    "        print(tag + \"Model file in \" + MODEL_NAME)\n",
    "        model = BertClassifier(MODEL_NAME, l)\n",
    "        model = model.to(device)\n",
    "    elif param['options']['params']['lang'] == \"en\":\n",
    "        MODEL_NAME = os.path.join(MODEL_NAME, param['options']['params']['lang'], param['options']['params']['base_model'])\n",
    "        BERT_MODEL = \"/srv/app/model/data/classification/en/bert_classification_en\"\n",
    "        print(tag + \"Model file in \" + MODEL_NAME)\n",
    "        model = BertClassifier(BERT_MODEL, l)\n",
    "        model = model.to(device)\n",
    "        model.load_state_dict(torch.load(os.path.join(MODEL_NAME, \"pytorch_model.pt\"), map_location=torch.device(device)))\n",
    "    else:\n",
    "        MODEL_NAME = os.path.join(MODEL_NAME, param['options']['params']['lang'], param['options']['params']['base_model'])\n",
    "        BERT_MODEL = \"/srv/app/model/data/classification/jp/bert_classification_jp\"\n",
    "        print(tag + \"Model file in \" + MODEL_NAME)\n",
    "        model = BertClassifier(BERT_MODEL, l)\n",
    "        model = model.to(device)\n",
    "        model.load_state_dict(torch.load(os.path.join(MODEL_NAME, \"pytorch_model.pt\"), map_location=torch.device(device)))\n",
    "        \n",
    "        \n",
    "    print(tag + \"Model Initialization: successfully finished\")\n",
    "    # GPU memory calculation\n",
    "    if torch.cuda.is_available(): \n",
    "        t = torch.cuda.get_device_properties(0).total_memory\n",
    "        r = torch.cuda.memory_reserved(0)\n",
    "        a = torch.cuda.memory_allocated(0)\n",
    "    else:\n",
    "        t, r, a = 0,0,0\n",
    "    f = r-a  # free inside reserved\n",
    "    load1, load5, load15 = psutil.getloadavg()\n",
    "    cpu_usage = (load15/os.cpu_count()) * 100\n",
    "    stat = shutil.disk_usage(\"/\")\n",
    "    \n",
    "    print(tag + \"#GPU memory --Total memory: {}, --Memory reserved: {}, --Memory allocated: {}. #CPU: {}% occupied. #disk {}\".format(t,r,a,cpu_usage,stat))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 3 - fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "name": "mltkc_fit"
   },
   "outputs": [],
   "source": [
    "def fit(model,df,param):  \n",
    "    tag = \"-- process=fine_tuning_progress model={} max_epoch={} -- \".format(param['options']['model_name'], param['options']['params']['max_epochs'])\n",
    "    l = len(list(df.columns)) - 1\n",
    "    df = df.reindex(sorted(df.columns), axis=1)\n",
    "    MODEL_DIRECTORY = os.path.join(\"/srv/app/model/data/classification\", param['options']['params']['lang'],param['options']['model_name'])\n",
    "    if \"batch_size\" in param['options']['params']:\n",
    "        print(tag + \"setting batch size to \", param['options']['params']['batch_size'])\n",
    "        batch_size_train = int(param['options']['params']['batch_size'])\n",
    "        batch_size_valid = int(param['options']['params']['batch_size'])\n",
    "    else:\n",
    "        batch_size_train = 4\n",
    "        batch_size_valid = 4\n",
    "    # Data preparation\n",
    "    def text_preprocessing(text):\n",
    "        if param['options']['params']['lang'] == \"en\":\n",
    "            text = text.lower()\n",
    "            text = re.sub(r\"what's\", \"what is \", text)\n",
    "            text = re.sub(r\"won't\", \"will not \", text)\n",
    "            text = re.sub(r\"\\'s\", \" \", text)\n",
    "            text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "            text = re.sub(r\"can't\", \"can not \", text)\n",
    "            text = re.sub(r\"n't\", \" not \", text)\n",
    "            text = re.sub(r\"i'm\", \"i am \", text)\n",
    "            text = re.sub(r\"\\'re\", \" are \", text)\n",
    "            text = re.sub(r\"\\'d\", \" would \", text)\n",
    "            text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "            text = re.sub(r\"\\'scuse\", \" excuse \", text)\n",
    "            text = re.sub(r\"\\'\\n\", \" \", text)\n",
    "            text = re.sub(r\"-\", \" \", text)\n",
    "            text = re.sub(r\"\\'\\xa0\", \" \", text)\n",
    "            text = re.sub('\\s+', ' ', text)\n",
    "            text = ''.join(c for c in text if not c.isnumeric())\n",
    "            text = re.sub(r'(@.*?)[\\s]', ' ', text)\n",
    "            text = re.sub(r'&amp;', '&', text)\n",
    "            text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        else:\n",
    "            text = re.sub(r'[\\r\\t\\n\\u3000]', '', text)\n",
    "            text = text.lower()\n",
    "            text = text.strip()\n",
    "        return text\n",
    "    \n",
    "    MODEL_NAME = \"/srv/app/model/data/classification\"\n",
    "    MODEL_NAME = os.path.join(MODEL_NAME, param['options']['params']['lang'], param['options']['params']['base_model'])\n",
    "    tokenizer = BertTokenizer.from_pretrained(MODEL_NAME,do_lower_case=True)\n",
    "    print(tag + \"tokenizer intialized\")\n",
    "\n",
    "    def preprocessing_for_bert(data):\n",
    "        input_ids = []\n",
    "        attention_masks = []   \n",
    "        for sent in data:\n",
    "            encoded_sent = tokenizer.encode_plus(\n",
    "            text = text_preprocessing(sent),   #preprocess sentence\n",
    "            add_special_tokens = True,         #Add `[CLS]` and `[SEP]`\n",
    "            max_length= max_length_src  ,             #Max length to truncate/pad\n",
    "            pad_to_max_length = True,          #pad sentence to max length \n",
    "            return_attention_mask= True        #Return attention mask \n",
    "            )\n",
    "            # Add the outputs to the lists\n",
    "            input_ids.append(encoded_sent.get('input_ids'))\n",
    "            attention_masks.append(encoded_sent.get('attention_mask'))\n",
    "        \n",
    "        #convert lists to tensors\n",
    "        input_ids = torch.tensor(input_ids)\n",
    "        attention_masks = torch.tensor(attention_masks)\n",
    "\n",
    "        return input_ids,attention_masks\n",
    "    \n",
    "    labels = list(df.columns)\n",
    "    labels.remove('text')\n",
    "    X = df.text.values\n",
    "    y = df[labels].values\n",
    "    X_train, X_val, y_train, y_val =train_test_split(X, y, test_size=0.1, random_state=42, shuffle=True)\n",
    "    print(tag + \"Data vectorization: started\")\n",
    "    train_inputs, train_masks = preprocessing_for_bert(X_train)\n",
    "    val_inputs, val_masks = preprocessing_for_bert(X_val)\n",
    "    train_labels = torch.tensor(y_train)\n",
    "    val_labels = torch.tensor(y_val)\n",
    "    \n",
    "    # Create the DataLoader for our training set\n",
    "    train_data = TensorDataset(train_inputs,train_masks, train_labels)\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size_train)\n",
    "\n",
    "    # Create the DataLoader for our validation set\n",
    "    val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
    "    val_sampler = SequentialSampler(val_data)\n",
    "    val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size_valid)\n",
    "    print(tag + \"Data vectorization: finished.\")\n",
    "    print(tag + \"#Training data: \" + str(len(train_data)) + \", #Test data: \" + str(len(val_data)))\n",
    "\n",
    "    \n",
    "    def initialize_model(epochs=4):\n",
    "        \"\"\"Initialize the Bert Classifier, the optimizer and the learning rate scheduler.\n",
    "        \"\"\"\n",
    "\n",
    "        # Instantiate Bert Classifier\n",
    "        bert_classifier = model\n",
    "\n",
    "        # Create the optimizer\n",
    "        optimizer = AdamW(bert_classifier.parameters(),\n",
    "                         lr=5e-5, #Default learning rate\n",
    "                         eps=1e-8 #Default epsilon value\n",
    "                         )\n",
    "        # Total number of training steps\n",
    "        total_steps = len(train_dataloader) * epochs\n",
    "        # Set up the learning rate scheduler\n",
    "        scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                                  num_warmup_steps=0, # Default value\n",
    "                                                  num_training_steps=total_steps)\n",
    "        return bert_classifier, optimizer, scheduler\n",
    "    \n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def set_seed(seed_value=42):\n",
    "        \"\"\"Set seed for reproducibility.\n",
    "        \"\"\"\n",
    "        random.seed(seed_value)\n",
    "        np.random.seed(seed_value)\n",
    "        torch.manual_seed(seed_value)\n",
    "        torch.cuda.manual_seed_all(seed_value)\n",
    "        \n",
    "    # Training function\n",
    "    def train(model, train_dataloader, val_dataloader=None, epochs=4, evaluation=False):\n",
    "        \"\"\"Train the BertClassifier model.\n",
    "        \"\"\"\n",
    "        # Start training loop\n",
    "        for epoch_i in range(epochs):\n",
    "            # =======================================\n",
    "            #               Training\n",
    "            # =======================================\n",
    "\n",
    "            # Measure the elapsed time of each epoch\n",
    "            t0_epoch, t0_batch = time.time(), time.time()\n",
    "\n",
    "            # Reset tracking variables at the beginning of each epoch\n",
    "            total_loss, batch_loss, batch_counts = 0, 0, 0\n",
    "            \n",
    "            total = len(train_dataloader)\n",
    "\n",
    "            # Put the model into the training mode\n",
    "            model.train()\n",
    "            for step, batch in enumerate(train_dataloader):\n",
    "                batch_counts +=1\n",
    "                # Load batch to GPU\n",
    "                b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "                # Zero out any previously calculated gradients\n",
    "                model.zero_grad()\n",
    "\n",
    "                # Perform a forward pass. This will return logits.\n",
    "                logits = model(b_input_ids, b_attn_mask)\n",
    "\n",
    "                # Compute loss and accumulate the loss values\n",
    "                loss = loss_fn(logits, b_labels.float())\n",
    "                batch_loss += loss.item()\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                # Perform a backward pass to calculate gradients\n",
    "                loss.backward()\n",
    "\n",
    "                # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "                # Update parameters and the learning rate\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                \n",
    "                print(tag + \"Processed {}% of the {}-th epoch. Finished {} out of {} batches. Loss: {} \".format(round(batch_counts/total*100), epoch_i+1, batch_counts, total, round(batch_loss / batch_counts,2)), flush=True)\n",
    "                \n",
    "                if (step % 50000 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n",
    "                    # Calculate time elapsed for 20 batches\n",
    "                    time_elapsed = time.time() - t0_batch\n",
    "                  \n",
    "                    # Reset batch tracking variables\n",
    "                    batch_loss, batch_counts = 0, 0\n",
    "                    t0_batch = time.time()\n",
    "\n",
    "            # Calculate the average loss over the entire training data\n",
    "            avg_train_loss = total_loss / len(train_dataloader)\n",
    "            \n",
    "            \n",
    "            tokenizer.save_pretrained(MODEL_DIRECTORY)\n",
    "            print(tag + \"tokenizer saved in \" + MODEL_DIRECTORY, flush=True)\n",
    "            torch.save(model.state_dict(),os.path.join(MODEL_DIRECTORY, \"pytorch_model.pt\"))\n",
    "            print(tag + \"model saved in \" + MODEL_DIRECTORY, flush=True)\n",
    "            # =======================================\n",
    "            #               Evaluation\n",
    "            # =======================================\n",
    "            if evaluation == True:\n",
    "                # After the completion of each training epoch, measure the model's performance\n",
    "                # on our validation set.\n",
    "                val_loss, val_accuracy = evaluate(model, val_dataloader)\n",
    "\n",
    "                # Print performance over the entire training data\n",
    "                time_elapsed = time.time() - t0_epoch\n",
    "                \n",
    "                print(tag + '[{}/{}] train loss: {:.4f}, valid loss: {:.4f}, valid accuracy: {:.4f} [{}{:.0f}s]'.format(\n",
    "                        epoch_i+1, epochs, avg_train_loss, val_loss, val_accuracy,\n",
    "                        str(int(math.floor(time_elapsed / 60))) + 'm' if math.floor(time_elapsed / 60) > 0 else '',\n",
    "                        time_elapsed % 60\n",
    "                    ), flush=True)\n",
    "\n",
    "        \n",
    "    def evaluate(model, val_dataloader):\n",
    "        \"\"\"After the completion of each training epoch, measure the model's performance\n",
    "        on our validation set.\n",
    "        \"\"\"\n",
    "        # Put the model into the evaluation mode. The dropout layers are disabled during\n",
    "        # the test time.\n",
    "        model.eval()\n",
    "\n",
    "        # Tracking variables\n",
    "        val_accuracy = []\n",
    "        val_loss = []\n",
    "\n",
    "        # For each batch in our validation set...\n",
    "        for batch in val_dataloader:\n",
    "            # Load batch to GPU\n",
    "            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "            # Compute logits\n",
    "            with torch.no_grad():\n",
    "                logits = model(b_input_ids, b_attn_mask)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = loss_fn(logits, b_labels.float())\n",
    "            val_loss.append(loss.item())\n",
    "            \n",
    "            accuracy = accuracy_thresh(logits.view(-1,l),b_labels.view(-1,l))\n",
    "        \n",
    "            val_accuracy.append(accuracy)\n",
    "\n",
    "        # Compute the average accuracy and loss over the validation set.\n",
    "        val_loss = np.mean(val_loss)\n",
    "        val_accuracy = np.mean(val_accuracy)\n",
    "\n",
    "        return val_loss, val_accuracy\n",
    "    \n",
    "    \n",
    "    def accuracy_thresh(y_pred, y_true, thresh:float=0.5, sigmoid:bool=True):\n",
    "        \"Compute accuracy when `y_pred` and `y_true` are the same size.\"\n",
    "        if sigmoid: \n",
    "            y_pred = y_pred.sigmoid()\n",
    "        return ((y_pred>thresh)==y_true.byte()).float().mean().item()\n",
    "\n",
    "    set_seed(42)    # Set seed for reproducibility\n",
    "    bert_classifier, optimizer, scheduler = initialize_model(epochs=int(param['options']['params']['max_epochs']))\n",
    "    train(bert_classifier, train_dataloader, val_dataloader, epochs=int(param['options']['params']['max_epochs']), evaluation=True)\n",
    "    \n",
    "\n",
    "    print(tag + \"Model fine-tuning successfully finished\")\n",
    "    returns = {}\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(model,df,param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 4 - apply the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "name": "mltkc_apply"
   },
   "outputs": [],
   "source": [
    "def apply(model,df,param):\n",
    "    tag = \"-- process=apply_progress model={} max_epoch={} -- \".format(param['options']['model_name'], param['options']['params']['max_epochs'])\n",
    "    df = df.reindex(sorted(df.columns), axis=1)\n",
    "    predict_labels = list(df.columns)\n",
    "    predict_labels.remove('text')\n",
    "    l = len(predict_labels)\n",
    "    \n",
    "    MODEL_DIRECTORY = \"/srv/app/model/data/classification\"\n",
    "    MODEL_DIRECTORY = os.path.join(MODEL_DIRECTORY, param['options']['params']['lang'], param['options']['model_name'])\n",
    "    tokenizer = BertTokenizer.from_pretrained(MODEL_DIRECTORY)\n",
    "    MODEL_NAME = os.path.join(\"/srv/app/model/data/classification\", param['options']['params']['lang'], param['options']['params']['base_model'])\n",
    "    MODEL_DIRECTORY = os.path.join(\"/srv/app/model/data/classification\", param['options']['params']['lang'],param['options']['model_name'])\n",
    "    if param['options']['params']['lang'] == \"en\":\n",
    "        model = BertClassifier(\"/srv/app/model/data/classification/en/bert_classification_en\",l)\n",
    "    else:\n",
    "        model = BertClassifier(\"/srv/app/model/data/classification/jp/bert_classification_jp\",l)\n",
    "    model = model.to(device)\n",
    "    model.load_state_dict(torch.load(os.path.join(MODEL_DIRECTORY, \"pytorch_model.pt\"), map_location=torch.device(device)))\n",
    "    print(tag + \"Fine-tuned model reloaded.\")\n",
    "    model.eval()\n",
    "    \n",
    "    def text_preprocessing(text):\n",
    "        if param['options']['params']['lang'] == \"en\":\n",
    "            text = text.lower()\n",
    "            text = re.sub(r\"what's\", \"what is \", text)\n",
    "            text = re.sub(r\"won't\", \"will not \", text)\n",
    "            text = re.sub(r\"\\'s\", \" \", text)\n",
    "            text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "            text = re.sub(r\"can't\", \"can not \", text)\n",
    "            text = re.sub(r\"n't\", \" not \", text)\n",
    "            text = re.sub(r\"i'm\", \"i am \", text)\n",
    "            text = re.sub(r\"\\'re\", \" are \", text)\n",
    "            text = re.sub(r\"\\'d\", \" would \", text)\n",
    "            text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "            text = re.sub(r\"\\'scuse\", \" excuse \", text)\n",
    "            text = re.sub(r\"\\'\\n\", \" \", text)\n",
    "            text = re.sub(r\"-\", \" \", text)\n",
    "            text = re.sub(r\"\\'\\xa0\", \" \", text)\n",
    "            text = re.sub('\\s+', ' ', text)\n",
    "            text = ''.join(c for c in text if not c.isnumeric())\n",
    "            text = re.sub(r'(@.*?)[\\s]', ' ', text)\n",
    "            text = re.sub(r'&amp;', '&', text)\n",
    "            text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        else:\n",
    "            text = re.sub(r'[\\r\\t\\n\\u3000]', '', text)\n",
    "            text = text.lower()\n",
    "            text = text.strip()\n",
    "        return text\n",
    "\n",
    "    def preprocessing_for_bert(data):\n",
    "        input_ids = []\n",
    "        attention_masks = []\n",
    "        for sent in data:\n",
    "            encoded_sent = tokenizer.encode_plus(\n",
    "            text = text_preprocessing(sent),   #preprocess sentence\n",
    "            add_special_tokens = True,         #Add `[CLS]` and `[SEP]`\n",
    "            max_length= max_length_src  ,      #Max length to truncate/pad\n",
    "            pad_to_max_length = True,          #pad sentence to max length \n",
    "            return_attention_mask= True        #Return attention mask \n",
    "            )\n",
    "            # Add the outputs to the lists\n",
    "            input_ids.append(encoded_sent.get('input_ids'))\n",
    "            attention_masks.append(encoded_sent.get('attention_mask'))\n",
    "        \n",
    "        #convert lists to tensors\n",
    "        input_ids = torch.tensor(input_ids)\n",
    "        attention_masks = torch.tensor(attention_masks)\n",
    "\n",
    "        return input_ids,attention_masks\n",
    "    \n",
    "    X = df[param['feature_variables'][0]].values.tolist()\n",
    "    labels = list(df.columns)\n",
    "    labels.remove('text')\n",
    "    y = df[labels].values\n",
    "    train_inputs, train_masks = preprocessing_for_bert(X)\n",
    "    train_labels = torch.tensor(y)\n",
    "\n",
    "    \n",
    "    # Create the DataLoader for our training set\n",
    "    train_data = TensorDataset(train_inputs,train_masks, train_labels)\n",
    "    train_dataloader = DataLoader(train_data, batch_size=batch_size_train, shuffle=False)\n",
    "    \n",
    "    all_logits = []\n",
    "    total_batches = len(train_dataloader)\n",
    "    i = 0\n",
    "    for batch in train_dataloader:\n",
    "        i += 1\n",
    "        b_input_ids, b_attn_mask = tuple(t.to(device) for t in batch)[:2]\n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "        print(tag + \"finished applying {} out of {} batches\".format(i, total_batches))\n",
    "        all_logits.append(logits)\n",
    "    all_logits = torch.cat(all_logits, dim=0)\n",
    "\n",
    "    probs = all_logits.sigmoid().cpu().numpy()\n",
    "    returns = pd.DataFrame(probs,columns=predict_labels)\n",
    "    print(tag + \"apply function successfully finished\")\n",
    "\n",
    "    return returns\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns = apply(None,df,param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 5 - save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "name": "mltkc_save"
   },
   "outputs": [],
   "source": [
    "# save model to name in expected convention \"<algo_name>_<model_name>.h5\"\n",
    "def save(model, name):\n",
    "    return {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 6 - load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "name": "mltkc_load"
   },
   "outputs": [],
   "source": [
    "# load model from name in expected convention \"<algo_name>_<model_name>.h5\"\n",
    "def load(path):\n",
    "    model = {}\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 7 - provide a summary of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "name": "mltkc_summary"
   },
   "outputs": [],
   "source": [
    "# return model summary\n",
    "def summary(model=None):\n",
    "    returns = {}\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of Stages\n",
    "All subsequent cells are not tagged and can be used for further freeform code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Inference Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the scripts for LLM inference using Ollama or other LLM services.\n",
    "Sample SPL: \n",
    "* Using local Ollama LLM:\n",
    "* * | makeresults | eval text=\"Answer the question and state your reasoning.\" | fit MLTKContainer algo=llm_rag_ollama_text_processing model_name=\"llama3.2\" prompt=\"Who are you?\" text into app:llm_rag_ollama_text_processing as LLM\n",
    "* Using other LLM service:\n",
    "* * | makeresults | eval text=\"Answer the question and state your reasoning.\" | fit MLTKContainer algo=llm_rag_ollama_text_processing llm_service=azure_openai prompt=\"Who are you?\" text into app:llm_rag_ollama_text_processing as LLM "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 0 - import libraries\n",
    "At stage 0 we define all imports necessary to run our subsequent code depending on various libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "name": "mltkc_import"
   },
   "outputs": [],
   "source": [
    "# this definition exposes all python module imports that should be available in all subsequent commands\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "from llama_index.core.llms import ChatMessage\n",
    "from app.model.llm_utils import create_llm, create_embedding_model\n",
    "# ...\n",
    "# global constants\n",
    "ollama_url = \"http://ollama:11434\"\n",
    "MODEL_DIRECTORY = \"/srv/app/model/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy version: 1.22.1\n",
      "pandas version: 2.0.3\n"
     ]
    }
   ],
   "source": [
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",
    "print(\"numpy version: \" + np.__version__)\n",
    "print(\"pandas version: \" + pd.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1 - get a data sample from Splunk\n",
    "In Splunk run a search to pipe a dataset into your notebook environment. Note: mode=stage is used in the | fit command to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "name": "mltkc_stage"
   },
   "outputs": [],
   "source": [
    "# this cell is not executed from MLTK and should only be used for staging data into the notebook environment\n",
    "def stage(name):\n",
    "    with open(\"data/\"+name+\".csv\", 'r') as f:\n",
    "        df = pd.read_csv(f)\n",
    "    with open(\"data/\"+name+\".json\", 'r') as f:\n",
    "        param = json.load(f)\n",
    "    return df, param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",
    "df, param = stage(\"llm_rag_ollama_text_processing\")\n",
    "print(df.describe())\n",
    "print(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 2 - create and initialize a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "name": "mltkc_init"
   },
   "outputs": [],
   "source": [
    "# initialize your model\n",
    "# available inputs: data and parameters\n",
    "# returns the model object which will be used as a reference to call fit, apply and summary subsequently\n",
    "def init(df,param):\n",
    "    model = {}\n",
    "    model['hyperparameter'] = 42.0\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",
    "print(init(df,param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 3 - fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "name": "mltkc_fit"
   },
   "outputs": [],
   "source": [
    "# train your model\n",
    "# returns a fit info json object and may modify the model object\n",
    "def fit(model,df,param):\n",
    "    # model.fit()\n",
    "    info = {\"message\": \"model trained\"}\n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",
    "print(fit(model,df,param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 4 - apply the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "name": "mltkc_apply"
   },
   "outputs": [],
   "source": [
    "# from fit command, we will pass parameters model and prompt.\n",
    "# sample prompt: You will examine if the email content given by the user is phishing. \n",
    "#                Only output **Phishing** if the content is phishing. \n",
    "#                Only output **Legit** if the email is legitimate. Do not give extra information.\n",
    "def apply(model,df,param):\n",
    "    try:\n",
    "        X = df[\"text\"].values.tolist()\n",
    "    except:\n",
    "        cols={'Result': [\"ERROR: Please make sure you have a field in the search result named \\'text\\'\"], 'Duration': [\"ERROR\"]}\n",
    "        returns=pd.DataFrame(data=cols)\n",
    "        return returns\n",
    "\n",
    "    try:\n",
    "        prompt = param['options']['params']['prompt'].strip(\"\\\"\")\n",
    "    except:\n",
    "        cols={'Result': [\"ERROR: Please make sure you set the parameter \\'prompt\\'\"], 'Duration': [\"ERROR\"]}\n",
    "        returns=pd.DataFrame(data=cols)\n",
    "        return returns\n",
    "\n",
    "    try:\n",
    "        service = param['options']['params']['llm_service'].strip(\"\\\"\")\n",
    "        print(f\"Using {service} LLM service.\")\n",
    "    except:\n",
    "        service = \"ollama\"\n",
    "        print(\"Using default Ollama LLM service.\")\n",
    "        \n",
    "    try:\n",
    "        model_name = param['options']['params']['model_name'].strip(\"\\\"\")\n",
    "    except:\n",
    "        model_name = None\n",
    "        print(\"No model name specified\")\n",
    "\n",
    "    llm, m = create_llm(service=service, model=model_name)\n",
    "\n",
    "    if llm is None:\n",
    "        cols={'Result': [m], 'Duration': [\"ERROR\"]}\n",
    "        returns=pd.DataFrame(data=cols)\n",
    "        return returns\n",
    "\n",
    "    outputs_label = []\n",
    "    outputs_duration = []\n",
    "\n",
    "    \n",
    "    for i in range(len(X)):\n",
    "        messages = [\n",
    "            ChatMessage(role=\"user\", content=X[i]),\n",
    "            ChatMessage(role=\"user\", content=param['options']['params']['prompt'].strip(\"\\\"\")),\n",
    "        ]\n",
    "        start_time = time.time()\n",
    "\n",
    "        try:\n",
    "            response = llm.chat(messages)\n",
    "            end_time = time.time()\n",
    "            outputs_label.append(response)\n",
    "            duration = round(end_time - start_time,2)\n",
    "            duration = str(duration) + \" s\"\n",
    "            outputs_duration.append(duration)\n",
    "        except Exception as e:\n",
    "            outputs_label.append(f\"ERROR at LLM generation: {e}\")\n",
    "            outputs_duration.append(\"ERROR\")\n",
    "        \n",
    "    cols={'Result': outputs_label, 'Duration': outputs_duration}\n",
    "    returns=pd.DataFrame(data=cols)\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",
    "print(apply(model,df,param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 5 - save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "name": "mltkc_save"
   },
   "outputs": [],
   "source": [
    "# save model to name in expected convention \"<algo_name>_<model_name>\"\n",
    "def save(model,name):\n",
    "    with open(MODEL_DIRECTORY + name + \".json\", 'w') as file:\n",
    "        json.dump(model, file)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 6 - load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "name": "mltkc_load"
   },
   "outputs": [],
   "source": [
    "# load model from name in expected convention \"<algo_name>_<model_name>\"\n",
    "def load(name):\n",
    "    model = {}\n",
    "    with open(MODEL_DIRECTORY + name + \".json\", 'r') as file:\n",
    "        model = json.load(file)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 7 - provide a summary of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "name": "mltkc_summary"
   },
   "outputs": [],
   "source": [
    "# return a model summary\n",
    "def summary(model=None):\n",
    "    returns = {\"version\": {\"numpy\": np.__version__, \"pandas\": pd.__version__} }\n",
    "    return returns\n",
    "\n",
    "def compute(model,df,param):\n",
    "    try:\n",
    "        X = df[\"text\"].values.tolist()\n",
    "    except:\n",
    "        cols={'Result': [\"ERROR: Please make sure you have a field in the search result named \\'text\\'\"], 'Duration': [\"ERROR\"]}\n",
    "        returns=pd.DataFrame(data=cols)\n",
    "        return returns\n",
    "\n",
    "    try:\n",
    "        prompt = param['options']['params']['prompt'].strip(\"\\\"\")\n",
    "    except:\n",
    "        cols={'Result': [\"ERROR: Please make sure you set the parameter \\'prompt\\'\"], 'Duration': [\"ERROR\"]}\n",
    "        returns=pd.DataFrame(data=cols)\n",
    "        return returns\n",
    "\n",
    "    try:\n",
    "        service = param['options']['params']['llm_service'].strip(\"\\\"\")\n",
    "        print(f\"Using {service} LLM service.\")\n",
    "    except:\n",
    "        service = \"ollama\"\n",
    "        print(\"Using default Ollama LLM service.\")\n",
    "        \n",
    "    try:\n",
    "        model_name = param['options']['params']['model_name'].strip(\"\\\"\")\n",
    "    except:\n",
    "        model_name = None\n",
    "        print(\"No model name specified\")\n",
    "\n",
    "    llm, m = create_llm(service=service, model=model_name)\n",
    "\n",
    "    if llm is None:\n",
    "        cols={'Result': [m], 'Duration': [\"ERROR\"]}\n",
    "        returns=pd.DataFrame(data=cols)\n",
    "        return returns\n",
    "\n",
    "    outputs_label = []\n",
    "    outputs_duration = []\n",
    "\n",
    "    \n",
    "    for i in range(len(X)):\n",
    "        messages = [\n",
    "            ChatMessage(role=\"user\", content=X[i]),\n",
    "            ChatMessage(role=\"user\", content=param['options']['params']['prompt'].strip(\"\\\"\")),\n",
    "        ]\n",
    "        start_time = time.time()\n",
    "\n",
    "        try:\n",
    "            response = llm.chat(messages)\n",
    "            end_time = time.time()\n",
    "            outputs_label.append(response)\n",
    "            duration = round(end_time - start_time,2)\n",
    "            duration = str(duration) + \" s\"\n",
    "            outputs_duration.append(duration)\n",
    "        except Exception as e:\n",
    "            outputs_label.append(f\"ERROR at LLM generation: {e}\")\n",
    "            outputs_duration.append(\"ERROR\")\n",
    "        \n",
    "    cols={'Result': outputs_label, 'Duration': outputs_duration}\n",
    "    returns=pd.DataFrame(data=cols)\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After implementing your fit, apply, save and load you can train your model:<br>\n",
    "| makeresults count=10<br>\n",
    "| streamstats c as i<br>\n",
    "| eval s = i%3<br>\n",
    "| eval feature_{s}=0<br>\n",
    "| foreach feature_* [eval &lt;&lt;FIELD&gt;&gt;=random()/pow(2,31)]<br>\n",
    "| fit MLTKContainer algo=barebone s from feature_* into app:barebone_model<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or apply your model:<br>\n",
    "| makeresults count=10<br>\n",
    "| streamstats c as i<br>\n",
    "| eval s = i%3<br>\n",
    "| eval feature_{s}=0<br>\n",
    "| foreach feature_* [eval &lt;&lt;FIELD&gt;&gt;=random()/pow(2,31)]<br>\n",
    "| apply barebone_model as the_meaning_of_life"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of Stages\n",
    "All subsequent cells are not tagged and can be used for further freeform code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

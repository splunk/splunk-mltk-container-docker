{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ollama Inference Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the scripts for LLM inference using Ollama containing running on the docker host. This is the playground for testing purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 0 - import libraries\n",
    "At stage 0 we define all imports necessary to run our subsequent code depending on various libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "name": "mltkc_import"
   },
   "outputs": [],
   "source": [
    "# this definition exposes all python module imports that should be available in all subsequent commands\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "# ...\n",
    "# global constants\n",
    "ollama_url = \"http://ollama:11434\"\n",
    "MODEL_DIRECTORY = \"/srv/app/model/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy version: 1.22.1\n",
      "pandas version: 2.0.3\n"
     ]
    }
   ],
   "source": [
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",
    "print(\"numpy version: \" + np.__version__)\n",
    "print(\"pandas version: \" + pd.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1 - get a data sample from Splunk\n",
    "In Splunk run a search to pipe a dataset into your notebook environment. Note: mode=stage is used in the | fit command to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "name": "mltkc_stage"
   },
   "outputs": [],
   "source": [
    "# this cell is not executed from MLTK and should only be used for staging data into the notebook environment\n",
    "def stage(name):\n",
    "    with open(\"data/\"+name+\".csv\", 'r') as f:\n",
    "        df = pd.read_csv(f)\n",
    "    with open(\"data/\"+name+\".json\", 'r') as f:\n",
    "        param = json.load(f)\n",
    "    return df, param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                     text\n",
      "count                                                  19\n",
      "unique                                                 19\n",
      "top     software at incredibly low prices ( 86 % lower...\n",
      "freq                                                    1\n",
      "{'options': {'params': {'mode': 'stage', 'algo': 'ollama_phishing_detection', 'model': '\"llama3\"', 'prompt': '\"You will examine if the email content given by the user is phishing. Only output **Phishing** if the content is phishing. Only output **Legit** if the email is legitimate. Do not give extra information.\"'}, 'args': ['text'], 'feature_variables': ['text'], 'model_name': 'ollama_phishing_detection', 'algo_name': 'MLTKContainer', 'mlspl_limits': {'disabled': False, 'handle_new_cat': 'default', 'max_distinct_cat_values': '100', 'max_distinct_cat_values_for_classifiers': '100', 'max_distinct_cat_values_for_scoring': '100', 'max_fit_time': '60000', 'max_inputs': '100000', 'max_memory_usage_mb': '4000', 'max_model_size_mb': '30', 'max_score_time': '600', 'use_sampling': '1'}, 'kfold_cv': None}, 'feature_variables': ['text']}\n"
     ]
    }
   ],
   "source": [
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",
    "df, param = stage(\"llm_rag_ollama_text_processing\")\n",
    "print(df.describe())\n",
    "print(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 2 - create and initialize a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "name": "mltkc_init"
   },
   "outputs": [],
   "source": [
    "# initialize your model\n",
    "# available inputs: data and parameters\n",
    "# returns the model object which will be used as a reference to call fit, apply and summary subsequently\n",
    "def init(df,param):\n",
    "    model = {}\n",
    "    model['hyperparameter'] = 42.0\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",
    "print(init(df,param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 3 - fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "name": "mltkc_fit"
   },
   "outputs": [],
   "source": [
    "# train your model\n",
    "# returns a fit info json object and may modify the model object\n",
    "def fit(model,df,param):\n",
    "    # model.fit()\n",
    "    info = {\"message\": \"model trained\"}\n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",
    "print(fit(model,df,param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "def oneshot(prompt, llm):\n",
    "    uri = \"http://ollama:11434/api/chat\"\n",
    "    headers = {'Content-Type': 'application/json'}\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    \n",
    "    data = {\n",
    "        \"model\": llm,\n",
    "        \"messages\": messages,\n",
    "        \"stream\": False,\n",
    "    }\n",
    "    \n",
    "    data = json.dumps(data)\n",
    "    response = requests.post(uri, headers=headers, data=data).json()\n",
    "    return response['message']['content']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " To create a Splunk Processing Language (SPL) query that searches for the keyword \"error\" in the internal index and from the sourcetype within the last 24 hours, you can use the following search pattern:\n",
      "\n",
      "```\n",
      "index=internal sourcetype=* error\n",
      "| stats latest(_time) as last_search_time by _raw, sourcetype, index\n",
      "| where _time > now(-1d)\n",
      "| table sourcetype, index, last_search_time, _raw\n",
      "```\n",
      "\n",
      "Here's a breakdown of the query:\n",
      "\n",
      "1. `index=internal` : Specify the index to search (in this case, the internal index).\n",
      "2. `sourcetype=*` : Look for any sourcetype in this search. You can replace the asterisk (*) with the specific sourcetype if you want to focus on a particular one.\n",
      "3. `error`: Filter events that contain the keyword \"error\".\n",
      "4. `| stats latest(_time) as last_search_time by _raw, sourcetype, index` : Compute the most recent time for each event and group them by raw event, sourcetype, and index. The result is a table with columns: sourcetype, index, last_search_time (the latest time of any event in the group), and _raw (the raw event).\n",
      "5. `where _time > now(-1d)` : Only keep events that have a `last_search_time` more recent than one day ago. This effectively limits the search to the last 24 hours.\n",
      "6. `| table sourcetype, index, last_search_time, _raw` : Display the final result in a tabular format with columns: sourcetype, index, last_search_time, and raw event.\n",
      "\n",
      "You can modify this SPL query to fit your specific needs, such as adjusting the time range or filtering by other fields.\n"
     ]
    }
   ],
   "source": [
    "r = oneshot(\"Write an SPL to search _internal index and splunkd sourcetype for the last 24 hours with keyword error pipe\", 'mistral')\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is an example SPL command that searches the _internal index and the splunkd sourcetype for events containing the keyword \"error\" in the pipe field, within the last 24 hours:\n",
      "```\n",
      "index=_internal \n",
      "sourcetype=splunkd \n",
      "| where search match(\"pipe error\") \n",
      "| timechart span=1h start=-24h\n",
      "```\n",
      "Let me explain what each part of this SPL command does:\n",
      "\n",
      "* `index=_internal`: This specifies that we want to search the _internal index, which contains internal Splunk logs and metrics.\n",
      "* `sourcetype=splunkd`: This specifies that we only want to look at events with the sourcetype \"splunkd\", which is the sourcetype for Splunk's own internal logs.\n",
      "* `| where search match(\"pipe error\")`: This part of the SPL uses the `where` command to filter the results. The `search` command searches for matches to the specified string (\"pipe error\" in this case), and the `match` function checks if that string is present in the pipe field of each event.\n",
      "* `| timechart span=1h start=-24h`: This part of the SPL uses the `timechart` command to summarize the results by hour over the last 24 hours. The `span=1h` argument specifies that we want to group the results by hour, and the `start=-24h` argument specifies that we want to look at the last 24 hours.\n",
      "\n",
      "When you run this SPL command, Splunk will return a table with hourly summaries of events from the _internal index and splunkd sourcetype that contain the keyword \"error\" in their pipe field, over the last 24 hours.\n"
     ]
    }
   ],
   "source": [
    "r = oneshot(\"Write an SPL to search _internal index and splunkd sourcetype for the last 24 hours with keyword error pipe\", 'llama3')\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 4 - apply the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "name": "mltkc_apply"
   },
   "outputs": [],
   "source": [
    "# from fit command, we will pass parameters model and prompt.\n",
    "# sample prompt: You will examine if the email content given by the user is phishing. \n",
    "#                Only output **Phishing** if the content is phishing. \n",
    "#                Only output **Legit** if the email is legitimate. Do not give extra information.\n",
    "def apply(model,df,param):\n",
    "    X = df[\"text\"].values.tolist()\n",
    "    uri = f\"{ollama_url}/api/chat\"\n",
    "    headers = {'Content-Type': 'application/json'}\n",
    "    outputs_label = []\n",
    "    outputs_duration = []\n",
    "    for i in range(len(X)):\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": param['options']['params']['prompt'].strip(\"\\\"\")},\n",
    "            {\"role\": \"user\", \"content\": X[i]}\n",
    "        ]\n",
    "        \n",
    "        data = {\n",
    "            \"model\": param['options']['params']['model_name'].strip(\"\\\"\"),\n",
    "            \"messages\": messages,\n",
    "            \"stream\": False,\n",
    "        }\n",
    "        \n",
    "        data = json.dumps(data)\n",
    "        try:\n",
    "            response = requests.post(uri, headers=headers, data=data).json()\n",
    "            outputs_label.append(response['message']['content'])\n",
    "            duration = round(int(response['total_duration']) / 1000000000, 2)\n",
    "            duration = str(duration) + \" s\"\n",
    "            outputs_duration.append(duration)\n",
    "        except:\n",
    "            outputs_label.append(\"ERROR\")\n",
    "            outputs_duration.append(\"ERROR\")\n",
    "        \n",
    "    cols={'Result': outputs_label, 'Duration': outputs_duration}\n",
    "    returns=pd.DataFrame(data=cols)\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",
    "print(apply(model,df,param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 5 - save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "name": "mltkc_save"
   },
   "outputs": [],
   "source": [
    "# save model to name in expected convention \"<algo_name>_<model_name>\"\n",
    "def save(model,name):\n",
    "    with open(MODEL_DIRECTORY + name + \".json\", 'w') as file:\n",
    "        json.dump(model, file)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 6 - load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "name": "mltkc_load"
   },
   "outputs": [],
   "source": [
    "# load model from name in expected convention \"<algo_name>_<model_name>\"\n",
    "def load(name):\n",
    "    model = {}\n",
    "    with open(MODEL_DIRECTORY + name + \".json\", 'r') as file:\n",
    "        model = json.load(file)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 7 - provide a summary of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "name": "mltkc_summary"
   },
   "outputs": [],
   "source": [
    "# return a model summary\n",
    "def summary(model=None):\n",
    "    returns = {\"version\": {\"numpy\": np.__version__, \"pandas\": pd.__version__} }\n",
    "    return returns\n",
    "\n",
    "def compute(model,df,param):\n",
    "    uri = f\"{ollama_url}/api/chat\"\n",
    "    headers = {'Content-Type': 'application/json'}\n",
    "    cols = []\n",
    "    for i in range(len(df)):\n",
    "        col = {}\n",
    "        X = df[i][\"text\"]\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": param['params']['prompt'].strip(\"\\\"\")},\n",
    "            {\"role\": \"user\", \"content\": X}\n",
    "        ]\n",
    "        \n",
    "        data = {\n",
    "            \"model\": param['params']['model_name'].strip(\"\\\"\"),\n",
    "            \"messages\": messages,\n",
    "            \"stream\": False,\n",
    "        }\n",
    "        \n",
    "        data = json.dumps(data)\n",
    "        try:\n",
    "            response = requests.post(uri, headers=headers, data=data).json()\n",
    "            col['Result'] = response['message']['content']\n",
    "            duration = round(int(response['total_duration']) / 1000000000, 2)\n",
    "            duration = str(duration) + \" s\"\n",
    "            col['Duration'] = duration\n",
    "        except:\n",
    "            col['Result'] = \"ERROR\"\n",
    "            col['Duration'] = \"ERROR\"\n",
    "        cols.append(col)\n",
    "    returns=cols\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After implementing your fit, apply, save and load you can train your model:<br>\n",
    "| makeresults count=10<br>\n",
    "| streamstats c as i<br>\n",
    "| eval s = i%3<br>\n",
    "| eval feature_{s}=0<br>\n",
    "| foreach feature_* [eval &lt;&lt;FIELD&gt;&gt;=random()/pow(2,31)]<br>\n",
    "| fit MLTKContainer algo=barebone s from feature_* into app:barebone_model<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or apply your model:<br>\n",
    "| makeresults count=10<br>\n",
    "| streamstats c as i<br>\n",
    "| eval s = i%3<br>\n",
    "| eval feature_{s}=0<br>\n",
    "| foreach feature_* [eval &lt;&lt;FIELD&gt;&gt;=random()/pow(2,31)]<br>\n",
    "| apply barebone_model as the_meaning_of_life"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of Stages\n",
    "All subsequent cells are not tagged and can be used for further freeform code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

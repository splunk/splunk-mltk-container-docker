{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Inferencing with Retrieval-Augmented Generation (RAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example Usage:\n",
    "* Use Local LLM and embedding:\n",
    "* * | makeresults\n",
    "| eval query = \"How to create an ivestigation on Splunk ES?\"\n",
    "| fit MLTKContainer algo=llm_rag_script model_name=\"llama3\" embedder_name=\"all-MiniLM-L6-v2\" embedder_dimension=384 collection_name=\"document_collection_splunk\" top_k=4 rag_type=Documents query into app:llm_rag_script as RAG\n",
    "* Use Cloud based LLM and embedding:\n",
    "* * | makeresults | eval query = \"Has there been MLTKContainer errors\" | fit MLTKContainer algo=llm_rag_script vectordb_service=milvus embedder_service=azure_openai embedder_dimension=3072 llm_service=azure_openai collection_name=\"test\" top_k=3 query into app:llm_rag_script as RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 0 - import libraries\n",
    "At stage 0 we define all imports necessary to run our subsequent code depending on various libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "name": "mltkc_import"
   },
   "outputs": [],
   "source": [
    "# this definition exposes all python module imports that should be available in all subsequent commands\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import llama_index\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Document, StorageContext, ServiceContext, Settings\n",
    "from llama_index.vector_stores.milvus import MilvusVectorStore\n",
    "import textwrap\n",
    "from llama_index.core.llms import ChatMessage, MessageRole\n",
    "from llama_index.core import ChatPromptTemplate\n",
    "from app.model.llm_utils import create_llm, create_embedding_model, create_vector_db\n",
    "\n",
    "# ...\n",
    "# global constants\n",
    "MODEL_DIRECTORY = \"/srv/app/model/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some logging settings \n",
    "import logging\n",
    "import sys\n",
    "import llama_index.core\n",
    "from llama_index.core.callbacks import (\n",
    "    CallbackManager,\n",
    "    LlamaDebugHandler,\n",
    "    CBEventType,\n",
    ")\n",
    "\n",
    "llama_index.core.set_global_handler(\"simple\")\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "llama_debug = LlamaDebugHandler(print_trace_on_end=True)\n",
    "callback_manager = CallbackManager([llama_debug])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy version: 1.26.4\n",
      "pandas version: 2.2.0\n"
     ]
    }
   ],
   "source": [
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",
    "print(\"numpy version: \" + np.__version__)\n",
    "print(\"pandas version: \" + pd.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "name": "mltkc_stage"
   },
   "outputs": [],
   "source": [
    "# this cell is not executed from MLTK and should only be used for staging data into the notebook environment\n",
    "def stage(name):\n",
    "    with open(\"data/\"+name+\".csv\", 'r') as f:\n",
    "        df = pd.read_csv(f)\n",
    "    with open(\"data/\"+name+\".json\", 'r') as f:\n",
    "        param = json.load(f)\n",
    "    return df, param"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 2 - create and initialize a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "name": "mltkc_init"
   },
   "outputs": [],
   "source": [
    "# initialize your model\n",
    "# available inputs: data and parameters\n",
    "# returns the model object which will be used as a reference to call fit, apply and summary subsequently\n",
    "def init(df,param):\n",
    "    model = {}\n",
    "    model['hyperparameter'] = 42.0\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 3 - fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "name": "mltkc_fit"
   },
   "outputs": [],
   "source": [
    "# train your model\n",
    "# returns a fit info json object and may modify the model object\n",
    "def fit(model,df,param):\n",
    "    info = {\"message\": \"model trained\"}\n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'message': 'model trained'}\n"
     ]
    }
   ],
   "source": [
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",
    "print(fit(model,df,param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 4 - apply the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "name": "mltkc_apply"
   },
   "outputs": [],
   "source": [
    "def apply(model,df,param):\n",
    "    X = df[\"query\"].values.tolist()\n",
    "\n",
    "    # In the previous version, Logs was used as d_type for data encoded via Pymilvus\n",
    "    # In this updated version, both document data and Splunk index data are encoded via llama-index. Therefore, the Logs option is legacy.\n",
    "    # Manually change d_type to Logs to utilize the legacy code remained in this notebook.\n",
    "    d_type = \"Documents\"\n",
    "\n",
    "    try:\n",
    "        vec_service = param['options']['params']['vectordb_service'].strip('\\\"')\n",
    "        print(f\"Using {vec_service} vector database service\")\n",
    "    except:\n",
    "        vec_service = \"milvus\"\n",
    "        print(\"Using default Milvus vector database service\")\n",
    "        \n",
    "    try:\n",
    "        embedder_service = param['options']['params']['embedder_service'].strip('\\\"')\n",
    "        print(f\"Using {embedder_service} embedding service\")\n",
    "    except:\n",
    "        embedder_service = \"huggingface\"\n",
    "        print(\"Using default Huggingface embedding service\")\n",
    "\n",
    "    try:\n",
    "        llm_service = param['options']['params']['llm_service'].strip(\"\\\"\")\n",
    "        print(f\"Using {llm_service} LLM service.\")\n",
    "    except:\n",
    "        llm_service = \"ollama\"\n",
    "        print(\"Using default Ollama LLM service.\")\n",
    "\n",
    "    try:\n",
    "        use_local= int(param['options']['params']['use_local'])\n",
    "    except:\n",
    "        use_local = 0\n",
    "        print(\"Not using embedding local model\") \n",
    "            \n",
    "    try:\n",
    "        embedder_name=param['options']['params']['embedder_name'].strip('\\\"')\n",
    "    except:\n",
    "        embedder_name = None\n",
    "        print(\"Embedding model name not specified\") \n",
    "    \n",
    "    try:\n",
    "        embedder_dimension=int(param['options']['params']['embedder_dimension'])\n",
    "    except:\n",
    "        embedder_dimension=None\n",
    "        print(\"Embedding dimension not specified\") \n",
    "\n",
    "    try:\n",
    "        model_name = param['options']['params']['model_name'].strip(\"\\\"\")\n",
    "    except:\n",
    "        model_name = None\n",
    "        print(\"LLM model name not specified\")\n",
    "\n",
    "    try:\n",
    "        embedder, output_dims, m = create_embedding_model(service=embedder_service, model=embedder_name, use_local=use_local)\n",
    "        if embedder is not None:\n",
    "            print(m)\n",
    "        else:\n",
    "            cols = {\"Results\": [f\"ERROR in embedding model loading: {m}. \"]}\n",
    "            returns = pd.DataFrame(data=cols)\n",
    "            return returns\n",
    "        if output_dims:\n",
    "            embedder_dimension = output_dims\n",
    "    except Exception as e:\n",
    "        cols = {\"Results\": [f\"Failed to initiate embedding model. ERROR: {e}\"]}\n",
    "        returns = pd.DataFrame(data=cols)\n",
    "        return returns\n",
    "\n",
    "    \n",
    "    llm, m = create_llm(service=llm_service, model=model_name)\n",
    "\n",
    "    if llm is None:\n",
    "        cols={'Result': [m]}\n",
    "        returns=pd.DataFrame(data=cols)\n",
    "        return returns\n",
    "\n",
    "    try:\n",
    "        collection_name = param['options']['params']['collection_name'].strip('\\\"')\n",
    "    except:\n",
    "        cols = {\"Response\": [\"ERROR: no collection specified. Please specify a vectorDB collection\"], \"References\": [\"None\"]}\n",
    "        result = pd.DataFrame(data=cols)\n",
    "        return result\n",
    "    \n",
    "    try:\n",
    "        top_k = int(param['options']['params']['top_k'])\n",
    "    except:\n",
    "        top_k = 5\n",
    "        print(\"Using top 5 results by default\")\n",
    "        \n",
    "    if d_type == \"Documents\":\n",
    "        qa_prompt_str = (\n",
    "            \"Below are the context information.\\n\"\n",
    "            \"---------------------\\n\"\n",
    "            \"{context_str}\\n\"\n",
    "            \"---------------------\\n\"\n",
    "            \"Given the context information as well as necessary prior knowledge, \"\n",
    "            \"answer the question: {query_str}\\n\"\n",
    "        )\n",
    "        chat_text_qa_msgs = [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"You are an expert Q&A system that is trusted around the world. Always answer the query using the provided context information and reasoning as detailed as possible\",\n",
    "            ),\n",
    "            (\"user\", qa_prompt_str),\n",
    "        ]\n",
    "    else:\n",
    "        qa_prompt_str = (\n",
    "            \"Past log messages below are given as context information.\\n\"\n",
    "            \"---------------------\\n\"\n",
    "            \"{context_str}\\n\"\n",
    "            \"---------------------\\n\"\n",
    "            \"Given the context information as well as necessary prior knowledge, \"\n",
    "            \"answer the question: {query_str}\\n\"\n",
    "        )\n",
    "        chat_text_qa_msgs = [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"You are an expert Q&A system that is trusted around the world. Always answer the query using the provided context information and reasoning as detailed as possible\",\n",
    "            ),\n",
    "            (\"user\", qa_prompt_str),\n",
    "        ]\n",
    "    \n",
    "    text_qa_template = ChatPromptTemplate.from_messages(chat_text_qa_msgs)\n",
    "\n",
    "\n",
    "    try:\n",
    "        Settings.llm = llm\n",
    "        Settings.embed_model = embedder\n",
    "        Settings.chunk_size = 1024\n",
    "    except Exception as e:\n",
    "        cols = {\"Response\": [f\"Could not load LLM or embedder. ERROR: {e}\"], \"References\": [\"ERROR\"]}\n",
    "        result = pd.DataFrame(data=cols)\n",
    "        return result\n",
    "    try:\n",
    "        if d_type == \"Documents\":\n",
    "            vector_store, v_m = create_vector_db(service=vec_service, collection_name=collection_name, dim=embedder_dimension)\n",
    "            if vector_store is None:\n",
    "                cols = {\"Response\": [f\"ERROR: Could not connect to vectordb. ERROR: {v_m}\"], \"References\": [\"ERROR\"]}\n",
    "                result = pd.DataFrame(data=cols)\n",
    "                return result\n",
    "        else:\n",
    "            vector_store = MilvusVectorStore(uri=MILVUS_ENDPOINT, token=\"\", collection_name=collection_name, embedding_field='embeddings', text_key='label', dim=embedder_dimension, overwrite=False)\n",
    "        index = VectorStoreIndex.from_vector_store(\n",
    "           vector_store=vector_store\n",
    "        )\n",
    "        query_engine = index.as_query_engine(similarity_top_k=top_k, text_qa_template=text_qa_template)\n",
    "    except Exception as e:\n",
    "        cols = {\"Response\": [f\"ERROR: Could not load collection. ERROR: {e}\"], \"References\": [\"ERROR\"]}\n",
    "        result = pd.DataFrame(data=cols)\n",
    "        return result\n",
    "        \n",
    "    l = []\n",
    "    f = []\n",
    "    try:\n",
    "        for i in range(len(X)):\n",
    "            r = query_engine.query(X[i])\n",
    "            l.append(r.response)\n",
    "            if d_type == \"Documents\":\n",
    "                files = \"\"\n",
    "                for node in r.source_nodes:\n",
    "                    files += str(node.node.metadata)\n",
    "                    files += \"\\n\"\n",
    "                    files += node.text\n",
    "                    files += \"\\n\"\n",
    "                f.append(files)\n",
    "            else:\n",
    "                logs = \"\"\n",
    "                for i in range(len(r.source_nodes)):\n",
    "                    logs += r.source_nodes[0].text\n",
    "                    logs += \"\\n\"\n",
    "                f.append(logs)  \n",
    "    except Exception as e:\n",
    "        cols = {\"Response\": [f\"Failed at querying. ERROR: {e}. Please check if the knowledge type is correct\"], \"References\": [\"None\"]}\n",
    "        result = pd.DataFrame(data=cols)\n",
    "        return result\n",
    "    \n",
    "    cols = {\"Response\": l, \"References\": f}\n",
    "    result = pd.DataFrame(data=cols)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",
    "print(apply(model,df,param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 5 - save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "name": "mltkc_save"
   },
   "outputs": [],
   "source": [
    "# save model to name in expected convention \"<algo_name>_<model_name>\"\n",
    "def save(model,name):\n",
    "    with open(MODEL_DIRECTORY + name + \".json\", 'w') as file:\n",
    "        json.dump(model, file)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 6 - load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "name": "mltkc_load"
   },
   "outputs": [],
   "source": [
    "# load model from name in expected convention \"<algo_name>_<model_name>\"\n",
    "def load(name):\n",
    "    model = {}\n",
    "    with open(MODEL_DIRECTORY + name + \".json\", 'r') as file:\n",
    "        model = json.load(file)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 7 - provide a summary of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "deletable": false,
    "name": "mltkc_summary"
   },
   "outputs": [],
   "source": [
    "# return a model summary\n",
    "def summary(model=None):\n",
    "    returns = {\"version\": {\"numpy\": np.__version__, \"pandas\": pd.__version__} }\n",
    "    return returns\n",
    "\n",
    "def compute(model,df,param):\n",
    "    X = df[\"query\"].values.tolist()\n",
    "\n",
    "    # In the previous version, Logs was used as d_type for data encoded via Pymilvus\n",
    "    # In this updated version, both document data and Splunk index data are encoded via llama-index. Therefore, the Logs option is legacy.\n",
    "    # Manually change d_type to Logs to utilize the legacy code remained in this notebook.\n",
    "    d_type = \"Documents\"\n",
    "\n",
    "    try:\n",
    "        vec_service = param['options']['params']['vectordb_service'].strip('\\\"')\n",
    "        print(f\"Using {vec_service} vector database service\")\n",
    "    except:\n",
    "        vec_service = \"milvus\"\n",
    "        print(\"Using default Milvus vector database service\")\n",
    "\n",
    "    try:\n",
    "        embedder_service = param['options']['params']['embedder_service'].strip('\\\"')\n",
    "        print(f\"Using {embedder_service} embedding service\")\n",
    "    except:\n",
    "        embedder_service = \"huggingface\"\n",
    "        print(\"Using default Huggingface embedding service\")\n",
    "\n",
    "    try:\n",
    "        llm_service = param['options']['params']['llm_service'].strip(\"\\\"\")\n",
    "        print(f\"Using {llm_service} LLM service.\")\n",
    "    except:\n",
    "        llm_service = \"ollama\"\n",
    "        print(\"Using default Ollama LLM service.\")\n",
    "\n",
    "    try:\n",
    "        use_local= int(param['options']['params']['use_local'])\n",
    "    except:\n",
    "        use_local = 0\n",
    "        print(\"Not using embedding local model\") \n",
    "            \n",
    "    try:\n",
    "        embedder_name=param['options']['params']['embedder_name'].strip('\\\"')\n",
    "    except:\n",
    "        embedder_name = None\n",
    "        print(\"Embedding model name not specified\") \n",
    "    \n",
    "    try:\n",
    "        embedder_dimension=int(param['options']['params']['embedder_dimension'])\n",
    "    except:\n",
    "        embedder_dimension=None\n",
    "        print(\"Embedding dimension not specified\") \n",
    "\n",
    "    try:\n",
    "        model_name = param['options']['params']['model_name'].strip(\"\\\"\")\n",
    "    except:\n",
    "        model_name = None\n",
    "        print(\"LLM model name not specified\")\n",
    "\n",
    "    try:\n",
    "        embedder, output_dims, m = create_embedding_model(service=embedder_service, model=embedder_name, use_local=use_local)\n",
    "        if embedder is not None:\n",
    "            print(m)\n",
    "        else:\n",
    "            cols = {\"Results\": [f\"ERROR in embedding model loading: {m}. \"]}\n",
    "            returns = pd.DataFrame(data=cols)\n",
    "            return returns\n",
    "        if output_dims:\n",
    "            embedder_dimension = output_dims\n",
    "    except Exception as e:\n",
    "        cols = {\"Results\": [f\"Failed to initiate embedding model. ERROR: {e}\"]}\n",
    "        returns = pd.DataFrame(data=cols)\n",
    "        return returns\n",
    "\n",
    "    \n",
    "    llm, m = create_llm(service=llm_service, model=model_name)\n",
    "\n",
    "    if llm is None:\n",
    "        cols={'Result': [m]}\n",
    "        returns=pd.DataFrame(data=cols)\n",
    "        return returns\n",
    "\n",
    "    try:\n",
    "        collection_name = param['options']['params']['collection_name'].strip('\\\"')\n",
    "    except:\n",
    "        cols = {\"Response\": [\"ERROR: no collection specified. Please specify a vectorDB collection\"], \"References\": [\"None\"]}\n",
    "        result = pd.DataFrame(data=cols)\n",
    "        return result\n",
    "    \n",
    "    try:\n",
    "        top_k = int(param['options']['params']['top_k'])\n",
    "    except:\n",
    "        top_k = 5\n",
    "        print(\"Using top 5 results by default\")\n",
    "        \n",
    "    if d_type == \"Documents\":\n",
    "        qa_prompt_str = (\n",
    "            \"Below are the context information.\\n\"\n",
    "            \"---------------------\\n\"\n",
    "            \"{context_str}\\n\"\n",
    "            \"---------------------\\n\"\n",
    "            \"Given the context information as well as necessary prior knowledge, \"\n",
    "            \"answer the question: {query_str}\\n\"\n",
    "        )\n",
    "        chat_text_qa_msgs = [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"You are an expert Q&A system that is trusted around the world. Always answer the query using the provided context information and reasoning as detailed as possible\",\n",
    "            ),\n",
    "            (\"user\", qa_prompt_str),\n",
    "        ]\n",
    "    else:\n",
    "        qa_prompt_str = (\n",
    "            \"Past log messages below are given as context information.\\n\"\n",
    "            \"---------------------\\n\"\n",
    "            \"{context_str}\\n\"\n",
    "            \"---------------------\\n\"\n",
    "            \"Given the context information as well as necessary prior knowledge, \"\n",
    "            \"answer the question: {query_str}\\n\"\n",
    "        )\n",
    "        chat_text_qa_msgs = [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"You are an expert Q&A system that is trusted around the world. Always answer the query using the provided context information and reasoning as detailed as possible\",\n",
    "            ),\n",
    "            (\"user\", qa_prompt_str),\n",
    "        ]\n",
    "    \n",
    "    text_qa_template = ChatPromptTemplate.from_messages(chat_text_qa_msgs)\n",
    "\n",
    "\n",
    "    try:\n",
    "        Settings.llm = llm\n",
    "        Settings.embed_model = embedder\n",
    "        Settings.chunk_size = 1024\n",
    "    except Exception as e:\n",
    "        cols = {\"Response\": [f\"Could not load LLM or embedder. ERROR: {e}\"], \"References\": [\"ERROR\"]}\n",
    "        result = pd.DataFrame(data=cols)\n",
    "        return result\n",
    "    try:\n",
    "        if d_type == \"Documents\":\n",
    "            vector_store, v_m = create_vector_db(service=vec_service, collection_name=collection_name, dim=embedder_dimension)\n",
    "            if vector_store is None:\n",
    "                cols = {\"Response\": [f\"ERROR: Could not connect to vectordb. ERROR: {v_m}\"], \"References\": [\"ERROR\"]}\n",
    "                result = pd.DataFrame(data=cols)\n",
    "                return result\n",
    "        else:\n",
    "            vector_store = MilvusVectorStore(uri=MILVUS_ENDPOINT, token=\"\", collection_name=collection_name, embedding_field='embeddings', text_key='label', dim=embedder_dimension, overwrite=False)\n",
    "        index = VectorStoreIndex.from_vector_store(\n",
    "           vector_store=vector_store\n",
    "        )\n",
    "        query_engine = index.as_query_engine(similarity_top_k=top_k, text_qa_template=text_qa_template)\n",
    "    except Exception as e:\n",
    "        cols = {\"Response\": [f\"ERROR: Could not load collection. ERROR: {e}\"], \"References\": [\"ERROR\"]}\n",
    "        result = pd.DataFrame(data=cols)\n",
    "        return result\n",
    "        \n",
    "    l = []\n",
    "    f = []\n",
    "    try:\n",
    "        for i in range(len(X)):\n",
    "            r = query_engine.query(X[i])\n",
    "            l.append(r.response)\n",
    "            if d_type == \"Documents\":\n",
    "                files = \"\"\n",
    "                for node in r.source_nodes:\n",
    "                    files += str(node.node.metadata)\n",
    "                    files += \"\\n\"\n",
    "                    files += node.text\n",
    "                    files += \"\\n\"\n",
    "                f.append(files)\n",
    "            else:\n",
    "                logs = \"\"\n",
    "                for i in range(len(r.source_nodes)):\n",
    "                    logs += r.source_nodes[0].text\n",
    "                    logs += \"\\n\"\n",
    "                f.append(logs)  \n",
    "    except Exception as e:\n",
    "        cols = {\"Response\": [f\"Failed at querying. ERROR: {e}. Please check if the knowledge type is correct\"], \"References\": [\"None\"]}\n",
    "        result = pd.DataFrame(data=cols)\n",
    "        return result\n",
    "    \n",
    "    cols = {\"Response\": l, \"References\": f}\n",
    "    result = pd.DataFrame(data=cols)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After implementing your fit, apply, save and load you can train your model:<br>\n",
    "| makeresults count=10<br>\n",
    "| streamstats c as i<br>\n",
    "| eval s = i%3<br>\n",
    "| eval feature_{s}=0<br>\n",
    "| foreach feature_* [eval &lt;&lt;FIELD&gt;&gt;=random()/pow(2,31)]<br>\n",
    "| fit MLTKContainer algo=barebone s from feature_* into app:barebone_model<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or apply your model:<br>\n",
    "| makeresults count=10<br>\n",
    "| streamstats c as i<br>\n",
    "| eval s = i%3<br>\n",
    "| eval feature_{s}=0<br>\n",
    "| foreach feature_* [eval &lt;&lt;FIELD&gt;&gt;=random()/pow(2,31)]<br>\n",
    "| apply barebone_model as the_meaning_of_life"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Send data back to Splunk HEC\n",
    "When you configured the Splunk HEC Settings in the DSDL app you can easily send back data to an index with [Splunk's HTTP Event Collector (HEC)](https://docs.splunk.com/Documentation/Splunk/latest/Data/UsetheHTTPEventCollector). Read more about data formats and options in the [documentation](https://docs.splunk.com/Documentation/Splunk/latest/Data/FormateventsforHTTPEventCollector#Event_metadata).\n",
    "\n",
    "### Use cases\n",
    "- you want to offload longer running, possibly distributed computations that need to deliver results asynchroneously back into Splunk. \n",
    "- you might not want to present results back into the search pipeline after your `| fit` or `| apply` command. \n",
    "- you can easily utilize this approach for any logging purposes or other profiling tasks in your ML code so you can actively monitor and analyze your processes.\n",
    "\n",
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dsdlsupport import SplunkHEC as SplunkHEC\n",
    "hec = SplunkHEC.SplunkHEC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example to send 10 hello world events\n",
    "response = hec.send_hello_world(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HEC endpoint http://host.docker.internal:8088/services/collector/event \n",
      "returned with status code 200 \n",
      "and response message: {\"text\":\"Success\",\"code\":0}\n"
     ]
    }
   ],
   "source": [
    "print(\"HEC endpoint %s \\nreturned with status code %s \\nand response message: %s\" % (response.url, response.status_code, response.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example to send a JSON object, e.g. to log some data\n",
    "from datetime import datetime\n",
    "response = hec.send({'event': {'message': 'operation done', 'log_level': 'INFO' }, 'time': datetime.now().timestamp()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HEC endpoint http://host.docker.internal:8088/services/collector/event \n",
      "returned with status code 200 \n",
      "and response message: {\"text\":\"Success\",\"code\":0}\n"
     ]
    }
   ],
   "source": [
    "print(\"HEC endpoint %s \\nreturned with status code %s \\nand response message: %s\" % (response.url, response.status_code, response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of Stages\n",
    "All subsequent cells are not tagged and can be used for further freeform code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

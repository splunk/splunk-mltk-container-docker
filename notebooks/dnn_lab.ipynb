{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splunk App for Data Science and Deep Learning - Notebook Template "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains a barebone example workflow how to work on custom containerized code that seamlessly interfaces with you Splunk platform by utilizing the Splunk App for Data Science and Deep Learning (DSDL) - formerly known as the Deep Learning Toolkit for Splunk (DLTK). Find more examples and information in the app and on the [DSDL splunkbase page](https://splunkbase.splunk.com/app/4607/#/details)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: By default every time you save this notebook the stages main cells are exported into a python module which can then get invoked by Splunk's MLTK SPL commands like <code> | fit ... | apply ... | summary </code>. Please read the Model Development Guide in the DSDL app for more information about this workflow.\n",
    "\n",
    "| fit MLTKContainer algo=dnn_lab num_hidden_layers=3 activation_name=ReLU dropout_rate=0.7 optimizer_name=Adam learning_rate=0.01 * into app:dnn_lab\n",
    "\n",
    "| inputlookup dnn_lab_data.csv\n",
    "| eval is_train = 100\n",
    "| head 1000\n",
    "| apply app:dnn_lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 0 - import libraries\n",
    "At stage 0 we define all imports necessary to run our subsequent code depending on various libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "deletable": false,
    "name": "mltkc_import"
   },
   "outputs": [],
   "source": [
    "# this definition exposes all python module imports that should be available in all subsequent commands\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import time\n",
    "\n",
    "# Custom Lion Optimizer\n",
    "class Lion(optim.Optimizer):\n",
    "    def __init__(self, params, lr=1e-4, betas=(0.9, 0.99), weight_decay=0.1):\n",
    "        defaults = dict(lr=lr, betas=betas, weight_decay=weight_decay)\n",
    "        super(Lion, self).__init__(params, defaults)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data\n",
    "                state = self.state[p]\n",
    "\n",
    "                # State initialization\n",
    "                if len(state) == 0:\n",
    "                    state['exp_avg'] = torch.zeros_like(p.data)\n",
    "\n",
    "                exp_avg = state['exp_avg']\n",
    "                beta1, beta2 = group['betas']\n",
    "\n",
    "                # Update momentum\n",
    "                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n",
    "\n",
    "                # Compute update\n",
    "                update = exp_avg.clone().mul_(beta2).add_(grad, alpha=1 - beta2).sign_()\n",
    "\n",
    "                # Weight decay\n",
    "                if group['weight_decay'] != 0:\n",
    "                    update.add_(p.data, alpha=group['weight_decay'])\n",
    "\n",
    "                # Update parameters\n",
    "                p.data.add_(update, alpha=-group['lr'])\n",
    "\n",
    "        return loss\n",
    "\n",
    "# Define the DNN model\n",
    "class SimpleDNN(nn.Module):\n",
    "    def __init__(self, input_dim, num_hidden_layers, nodes_per_layer, activation_func, dropout_rate):\n",
    "        super(SimpleDNN, self).__init__()\n",
    "        layers = []\n",
    "        # Input layer\n",
    "        layers.append(nn.Linear(input_dim, nodes_per_layer))\n",
    "        layers.append(activation_func)\n",
    "        if dropout_rate > 0:\n",
    "            layers.append(nn.Dropout(dropout_rate))\n",
    "\n",
    "        # Hidden layers\n",
    "        for _ in range(num_hidden_layers - 1):\n",
    "            layers.append(nn.Linear(nodes_per_layer, nodes_per_layer))\n",
    "            layers.append(activation_func)\n",
    "            if dropout_rate > 0:\n",
    "                layers.append(nn.Dropout(dropout_rate))\n",
    "\n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(nodes_per_layer, 2))  # Binary classification\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "# global constants\n",
    "nodes_per_layer = 64\n",
    "batch_size = 512\n",
    "class_weight = 1\n",
    "num_epochs = 10\n",
    "MODEL_DIRECTORY = \"/srv/app/model/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy version: 1.26.4\n",
      "pandas version: 2.2.3\n"
     ]
    }
   ],
   "source": [
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",
    "print(\"numpy version: \" + np.__version__)\n",
    "print(\"pandas version: \" + pd.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To stage the data with correct default parameters use this search:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| inputlookup dnn_lab_split.csv\n",
    "| fit MLTKContainer mode=stage num_hidden_layers=5 activation_name=\"Tanh\" dropout_rate=0.0 learning_rate=0.01 optimizer_name=\"Adam\" algo=dnn_lab * into app:dnn_lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you run this search your data set sample is available as a csv inside the container to develop your model. The name is taken from the into keyword (\"dnn_lab\" in the example above) or set to \"default\" if no into keyword is present. This step is intended to work with a subset of your data to create your custom model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "deletable": false,
    "name": "mltkc_stage"
   },
   "outputs": [],
   "source": [
    "# this cell is not executed from MLTK and should only be used for staging data into the notebook environment\n",
    "def stage(name):\n",
    "    with open(\"data/\"+name+\".csv\", 'r') as f:\n",
    "        df = pd.read_csv(f)\n",
    "    with open(\"data/\"+name+\".json\", 'r') as f:\n",
    "        param = json.load(f)\n",
    "    return df, param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",
    "df, param = stage(\"dnn_lab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SS_ct_srv_src</th>\n",
       "      <th>SS_dur</th>\n",
       "      <th>SS_rate</th>\n",
       "      <th>SS_sbytes</th>\n",
       "      <th>SS_sinpkt</th>\n",
       "      <th>SS_sjit</th>\n",
       "      <th>SS_sloss</th>\n",
       "      <th>SS_spkts</th>\n",
       "      <th>SS_sttl</th>\n",
       "      <th>label</th>\n",
       "      <th>is_train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.852710</td>\n",
       "      <td>-0.188279</td>\n",
       "      <td>-0.538636</td>\n",
       "      <td>-0.047235</td>\n",
       "      <td>-0.146543</td>\n",
       "      <td>-0.054983</td>\n",
       "      <td>-0.049680</td>\n",
       "      <td>-0.087088</td>\n",
       "      <td>-0.938280</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.581366</td>\n",
       "      <td>-0.221547</td>\n",
       "      <td>-0.513440</td>\n",
       "      <td>-0.048464</td>\n",
       "      <td>-0.150094</td>\n",
       "      <td>-0.092147</td>\n",
       "      <td>-0.072361</td>\n",
       "      <td>-0.119896</td>\n",
       "      <td>-1.226106</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.171630</td>\n",
       "      <td>-0.221774</td>\n",
       "      <td>1.629959</td>\n",
       "      <td>-0.049813</td>\n",
       "      <td>-0.150149</td>\n",
       "      <td>-0.092163</td>\n",
       "      <td>-0.072361</td>\n",
       "      <td>-0.130831</td>\n",
       "      <td>0.844383</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.647842</td>\n",
       "      <td>-0.094751</td>\n",
       "      <td>-0.538947</td>\n",
       "      <td>-0.047226</td>\n",
       "      <td>-0.136473</td>\n",
       "      <td>0.051424</td>\n",
       "      <td>-0.049680</td>\n",
       "      <td>-0.087088</td>\n",
       "      <td>-0.938280</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.238106</td>\n",
       "      <td>-0.210221</td>\n",
       "      <td>-0.538123</td>\n",
       "      <td>-0.049294</td>\n",
       "      <td>-0.147918</td>\n",
       "      <td>-0.060849</td>\n",
       "      <td>-0.061020</td>\n",
       "      <td>-0.108960</td>\n",
       "      <td>-1.226106</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>-0.033238</td>\n",
       "      <td>-0.221773</td>\n",
       "      <td>0.183947</td>\n",
       "      <td>-0.049813</td>\n",
       "      <td>-0.150148</td>\n",
       "      <td>-0.092163</td>\n",
       "      <td>-0.072361</td>\n",
       "      <td>-0.130831</td>\n",
       "      <td>0.844383</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>-0.647842</td>\n",
       "      <td>-0.221654</td>\n",
       "      <td>-0.518401</td>\n",
       "      <td>-0.050116</td>\n",
       "      <td>-0.150148</td>\n",
       "      <td>-0.092163</td>\n",
       "      <td>-0.072361</td>\n",
       "      <td>-0.130831</td>\n",
       "      <td>-1.226106</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>-0.033238</td>\n",
       "      <td>-0.221774</td>\n",
       "      <td>5.967993</td>\n",
       "      <td>-0.049813</td>\n",
       "      <td>-0.150149</td>\n",
       "      <td>-0.092163</td>\n",
       "      <td>-0.072361</td>\n",
       "      <td>-0.130831</td>\n",
       "      <td>0.844383</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>-0.852710</td>\n",
       "      <td>-0.149120</td>\n",
       "      <td>-0.538817</td>\n",
       "      <td>-0.016404</td>\n",
       "      <td>-0.144734</td>\n",
       "      <td>-0.026683</td>\n",
       "      <td>-0.026999</td>\n",
       "      <td>-0.065217</td>\n",
       "      <td>-0.000525</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>-0.442974</td>\n",
       "      <td>-0.150594</td>\n",
       "      <td>-0.537972</td>\n",
       "      <td>0.110562</td>\n",
       "      <td>-0.148798</td>\n",
       "      <td>-0.071597</td>\n",
       "      <td>0.131766</td>\n",
       "      <td>0.142562</td>\n",
       "      <td>-1.226106</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      SS_ct_srv_src    SS_dur   SS_rate  SS_sbytes  SS_sinpkt   SS_sjit  \\\n",
       "0         -0.852710 -0.188279 -0.538636  -0.047235  -0.146543 -0.054983   \n",
       "1          0.581366 -0.221547 -0.513440  -0.048464  -0.150094 -0.092147   \n",
       "2          0.171630 -0.221774  1.629959  -0.049813  -0.150149 -0.092163   \n",
       "3         -0.647842 -0.094751 -0.538947  -0.047226  -0.136473  0.051424   \n",
       "4         -0.238106 -0.210221 -0.538123  -0.049294  -0.147918 -0.060849   \n",
       "...             ...       ...       ...        ...        ...       ...   \n",
       "9995      -0.033238 -0.221773  0.183947  -0.049813  -0.150148 -0.092163   \n",
       "9996      -0.647842 -0.221654 -0.518401  -0.050116  -0.150148 -0.092163   \n",
       "9997      -0.033238 -0.221774  5.967993  -0.049813  -0.150149 -0.092163   \n",
       "9998      -0.852710 -0.149120 -0.538817  -0.016404  -0.144734 -0.026683   \n",
       "9999      -0.442974 -0.150594 -0.537972   0.110562  -0.148798 -0.071597   \n",
       "\n",
       "      SS_sloss  SS_spkts   SS_sttl  label  is_train  \n",
       "0    -0.049680 -0.087088 -0.938280      1         1  \n",
       "1    -0.072361 -0.119896 -1.226106      0         1  \n",
       "2    -0.072361 -0.130831  0.844383      1         1  \n",
       "3    -0.049680 -0.087088 -0.938280      0         1  \n",
       "4    -0.061020 -0.108960 -1.226106      0         1  \n",
       "...        ...       ...       ...    ...       ...  \n",
       "9995 -0.072361 -0.130831  0.844383      1         0  \n",
       "9996 -0.072361 -0.130831 -1.226106      0         0  \n",
       "9997 -0.072361 -0.130831  0.844383      1         0  \n",
       "9998 -0.026999 -0.065217 -0.000525      1         0  \n",
       "9999  0.131766  0.142562 -1.226106      0         0  \n",
       "\n",
       "[10000 rows x 11 columns]"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'options': {'params': {'mode': 'stage',\n",
       "   'num_hidden_layers': '5',\n",
       "   'activation_name': '\"Tanh\"',\n",
       "   'dropout_rate': '0.0',\n",
       "   'learning_rate': '0.01',\n",
       "   'optimizer_name': '\"Adam\"',\n",
       "   'algo': 'dnn_lab'},\n",
       "  'args': ['*'],\n",
       "  'feature_variables': ['*'],\n",
       "  'model_name': 'dnn_lab',\n",
       "  'algo_name': 'MLTKContainer',\n",
       "  'mlspl_limits': {'disabled': False,\n",
       "   'handle_new_cat': 'default',\n",
       "   'max_distinct_cat_values': '100',\n",
       "   'max_distinct_cat_values_for_classifiers': '100',\n",
       "   'max_distinct_cat_values_for_scoring': '100',\n",
       "   'max_fit_time': '6000',\n",
       "   'max_inputs': '1000000',\n",
       "   'max_memory_usage_mb': '16000',\n",
       "   'max_model_size_mb': '3000',\n",
       "   'max_score_time': '6000',\n",
       "   'use_sampling': '1'},\n",
       "  'kfold_cv': None},\n",
       " 'feature_variables': ['SS_ct_srv_src',\n",
       "  'SS_dur',\n",
       "  'SS_rate',\n",
       "  'SS_sbytes',\n",
       "  'SS_sinpkt',\n",
       "  'SS_sjit',\n",
       "  'SS_sloss',\n",
       "  'SS_spkts',\n",
       "  'SS_sttl',\n",
       "  'label',\n",
       "  'is_train']}"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 2 - create and initialize a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "deletable": false,
    "name": "mltkc_init"
   },
   "outputs": [],
   "source": [
    "# initialize your model\n",
    "# available inputs: data and parameters\n",
    "# returns the model object which will be used as a reference to call fit, apply and summary subsequently\n",
    "def init(df,param):\n",
    "    model = {}\n",
    "    input_dim = len(df.columns)-2 #remove training and flag field in input dimensionality\n",
    "    num_hidden_layers = int(param['options']['params']['num_hidden_layers'])\n",
    "    activation_name = param['options']['params']['activation_name'].strip('\\\"')\n",
    "    # Map activation functions\n",
    "    activation_mapping = {\n",
    "        'ReLU': nn.ReLU(),\n",
    "        'GELU': nn.GELU(),\n",
    "        'Tanh': nn.Tanh()\n",
    "    }\n",
    "    activation_func = activation_mapping[activation_name]\n",
    "    dropout_rate = float(param['options']['params']['dropout_rate'])\n",
    "    \n",
    "    # Convert to PyTorch tensors\n",
    "    device = torch.device('cpu')\n",
    "    model['num_hidden_layers'] = num_hidden_layers\n",
    "    model['input_dim'] = input_dim\n",
    "    model['nodes_per_layer'] = nodes_per_layer\n",
    "    model['activation_name'] = activation_name\n",
    "    model['dropout_rate'] = dropout_rate\n",
    "    model['dnn'] = SimpleDNN(input_dim, num_hidden_layers, nodes_per_layer, activation_func, dropout_rate).to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_hidden_layers': 5, 'input_dim': 9, 'nodes_per_layer': 64, 'activation_name': 'Tanh', 'dropout_rate': 0.0, 'dnn': SimpleDNN(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=9, out_features=64, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (3): Tanh()\n",
      "    (4): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (5): Tanh()\n",
      "    (6): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (7): Tanh()\n",
      "    (8): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (9): Tanh()\n",
      "    (10): Linear(in_features=64, out_features=2, bias=True)\n",
      "  )\n",
      ")}\n"
     ]
    }
   ],
   "source": [
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",
    "model = init(df,param)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 3 - fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "deletable": false,
    "name": "mltkc_fit"
   },
   "outputs": [],
   "source": [
    "# train your model\n",
    "# returns a fit info json object and may modify the model object\n",
    "def fit(model,df,param):\n",
    "    summary_list = {}\n",
    "    df_train = df[df['is_train'] == 1]\n",
    "    df_test = df[df['is_train'] == 0]\n",
    "    X_train = df_train.drop('label', axis=1).drop('is_train', axis=1)\n",
    "    X_test = df_test.drop('label', axis=1).drop('is_train', axis=1)\n",
    "    y_train = df_train['label']\n",
    "    y_test = df_test['label']\n",
    "    \n",
    "    print(\"\\nShape of X_train:\", X_train.shape)\n",
    "    print(\"Shape of X_test:\", X_test.shape)\n",
    "    print(\"Shape of y_train:\", y_train.shape)\n",
    "    print(\"Shape of y_test:\", y_test.shape)\n",
    "\n",
    "    device = torch.device('cpu')\n",
    "    # Convert pandas Series to NumPy arrays before creating tensors\n",
    "    X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32).to(device)\n",
    "    y_train_tensor = torch.tensor(y_train.values, dtype=torch.long).to(device) # Convert Series to numpy array\n",
    "    X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32).to(device)\n",
    "    y_test_tensor = torch.tensor(y_test.values, dtype=torch.long).to(device)   # Convert Series to numpy array\n",
    "    \n",
    "    # Create TensorDataset and DataLoader\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "\n",
    "    learning_rate = float(param['options']['params']['learning_rate'])\n",
    "    optimizer_name = param['options']['params']['optimizer_name'].strip('\\\"')\n",
    "\n",
    "    # Calculate class weights\n",
    "    total_samples = len(y_train)\n",
    "    num_class_0 = np.sum(y_train == 0)\n",
    "    num_class_1 = np.sum(y_train == 1)\n",
    "    weight_for_class_0 = total_samples / (2.0 * num_class_0)\n",
    "    weight_for_class_1 = (total_samples / (2.0 * num_class_1)) * class_weight\n",
    "    class_weights = torch.tensor([weight_for_class_0, weight_for_class_1], dtype=torch.float32).to(device)\n",
    "\n",
    "    # Create DataLoader\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Define loss function with class weights\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    \n",
    "    # Define optimizer\n",
    "    if optimizer_name == 'Adam':\n",
    "        optimizer = optim.Adam(model['dnn'].parameters(), lr=learning_rate)\n",
    "    elif optimizer_name == 'RMSprop':\n",
    "        optimizer = optim.RMSprop(model['dnn'].parameters(), lr=learning_rate)\n",
    "    elif optimizer_name == 'Lion':\n",
    "        optimizer = Lion(model['dnn'].parameters(), lr=learning_rate, betas=(0.9, 0.99), weight_decay=0.1)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown optimizer: {optimizer_name}\")\n",
    "\n",
    "    # Training loop\n",
    "    print(f\"\\nTraining with: Layers={model['num_hidden_layers']}, Nodes={model['nodes_per_layer']}, LR={learning_rate}, \"\n",
    "          f\"Batch={batch_size}, Epochs={num_epochs}, Dropout={model['dropout_rate']}, Activation={model['activation_name']}, \"\n",
    "          f\"Optimizer={optimizer_name}, Class Weight={class_weight}\")\n",
    "    summary_list['training_settings'] = f\"\\nTraining with: Layers={model['num_hidden_layers']}, Nodes={model['nodes_per_layer']}, LR={learning_rate}, Batch={batch_size}, Epochs={num_epochs}, Dropout={model['dropout_rate']}, Activation={model['activation_name']}, Optimizer={optimizer_name}, Class Weight={class_weight}\"\n",
    "    start_time = time.time()\n",
    "    model['dnn'].train()\n",
    "    loss_list = []\n",
    "    epoch_list = []\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_list.append(epoch+1)\n",
    "        epoch_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model['dnn'](inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model['dnn'].parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item() * inputs.size(0)\n",
    "    \n",
    "        epoch_loss = epoch_loss / len(train_dataset)\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}')\n",
    "        loss_list.append(round(epoch_loss,4))\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"Training completed in {training_time:.2f} seconds\")\n",
    "    summary_list['training_time'] = f\"Training completed in {training_time:.2f} seconds\"\n",
    "    summary_list['epoch_number'] = epoch_list\n",
    "    summary_list['loss_list'] = loss_list\n",
    "    summary_list['final_loss'] = round(epoch_loss,4)\n",
    "    with open(MODEL_DIRECTORY + \"dnn_lab_loss.json\", 'w') as file:\n",
    "        json.dump(summary_list, file)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shape of X_train: (8000, 9)\n",
      "Shape of X_test: (2000, 9)\n",
      "Shape of y_train: (8000,)\n",
      "Shape of y_test: (2000,)\n",
      "\n",
      "Training with: Layers=5, Nodes=64, LR=0.01, Batch=512, Epochs=10, Dropout=0.0, Activation=Tanh, Optimizer=Adam, Class Weight=1\n",
      "Epoch 1/10, Loss: 0.4353\n",
      "Epoch 2/10, Loss: 0.3488\n",
      "Epoch 3/10, Loss: 0.3216\n",
      "Epoch 4/10, Loss: 0.3065\n",
      "Epoch 5/10, Loss: 0.2857\n",
      "Epoch 6/10, Loss: 0.2665\n",
      "Epoch 7/10, Loss: 0.2406\n",
      "Epoch 8/10, Loss: 0.2309\n",
      "Epoch 9/10, Loss: 0.2295\n",
      "Epoch 10/10, Loss: 0.2317\n",
      "Training completed in 0.88 seconds\n",
      "{'num_hidden_layers': 5, 'input_dim': 9, 'nodes_per_layer': 64, 'activation_name': 'Tanh', 'dropout_rate': 0.0, 'dnn': SimpleDNN(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=9, out_features=64, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (3): Tanh()\n",
      "    (4): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (5): Tanh()\n",
      "    (6): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (7): Tanh()\n",
      "    (8): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (9): Tanh()\n",
      "    (10): Linear(in_features=64, out_features=2, bias=True)\n",
      "  )\n",
      ")}\n"
     ]
    }
   ],
   "source": [
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",
    "print(fit(model,df,param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 4 - apply the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "deletable": false,
    "name": "mltkc_apply"
   },
   "outputs": [],
   "source": [
    "# apply your model\n",
    "# returns the calculated results\n",
    "def apply(model,df,param):\n",
    "    try:\n",
    "        X = df.drop('label', axis=1)\n",
    "    except:\n",
    "        X = df\n",
    "    try:\n",
    "        X = X.drop('is_train', axis=1) \n",
    "    except:\n",
    "        X = df\n",
    "    try:\n",
    "        device = torch.device('cpu')\n",
    "        X_tensor = torch.tensor(X.values, dtype=torch.float32).to(device)\n",
    "        model['dnn'].eval()\n",
    "        predictions = []\n",
    "        with torch.no_grad(): \n",
    "            for i in range(X_tensor.shape[0]): \n",
    "                inputs = X_tensor[i:i+1] \n",
    "                output = model['dnn'](inputs)\n",
    "                _, predicted = torch.max(output, 1)\n",
    "                predictions.append(predicted.tolist()[0])   \n",
    "        cols = {\"Result\": predictions}\n",
    "        result = pd.DataFrame(data=cols)\n",
    "    except Exception as e:\n",
    "        cols = {\"Error in Inference\": [str(model)]}\n",
    "        result = pd.DataFrame(data=cols)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Result\n",
      "0        1\n",
      "1        0\n",
      "2        1\n",
      "3        1\n",
      "4        0\n",
      "..     ...\n",
      "95       0\n",
      "96       1\n",
      "97       0\n",
      "98       0\n",
      "99       0\n",
      "\n",
      "[100 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",
    "print(apply(model,df[:100],param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 5 - save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "deletable": false,
    "name": "mltkc_save"
   },
   "outputs": [],
   "source": [
    "# save model to name in expected convention \"<algo_name>_<model_name>\"\n",
    "def save(model,name):\n",
    "    model_path = MODEL_DIRECTORY + name + \".pth\"\n",
    "    torch.save(model['dnn'], model_path)\n",
    "    with open(MODEL_DIRECTORY + name + \".json\", 'w') as file:\n",
    "        model_files = model.copy()\n",
    "        model_files.pop('dnn', None)\n",
    "        json.dump(model_files, file)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_hidden_layers': 5,\n",
       " 'input_dim': 9,\n",
       " 'nodes_per_layer': 64,\n",
       " 'activation_name': 'Tanh',\n",
       " 'dropout_rate': 0.0,\n",
       " 'dnn': SimpleDNN(\n",
       "   (layers): Sequential(\n",
       "     (0): Linear(in_features=9, out_features=64, bias=True)\n",
       "     (1): Tanh()\n",
       "     (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "     (3): Tanh()\n",
       "     (4): Linear(in_features=64, out_features=64, bias=True)\n",
       "     (5): Tanh()\n",
       "     (6): Linear(in_features=64, out_features=64, bias=True)\n",
       "     (7): Tanh()\n",
       "     (8): Linear(in_features=64, out_features=64, bias=True)\n",
       "     (9): Tanh()\n",
       "     (10): Linear(in_features=64, out_features=2, bias=True)\n",
       "   )\n",
       " )}"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save(model,\"dnn_lab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 6 - load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "deletable": false,
    "name": "mltkc_load"
   },
   "outputs": [],
   "source": [
    "# load model from name in expected convention \"<algo_name>_<model_name>\"\n",
    "def load(name):\n",
    "    model = {}\n",
    "    with open(MODEL_DIRECTORY + name + \".json\", 'r') as file:\n",
    "        model_params = json.load(file)\n",
    "\n",
    "    input_dim = model_params['input_dim']\n",
    "    num_hidden_layers = int(model_params['num_hidden_layers'])\n",
    "    activation_name = model_params['activation_name']\n",
    "    # Map activation functions\n",
    "    activation_mapping = {\n",
    "        'ReLU': nn.ReLU(),\n",
    "        'GELU': nn.GELU(),\n",
    "        'Tanh': nn.Tanh()\n",
    "    }\n",
    "    activation_func = activation_mapping[activation_name]\n",
    "    dropout_rate = float(model_params['dropout_rate'])\n",
    "    device = torch.device('cpu')\n",
    "    nodes_per_layer = model_params['nodes_per_layer']\n",
    "\n",
    "    model['dnn'] = SimpleDNN(input_dim, num_hidden_layers, nodes_per_layer, activation_func, dropout_rate).to(device)\n",
    "    model_path = MODEL_DIRECTORY + name + \".pth\"\n",
    "    model['dnn'] = torch.load(model_path, weights_only=False)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load(\"dnn_lab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dnn': SimpleDNN(\n",
       "   (layers): Sequential(\n",
       "     (0): Linear(in_features=9, out_features=64, bias=True)\n",
       "     (1): Tanh()\n",
       "     (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "     (3): Tanh()\n",
       "     (4): Linear(in_features=64, out_features=64, bias=True)\n",
       "     (5): Tanh()\n",
       "     (6): Linear(in_features=64, out_features=64, bias=True)\n",
       "     (7): Tanh()\n",
       "     (8): Linear(in_features=64, out_features=64, bias=True)\n",
       "     (9): Tanh()\n",
       "     (10): Linear(in_features=64, out_features=2, bias=True)\n",
       "   )\n",
       " )}"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 7 - provide a summary of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "deletable": false,
    "name": "mltkc_summary"
   },
   "outputs": [],
   "source": [
    "# return a model summary\n",
    "def summary(model=None):\n",
    "    try:\n",
    "        with open(MODEL_DIRECTORY + \"dnn_lab_loss.json\", 'r') as file:\n",
    "            loss_info = json.load(file)\n",
    "    except:\n",
    "        loss_info = {'training_settings': \"None\", 'training_time': \"None\", 'epoch_number':\"None\", 'loss_list':\"None\", 'final_loss': \"None\"}\n",
    "    returns = loss_info\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After implementing your fit, apply, save and load you can train your model:<br>\n",
    "| makeresults count=10<br>\n",
    "| streamstats c as i<br>\n",
    "| eval s = i%3<br>\n",
    "| eval feature_{s}=0<br>\n",
    "| foreach feature_* [eval &lt;&lt;FIELD&gt;&gt;=random()/pow(2,31)]<br>\n",
    "| fit MLTKContainer algo=barebone s from feature_* into app:barebone_model<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or apply your model:<br>\n",
    "| makeresults count=10<br>\n",
    "| streamstats c as i<br>\n",
    "| eval s = i%3<br>\n",
    "| eval feature_{s}=0<br>\n",
    "| foreach feature_* [eval &lt;&lt;FIELD&gt;&gt;=random()/pow(2,31)]<br>\n",
    "| apply barebone_model as the_meaning_of_life"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Send data back to Splunk HEC\n",
    "When you configured the Splunk HEC Settings in the DSDL app you can easily send back data to an index with [Splunk's HTTP Event Collector (HEC)](https://docs.splunk.com/Documentation/Splunk/latest/Data/UsetheHTTPEventCollector). Read more about data formats and options in the [documentation](https://docs.splunk.com/Documentation/Splunk/latest/Data/FormateventsforHTTPEventCollector#Event_metadata).\n",
    "\n",
    "### Use cases\n",
    "- you want to offload longer running, possibly distributed computations that need to deliver results asynchroneously back into Splunk. \n",
    "- you might not want to present results back into the search pipeline after your `| fit` or `| apply` command. \n",
    "- you can easily utilize this approach for any logging purposes or other profiling tasks in your ML code so you can actively monitor and analyze your processes.\n",
    "\n",
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dsdlsupport import SplunkHEC as SplunkHEC\n",
    "hec = SplunkHEC.SplunkHEC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example to send 10 hello world events\n",
    "response = hec.send_hello_world(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"HEC endpoint %s \\nreturned with status code %s \\nand response message: %s\" % (response.url, response.status_code, response.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example to send a JSON object, e.g. to log some data\n",
    "from datetime import datetime\n",
    "response = hec.send({'event': {'message': 'operation done', 'log_level': 'INFO' }, 'time': datetime.now().timestamp()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"HEC endpoint %s \\nreturned with status code %s \\nand response message: %s\" % (response.url, response.status_code, response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of Stages\n",
    "All subsequent cells are not tagged and can be used for further freeform code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

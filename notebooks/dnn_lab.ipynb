{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Deep Neural Network Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains code used to run the PyTorch Neural Network Lab in the DSDL App."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: By default every time you save this notebook the stages main cells are exported into a python module which can then get invoked by Splunk's MLTK SPL commands like <code> | fit ... | apply ... | summary </code>. Please read the Model Development Guide in the DSDL app for more information about this workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 0 - import libraries\n",
    "At stage 0 we define all imports necessary to run our subsequent code depending on various libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "deletable": false,
    "name": "mltkc_import"
   },
   "outputs": [],
   "source": [
    "# This definition exposes all python module imports that should be available in all subsequent commands\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import time\n",
    "\n",
    "# Custom Lion Optimizer\n",
    "class Lion(optim.Optimizer):\n",
    "    def __init__(self, params, lr=1e-4, betas=(0.9, 0.99), weight_decay=0.1):\n",
    "        defaults = dict(lr=lr, betas=betas, weight_decay=weight_decay)\n",
    "        super(Lion, self).__init__(params, defaults)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data\n",
    "                state = self.state[p]\n",
    "\n",
    "                # State initialization\n",
    "                if len(state) == 0:\n",
    "                    state['exp_avg'] = torch.zeros_like(p.data)\n",
    "\n",
    "                exp_avg = state['exp_avg']\n",
    "                beta1, beta2 = group['betas']\n",
    "\n",
    "                # Update momentum\n",
    "                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n",
    "\n",
    "                # Compute update\n",
    "                update = exp_avg.clone().mul_(beta2).add_(grad, alpha=1 - beta2).sign_()\n",
    "\n",
    "                # Weight decay\n",
    "                if group['weight_decay'] != 0:\n",
    "                    update.add_(p.data, alpha=group['weight_decay'])\n",
    "\n",
    "                # Update parameters\n",
    "                p.data.add_(update, alpha=-group['lr'])\n",
    "\n",
    "        return loss\n",
    "\n",
    "# Define the DNN model\n",
    "class SimpleDNN(nn.Module):\n",
    "    def __init__(self, input_dim, num_hidden_layers, nodes_per_layer, activation_func, dropout_rate):\n",
    "        super(SimpleDNN, self).__init__()\n",
    "        layers = []\n",
    "        \n",
    "        # Input layer\n",
    "        layers.append(nn.Linear(input_dim, nodes_per_layer))\n",
    "        layers.append(activation_func)\n",
    "        if dropout_rate > 0:\n",
    "            layers.append(nn.Dropout(dropout_rate))\n",
    "\n",
    "        # Hidden layers\n",
    "        for _ in range(num_hidden_layers - 1):\n",
    "            layers.append(nn.Linear(nodes_per_layer, nodes_per_layer))\n",
    "            layers.append(activation_func)\n",
    "            if dropout_rate > 0:\n",
    "                layers.append(nn.Dropout(dropout_rate))\n",
    "\n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(nodes_per_layer, 2))  # Binary classification\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "# Global constants\n",
    "nodes_per_layer = 128\n",
    "batch_size = 256\n",
    "class_weight = .3\n",
    "num_epochs = 10\n",
    "MODEL_DIRECTORY = \"/srv/app/model/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy version: 1.26.4\n",
      "pandas version: 2.2.3\n"
     ]
    }
   ],
   "source": [
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",
    "print(\"numpy version: \" + np.__version__)\n",
    "print(\"pandas version: \" + pd.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To stage the data with correct default parameters use this search:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| inputlookup dnn_lab_split.csv\n",
    "| fit MLTKContainer mode=stage num_hidden_layers=3 activation_name=\"Tanh\" dropout_rate=0.0 learning_rate=0.0001 optimizer_name=\"RMSprop\" algo=dnn_lab * into app:dnn_lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you run this search your data set sample is available as a csv inside the container to develop your model. The name is taken from the into keyword (\"dnn_lab\" in the example above) or set to \"default\" if no into keyword is present. This step is intended to work with a subset of your data to create your custom model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "deletable": false,
    "name": "mltkc_stage"
   },
   "outputs": [],
   "source": [
    "# This cell is not executed from MLTK and should only be used for staging data into the notebook environment\n",
    "def stage(name):\n",
    "    with open(\"data/\"+name+\".csv\", 'r') as f:\n",
    "        df = pd.read_csv(f)\n",
    "    with open(\"data/\"+name+\".json\", 'r') as f:\n",
    "        param = json.load(f)\n",
    "    return df, param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",
    "df, param = stage(\"dnn_lab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SS_bytes</th>\n",
       "      <th>SS_duration</th>\n",
       "      <th>SS_inter_packet_time</th>\n",
       "      <th>SS_jitter</th>\n",
       "      <th>SS_loss</th>\n",
       "      <th>SS_packets</th>\n",
       "      <th>SS_rate</th>\n",
       "      <th>SS_service_connections</th>\n",
       "      <th>SS_time_to_live</th>\n",
       "      <th>is_train</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.048395</td>\n",
       "      <td>-0.219041</td>\n",
       "      <td>-0.159467</td>\n",
       "      <td>-0.095410</td>\n",
       "      <td>-0.074079</td>\n",
       "      <td>-0.139108</td>\n",
       "      <td>0.251117</td>\n",
       "      <td>-0.249675</td>\n",
       "      <td>0.921018</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.031063</td>\n",
       "      <td>-0.212911</td>\n",
       "      <td>-0.159388</td>\n",
       "      <td>-0.094396</td>\n",
       "      <td>0.009384</td>\n",
       "      <td>0.238575</td>\n",
       "      <td>-0.479776</td>\n",
       "      <td>0.344239</td>\n",
       "      <td>-1.118512</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.048640</td>\n",
       "      <td>-0.218918</td>\n",
       "      <td>-0.159468</td>\n",
       "      <td>-0.095410</td>\n",
       "      <td>-0.074079</td>\n",
       "      <td>-0.139108</td>\n",
       "      <td>-0.478174</td>\n",
       "      <td>0.542210</td>\n",
       "      <td>-1.118512</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.048949</td>\n",
       "      <td>-0.219041</td>\n",
       "      <td>-0.159467</td>\n",
       "      <td>-0.095410</td>\n",
       "      <td>-0.074079</td>\n",
       "      <td>-0.139108</td>\n",
       "      <td>0.251117</td>\n",
       "      <td>-0.051704</td>\n",
       "      <td>0.921018</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.048395</td>\n",
       "      <td>-0.219041</td>\n",
       "      <td>-0.159467</td>\n",
       "      <td>-0.095410</td>\n",
       "      <td>-0.074079</td>\n",
       "      <td>-0.139108</td>\n",
       "      <td>0.345019</td>\n",
       "      <td>0.146267</td>\n",
       "      <td>0.921018</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>-0.039970</td>\n",
       "      <td>-0.198063</td>\n",
       "      <td>-0.158608</td>\n",
       "      <td>-0.084458</td>\n",
       "      <td>-0.002540</td>\n",
       "      <td>-0.024659</td>\n",
       "      <td>-0.498151</td>\n",
       "      <td>0.740181</td>\n",
       "      <td>-1.118512</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>-0.041206</td>\n",
       "      <td>-0.219042</td>\n",
       "      <td>-0.159467</td>\n",
       "      <td>-0.095410</td>\n",
       "      <td>-0.074079</td>\n",
       "      <td>-0.139108</td>\n",
       "      <td>0.852092</td>\n",
       "      <td>-0.447646</td>\n",
       "      <td>0.921018</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>-0.048704</td>\n",
       "      <td>-0.218896</td>\n",
       "      <td>-0.159467</td>\n",
       "      <td>-0.095410</td>\n",
       "      <td>-0.074079</td>\n",
       "      <td>-0.139108</td>\n",
       "      <td>-0.481425</td>\n",
       "      <td>-0.843588</td>\n",
       "      <td>-1.118512</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>-0.049049</td>\n",
       "      <td>4.992334</td>\n",
       "      <td>4.335346</td>\n",
       "      <td>-0.095206</td>\n",
       "      <td>-0.074079</td>\n",
       "      <td>-0.139108</td>\n",
       "      <td>-0.500101</td>\n",
       "      <td>-0.843588</td>\n",
       "      <td>-1.402034</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>-0.048395</td>\n",
       "      <td>-0.219041</td>\n",
       "      <td>-0.159467</td>\n",
       "      <td>-0.095410</td>\n",
       "      <td>-0.074079</td>\n",
       "      <td>-0.139108</td>\n",
       "      <td>0.345019</td>\n",
       "      <td>-0.051704</td>\n",
       "      <td>0.921018</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      SS_bytes  SS_duration  SS_inter_packet_time  SS_jitter   SS_loss  \\\n",
       "0    -0.048395    -0.219041             -0.159467  -0.095410 -0.074079   \n",
       "1    -0.031063    -0.212911             -0.159388  -0.094396  0.009384   \n",
       "2    -0.048640    -0.218918             -0.159468  -0.095410 -0.074079   \n",
       "3    -0.048949    -0.219041             -0.159467  -0.095410 -0.074079   \n",
       "4    -0.048395    -0.219041             -0.159467  -0.095410 -0.074079   \n",
       "...        ...          ...                   ...        ...       ...   \n",
       "9995 -0.039970    -0.198063             -0.158608  -0.084458 -0.002540   \n",
       "9996 -0.041206    -0.219042             -0.159467  -0.095410 -0.074079   \n",
       "9997 -0.048704    -0.218896             -0.159467  -0.095410 -0.074079   \n",
       "9998 -0.049049     4.992334              4.335346  -0.095206 -0.074079   \n",
       "9999 -0.048395    -0.219041             -0.159467  -0.095410 -0.074079   \n",
       "\n",
       "      SS_packets   SS_rate  SS_service_connections  SS_time_to_live  is_train  \\\n",
       "0      -0.139108  0.251117               -0.249675         0.921018         1   \n",
       "1       0.238575 -0.479776                0.344239        -1.118512         1   \n",
       "2      -0.139108 -0.478174                0.542210        -1.118512         1   \n",
       "3      -0.139108  0.251117               -0.051704         0.921018         1   \n",
       "4      -0.139108  0.345019                0.146267         0.921018         1   \n",
       "...          ...       ...                     ...              ...       ...   \n",
       "9995   -0.024659 -0.498151                0.740181        -1.118512         0   \n",
       "9996   -0.139108  0.852092               -0.447646         0.921018         0   \n",
       "9997   -0.139108 -0.481425               -0.843588        -1.118512         0   \n",
       "9998   -0.139108 -0.500101               -0.843588        -1.402034         0   \n",
       "9999   -0.139108  0.345019               -0.051704         0.921018         0   \n",
       "\n",
       "      label  \n",
       "0         1  \n",
       "1         0  \n",
       "2         0  \n",
       "3         0  \n",
       "4         1  \n",
       "...     ...  \n",
       "9995      0  \n",
       "9996      0  \n",
       "9997      0  \n",
       "9998      0  \n",
       "9999      1  \n",
       "\n",
       "[10000 rows x 11 columns]"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'options': {'params': {'mode': 'stage',\n",
       "   'num_hidden_layers': '3',\n",
       "   'activation_name': '\"Tanh\"',\n",
       "   'dropout_rate': '0.0',\n",
       "   'learning_rate': '0.0001',\n",
       "   'optimizer_name': '\"RMSprop\"',\n",
       "   'algo': 'dnn_lab'},\n",
       "  'args': ['*'],\n",
       "  'feature_variables': ['*'],\n",
       "  'model_name': 'dnn_lab',\n",
       "  'algo_name': 'MLTKContainer',\n",
       "  'mlspl_limits': {'disabled': False,\n",
       "   'handle_new_cat': 'default',\n",
       "   'max_distinct_cat_values': '100',\n",
       "   'max_distinct_cat_values_for_classifiers': '100',\n",
       "   'max_distinct_cat_values_for_scoring': '100',\n",
       "   'max_fit_time': '6000',\n",
       "   'max_inputs': '1000000',\n",
       "   'max_memory_usage_mb': '16000',\n",
       "   'max_model_size_mb': '3000',\n",
       "   'max_score_time': '6000',\n",
       "   'use_sampling': '1'},\n",
       "  'kfold_cv': None},\n",
       " 'feature_variables': ['SS_bytes',\n",
       "  'SS_duration',\n",
       "  'SS_inter_packet_time',\n",
       "  'SS_jitter',\n",
       "  'SS_loss',\n",
       "  'SS_packets',\n",
       "  'SS_rate',\n",
       "  'SS_service_connections',\n",
       "  'SS_time_to_live',\n",
       "  'is_train',\n",
       "  'label']}"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 2 - create and initialize a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "deletable": false,
    "name": "mltkc_init"
   },
   "outputs": [],
   "source": [
    "# initialize your model\n",
    "# available inputs: data and parameters\n",
    "# returns the model object which will be used as a reference to call fit, apply and summary subsequently\n",
    "def init(df,param):\n",
    "    model = {}\n",
    "    input_dim = len(df.columns)-2 #remove training and flag field in input dimensionality\n",
    "    num_hidden_layers = int(param['options']['params']['num_hidden_layers'])\n",
    "    activation_name = param['options']['params']['activation_name'].strip('\\\"')\n",
    "    \n",
    "    # Map activation functions\n",
    "    activation_mapping = {\n",
    "        'ReLU': nn.ReLU(),\n",
    "        'GELU': nn.GELU(),\n",
    "        'Tanh': nn.Tanh()\n",
    "    }\n",
    "    activation_func = activation_mapping[activation_name]\n",
    "    dropout_rate = float(param['options']['params']['dropout_rate'])\n",
    "    \n",
    "    # Convert to PyTorch tensors\n",
    "    device = torch.device('cpu')\n",
    "    model['num_hidden_layers'] = num_hidden_layers\n",
    "    model['input_dim'] = input_dim\n",
    "    model['nodes_per_layer'] = nodes_per_layer\n",
    "    model['activation_name'] = activation_name\n",
    "    model['dropout_rate'] = dropout_rate\n",
    "    model['dnn'] = SimpleDNN(input_dim, num_hidden_layers, nodes_per_layer, activation_func, dropout_rate).to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_hidden_layers': 3, 'input_dim': 9, 'nodes_per_layer': 128, 'activation_name': 'Tanh', 'dropout_rate': 0.0, 'dnn': SimpleDNN(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=9, out_features=128, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (3): Tanh()\n",
      "    (4): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (5): Tanh()\n",
      "    (6): Linear(in_features=128, out_features=2, bias=True)\n",
      "  )\n",
      ")}\n"
     ]
    }
   ],
   "source": [
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",
    "model = init(df,param)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 3 - fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "deletable": false,
    "name": "mltkc_fit"
   },
   "outputs": [],
   "source": [
    "# Train your model\n",
    "# Returns a fit info json object and may modify the model object\n",
    "def fit(model,df,param):\n",
    "    summary_list = {}\n",
    "    df_train = df[df['is_train'] == 1]\n",
    "    df_test = df[df['is_train'] == 0]\n",
    "    X_train = df_train.drop('label', axis=1).drop('is_train', axis=1)\n",
    "    X_test = df_test.drop('label', axis=1).drop('is_train', axis=1)\n",
    "    y_train = df_train['label']\n",
    "    y_test = df_test['label']\n",
    "    \n",
    "    print(\"\\nShape of X_train:\", X_train.shape)\n",
    "    print(\"Shape of X_test:\", X_test.shape)\n",
    "    print(\"Shape of y_train:\", y_train.shape)\n",
    "    print(\"Shape of y_test:\", y_test.shape)\n",
    "\n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "    # Convert pandas Series to NumPy arrays before creating tensors\n",
    "    X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32).to(device)\n",
    "    y_train_tensor = torch.tensor(y_train.values, dtype=torch.long).to(device) # Convert Series to numpy array\n",
    "    X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32).to(device)\n",
    "    y_test_tensor = torch.tensor(y_test.values, dtype=torch.long).to(device)   # Convert Series to numpy array\n",
    "    \n",
    "    # Create TensorDataset and DataLoader\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "\n",
    "    learning_rate = float(param['options']['params']['learning_rate'])\n",
    "    optimizer_name = param['options']['params']['optimizer_name'].strip('\\\"')\n",
    "\n",
    "    # Calculate class weights\n",
    "    total_samples = len(y_train)\n",
    "    num_class_0 = np.sum(y_train == 0)\n",
    "    num_class_1 = np.sum(y_train == 1)\n",
    "    weight_for_class_0 = total_samples / (2.0 * num_class_0)\n",
    "    weight_for_class_1 = (total_samples / (2.0 * num_class_1)) * class_weight\n",
    "    class_weights = torch.tensor([weight_for_class_0, weight_for_class_1], dtype=torch.float32).to(device)\n",
    "\n",
    "    # Create DataLoader\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Define loss function with class weights\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    \n",
    "    # Define optimizer\n",
    "    if optimizer_name == 'Adam':\n",
    "        optimizer = optim.Adam(model['dnn'].parameters(), lr=learning_rate)\n",
    "    elif optimizer_name == 'RMSprop':\n",
    "        optimizer = optim.RMSprop(model['dnn'].parameters(), lr=learning_rate)\n",
    "    elif optimizer_name == 'Lion':\n",
    "        optimizer = Lion(model['dnn'].parameters(), lr=learning_rate, betas=(0.9, 0.99), weight_decay=0.1)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown optimizer: {optimizer_name}\")\n",
    "\n",
    "    # Training loop\n",
    "    print(f\"\\nTraining with: Layers={model['num_hidden_layers']}, Nodes={model['nodes_per_layer']}, LR={learning_rate}, \"\n",
    "          f\"Batch={batch_size}, Epochs={num_epochs}, Dropout={model['dropout_rate']}, Activation={model['activation_name']}, \"\n",
    "          f\"Optimizer={optimizer_name}, Class Weight={class_weight}\")\n",
    "    summary_list['training_settings'] = f\"\\nTraining with: Layers={model['num_hidden_layers']}, Nodes={model['nodes_per_layer']}, LR={learning_rate}, Batch={batch_size}, Epochs={num_epochs}, Dropout={model['dropout_rate']}, Activation={model['activation_name']}, Optimizer={optimizer_name}, Class Weight={class_weight}\"\n",
    "    start_time = time.time()\n",
    "    model['dnn'].train()\n",
    "    loss_list = []\n",
    "    epoch_list = []\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_list.append(epoch+1)\n",
    "        epoch_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model['dnn'](inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model['dnn'].parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item() * inputs.size(0)\n",
    "    \n",
    "        epoch_loss = epoch_loss / len(train_dataset)\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}')\n",
    "        loss_list.append(round(epoch_loss,4))\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"Training completed in {training_time:.2f} seconds\")\n",
    "    summary_list['training_time'] = f\"Training completed in {training_time:.2f} seconds\"\n",
    "    summary_list['epoch_number'] = epoch_list\n",
    "    summary_list['loss_list'] = loss_list\n",
    "    summary_list['final_loss'] = round(epoch_loss,4)\n",
    "    with open(MODEL_DIRECTORY + \"dnn_lab_loss.json\", 'w') as file:\n",
    "        json.dump(summary_list, file)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shape of X_train: (8000, 9)\n",
      "Shape of X_test: (2000, 9)\n",
      "Shape of y_train: (8000,)\n",
      "Shape of y_test: (2000,)\n",
      "\n",
      "Training with: Layers=3, Nodes=128, LR=0.0001, Batch=256, Epochs=10, Dropout=0.0, Activation=Tanh, Optimizer=RMSprop, Class Weight=0.3\n",
      "Epoch 1/10, Loss: 0.4057\n",
      "Epoch 2/10, Loss: 0.3230\n",
      "Epoch 3/10, Loss: 0.3167\n",
      "Epoch 4/10, Loss: 0.3132\n",
      "Epoch 5/10, Loss: 0.3110\n",
      "Epoch 6/10, Loss: 0.3087\n",
      "Epoch 7/10, Loss: 0.3063\n",
      "Epoch 8/10, Loss: 0.3034\n",
      "Epoch 9/10, Loss: 0.3004\n",
      "Epoch 10/10, Loss: 0.2971\n",
      "Training completed in 1.13 seconds\n",
      "{'num_hidden_layers': 3, 'input_dim': 9, 'nodes_per_layer': 128, 'activation_name': 'Tanh', 'dropout_rate': 0.0, 'dnn': SimpleDNN(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=9, out_features=128, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (3): Tanh()\n",
      "    (4): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (5): Tanh()\n",
      "    (6): Linear(in_features=128, out_features=2, bias=True)\n",
      "  )\n",
      ")}\n"
     ]
    }
   ],
   "source": [
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",
    "print(fit(model,df,param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 4 - apply the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "deletable": false,
    "name": "mltkc_apply"
   },
   "outputs": [],
   "source": [
    "# Apply your model\n",
    "# Returns the calculated results\n",
    "def apply(model,df,param):\n",
    "    try:\n",
    "        X = df.drop('label', axis=1)\n",
    "    except:\n",
    "        X = df\n",
    "    try:\n",
    "        X = X.drop('is_train', axis=1) \n",
    "    except:\n",
    "        X = df\n",
    "    try:\n",
    "        device = torch.device('cpu')\n",
    "        X_tensor = torch.tensor(X.values, dtype=torch.float32).to(device)\n",
    "        model['dnn'].eval()\n",
    "        predictions = []\n",
    "        with torch.no_grad(): \n",
    "            for i in range(X_tensor.shape[0]): \n",
    "                inputs = X_tensor[i:i+1] \n",
    "                output = model['dnn'](inputs)\n",
    "                _, predicted = torch.max(output, 1)\n",
    "                predictions.append(predicted.tolist()[0])   \n",
    "        cols = {\"Result\": predictions}\n",
    "        result = pd.DataFrame(data=cols)\n",
    "    except Exception as e:\n",
    "        cols = {\"Error in Inference\": [str(model)]}\n",
    "        result = pd.DataFrame(data=cols)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Result\n",
      "0        1\n",
      "1        0\n",
      "2        0\n",
      "3        1\n",
      "4        1\n",
      "..     ...\n",
      "95       0\n",
      "96       1\n",
      "97       1\n",
      "98       0\n",
      "99       0\n",
      "\n",
      "[100 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",
    "print(apply(model,df[:100],param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 5 - save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "deletable": false,
    "name": "mltkc_save"
   },
   "outputs": [],
   "source": [
    "# Save model to name in expected convention \"<algo_name>_<model_name>\"\n",
    "def save(model,name):\n",
    "    model_path = MODEL_DIRECTORY + name + \".pth\"\n",
    "    torch.save(model['dnn'], model_path)\n",
    "    with open(MODEL_DIRECTORY + name + \".json\", 'w') as file:\n",
    "        model_files = model.copy()\n",
    "        model_files.pop('dnn', None)\n",
    "        json.dump(model_files, file)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_hidden_layers': 3,\n",
       " 'input_dim': 9,\n",
       " 'nodes_per_layer': 128,\n",
       " 'activation_name': 'Tanh',\n",
       " 'dropout_rate': 0.0,\n",
       " 'dnn': SimpleDNN(\n",
       "   (layers): Sequential(\n",
       "     (0): Linear(in_features=9, out_features=128, bias=True)\n",
       "     (1): Tanh()\n",
       "     (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "     (3): Tanh()\n",
       "     (4): Linear(in_features=128, out_features=128, bias=True)\n",
       "     (5): Tanh()\n",
       "     (6): Linear(in_features=128, out_features=2, bias=True)\n",
       "   )\n",
       " )}"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save(model,\"dnn_lab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 6 - load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "deletable": false,
    "name": "mltkc_load"
   },
   "outputs": [],
   "source": [
    "# Load model from name in expected convention \"<algo_name>_<model_name>\"\n",
    "def load(name):\n",
    "    model = {}\n",
    "    with open(MODEL_DIRECTORY + name + \".json\", 'r') as file:\n",
    "        model_params = json.load(file)\n",
    "\n",
    "    input_dim = model_params['input_dim']\n",
    "    num_hidden_layers = int(model_params['num_hidden_layers'])\n",
    "    activation_name = model_params['activation_name']\n",
    "    # Map activation functions\n",
    "    activation_mapping = {\n",
    "        'ReLU': nn.ReLU(),\n",
    "        'GELU': nn.GELU(),\n",
    "        'Tanh': nn.Tanh()\n",
    "    }\n",
    "    activation_func = activation_mapping[activation_name]\n",
    "    dropout_rate = float(model_params['dropout_rate'])\n",
    "    device = torch.device('cpu')\n",
    "    nodes_per_layer = model_params['nodes_per_layer']\n",
    "\n",
    "    model['dnn'] = SimpleDNN(input_dim, num_hidden_layers, nodes_per_layer, activation_func, dropout_rate).to(device)\n",
    "    model_path = MODEL_DIRECTORY + name + \".pth\"\n",
    "    model['dnn'] = torch.load(model_path, weights_only=False)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load(\"dnn_lab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dnn': SimpleDNN(\n",
       "   (layers): Sequential(\n",
       "     (0): Linear(in_features=9, out_features=128, bias=True)\n",
       "     (1): Tanh()\n",
       "     (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "     (3): Tanh()\n",
       "     (4): Linear(in_features=128, out_features=128, bias=True)\n",
       "     (5): Tanh()\n",
       "     (6): Linear(in_features=128, out_features=2, bias=True)\n",
       "   )\n",
       " )}"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 7 - provide a summary of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "deletable": false,
    "name": "mltkc_summary"
   },
   "outputs": [],
   "source": [
    "# Return a model summary\n",
    "def summary(model=None):\n",
    "    try:\n",
    "        with open(MODEL_DIRECTORY + \"dnn_lab_loss.json\", 'r') as file:\n",
    "            loss_info = json.load(file)\n",
    "    except:\n",
    "        loss_info = {'training_settings': \"None\", 'training_time': \"None\", 'epoch_number':\"None\", 'loss_list':\"None\", 'final_loss': \"None\"}\n",
    "    returns = loss_info\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of Stages\n",
    "All subsequent cells are not tagged and can be used for further freeform code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

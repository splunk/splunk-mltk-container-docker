{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding document data to vector DB using llama-index schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example Usage: | makeresults | fit MLTKContainer algo=llm_rag_document_encoder data_path=\"/srv/notebooks/data/various_data\" embedder_name=\"all-MiniLM-L6-v2\" use_local=1 embedder_dimension=384 collection_name=\"document_collection_example\" _time into app:llm_rag_document_encoder as Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 0 - import libraries\n",
    "At stage 0 we define all imports necessary to run our subsequent code depending on various libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "name": "mltkc_import",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-17 00:17:12.107905: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-06-17 00:17:12.147666: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# this definition exposes all python module imports that should be available in all subsequent commands\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pymilvus\n",
    "from pymilvus import (\n",
    "    connections,\n",
    "    utility,\n",
    "    FieldSchema, CollectionSchema, DataType,\n",
    "    Collection,\n",
    ")\n",
    "import llama_index\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Document, StorageContext, ServiceContext\n",
    "from llama_index.readers.file import DocxReader, CSVReader, PDFReader, PptxReader, XMLReader, IPYNBReader \n",
    "from llama_index.vector_stores.milvus import MilvusVectorStore\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "import textwrap\n",
    "\n",
    "# ...\n",
    "# global constants\n",
    "MODEL_DIRECTORY = \"/srv/app/model/data/\"\n",
    "MILVUS_ENDPOINT = \"http://milvus-standalone:19530\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional logging settings for llama-index\n",
    "import logging\n",
    "import sys\n",
    "import llama_index.core\n",
    "from llama_index.core.callbacks import (\n",
    "    CallbackManager,\n",
    "    LlamaDebugHandler,\n",
    "    CBEventType,\n",
    ")\n",
    "\n",
    "llama_index.core.set_global_handler(\"simple\")\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "llama_debug = LlamaDebugHandler(print_trace_on_end=True)\n",
    "callback_manager = CallbackManager([llama_debug])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy version: 1.26.4\n",
      "pandas version: 2.2.0\n"
     ]
    }
   ],
   "source": [
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",
    "print(\"numpy version: \" + np.__version__)\n",
    "print(\"pandas version: \" + pd.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "name": "mltkc_stage"
   },
   "outputs": [],
   "source": [
    "# this cell is not executed from MLTK and should only be used for staging data into the notebook environment\n",
    "def stage(name):\n",
    "    with open(\"data/\"+name+\".csv\", 'r') as f:\n",
    "        df = pd.read_csv(f)\n",
    "    with open(\"data/\"+name+\".json\", 'r') as f:\n",
    "        param = json.load(f)\n",
    "    return df, param"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 2 - create and initialize a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "name": "mltkc_init"
   },
   "outputs": [],
   "source": [
    "# initialize your model\n",
    "# available inputs: data and parameters\n",
    "# returns the model object which will be used as a reference to call fit, apply and summary subsequently\n",
    "def init(df,param):\n",
    "    model = {}\n",
    "    model['hyperparameter'] = 42.0\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 3 - fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "name": "mltkc_fit"
   },
   "outputs": [],
   "source": [
    "# train your model\n",
    "# returns a fit info json object and may modify the model object\n",
    "def fit(model,df,param):\n",
    "    info = {\"message\": \"model trained\"}\n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",
    "print(fit(model,df,param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 4 - apply the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "name": "mltkc_apply"
   },
   "outputs": [],
   "source": [
    "# apply your model\n",
    "# returns the calculated results\n",
    "def apply(model,df,param):\n",
    "    # Datapath Example: '/srv/notebooks/data/splunk_doc/'\n",
    "    data_path = param['options']['params']['data_path'].strip('\\\"')\n",
    "    use_local= int(param['options']['params']['use_local'])\n",
    "    # Embedder Example: 'all-MiniLM-L6-v2'\n",
    "    try:\n",
    "        embedder_name = param['options']['params']['embedder_name'].strip('\\\"')\n",
    "        embedder_dimension = param['options']['params']['embedder_dimension']\n",
    "        collection_name = param['options']['params']['collection_name'].strip('\\\"')\n",
    "    except:\n",
    "        embedder_name = 'all-MiniLM-L6-v2'\n",
    "        embedder_dimension = 384\n",
    "        collection_name = \"default-doc-collection\"\n",
    "    # Dimension checking for default embedders\n",
    "    if embedder_name == 'intfloat/multilingual-e5-large':\n",
    "        embedder_dimension = 1024\n",
    "    elif embedder_name == 'all-MiniLM-L6-v2':\n",
    "        embedder_dimension = 384\n",
    "    else:\n",
    "        embedder_dimension = embedder_dimension\n",
    "    # Using local embedder checkpoints\n",
    "    if use_local:\n",
    "        embedder_name = f'/srv/app/model/data/{embedder_name}'\n",
    "        print(\"Using local embedding model checkpoints\")\n",
    "        \n",
    "    # To support pptx files, huggingface extractor needs to be downloaded. Skipping support for this version\n",
    "    # Special parser for CSV data\n",
    "    parser = CSVReader()\n",
    "    file_extractor = {\".csv\": parser}\n",
    "    try:\n",
    "        # Create document dataloader - recursively find data from sub-directories\n",
    "        # Add desired file extensions in required_exts. For example: required_exts=[\".csv\", \".xml\", \".pdf\", \".docx\", \".ipynb\"]\n",
    "        documents = SimpleDirectoryReader(\n",
    "            input_dir=data_path, recursive=True, file_extractor=file_extractor, required_exts=[\".ipynb\", \".csv\", \".xml\", \".pdf\", \".txt\", \".docx\"]\n",
    "        ).load_data()\n",
    "    except:\n",
    "        documents = None\n",
    "        message = \"ERROR: No data in the directory specified. Check if the directory exists and contains files.\"\n",
    "    # Create Transformers embedding model \n",
    "    ## TODO: add local loading option\n",
    "    try:\n",
    "        transformer_embedder = HuggingFaceEmbedding(model_name=embedder_name)\n",
    "        print(f'Loaded embedder from {embedder_name}')\n",
    "    except:\n",
    "        transformer_embedder = None\n",
    "        message = \"ERROR: embedding model is not loaded. Check if the model name is correct. For local loading, check if the path exists\"\n",
    "\n",
    "    if (documents is not None) & (transformer_embedder is not None):\n",
    "        service_context = ServiceContext.from_defaults(\n",
    "            llm=None, embed_model=transformer_embedder, chunk_size=1024\n",
    "        )\n",
    "        vector_store = MilvusVectorStore(uri=MILVUS_ENDPOINT, token=\"\", collection_name=collection_name, dim=embedder_dimension, overwrite=False)\n",
    "        storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "        # Index document data\n",
    "        index = VectorStoreIndex.from_documents(\n",
    "            documents, storage_context=storage_context, service_context=service_context\n",
    "        )\n",
    "    \n",
    "        # Prepare output dataframe\n",
    "        embedder = [str(transformer_embedder)]\n",
    "        vector_store = [str(vector_store)]\n",
    "        document = []\n",
    "        for d in documents:\n",
    "            document.append(str(d.metadata['file_path']))\n",
    "        document = str(list(dict.fromkeys(document)))\n",
    "        cols = {\"Embedder_Info\": embedder, \"Vector_Store_Info\": vector_store, \"Documents_Info\": [document], \"Message\": [\"Success\"]}\n",
    "    else:\n",
    "        cols = {\"Embedder_Info\": [\"No Result\"], \"Vector_Store_Info\": [\"No Result\"], \"Documents_Info\": [\"No Result\"], \"Message\": [message]}\n",
    "    result = pd.DataFrame(data=cols)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",
    "print(apply(None,None,None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 5 - save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "name": "mltkc_save"
   },
   "outputs": [],
   "source": [
    "# save model to name in expected convention \"<algo_name>_<model_name>\"\n",
    "def save(model,name):\n",
    "    with open(MODEL_DIRECTORY + name + \".json\", 'w') as file:\n",
    "        json.dump(model, file)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 6 - load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "name": "mltkc_load"
   },
   "outputs": [],
   "source": [
    "# load model from name in expected convention \"<algo_name>_<model_name>\"\n",
    "def load(name):\n",
    "    model = {}\n",
    "    with open(MODEL_DIRECTORY + name + \".json\", 'r') as file:\n",
    "        model = json.load(file)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 7 - provide a summary of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "name": "mltkc_summary"
   },
   "outputs": [],
   "source": [
    "# return a model summary\n",
    "def summary(model=None):\n",
    "    returns = {\"version\": {\"numpy\": np.__version__, \"pandas\": pd.__version__} }\n",
    "    return returns\n",
    "def compute(model,df,param):\n",
    "    # Datapath Example: '/srv/notebooks/data/splunk_doc/'\n",
    "    data_path = param['params']['data_path'].strip('\\\"')\n",
    "    use_local= int(param['params']['use_local'])\n",
    "    # Embedder Example: 'all-MiniLM-L6-v2'\n",
    "    try:\n",
    "        embedder_name = param['params']['embedder_name'].strip('\\\"')\n",
    "        embedder_dimension = param['params']['embedder_dimension']\n",
    "        collection_name = param['params']['collection_name'].strip('\\\"')\n",
    "    except:\n",
    "        embedder_name = 'all-MiniLM-L6-v2'\n",
    "        embedder_dimension = 384\n",
    "        collection_name = \"default-doc-collection\"\n",
    "    # Dimension checking for default embedders\n",
    "    if embedder_name == 'intfloat/multilingual-e5-large':\n",
    "        embedder_dimension = 1024\n",
    "    elif embedder_name == 'all-MiniLM-L6-v2':\n",
    "        embedder_dimension = 384\n",
    "    else:\n",
    "        embedder_dimension = embedder_dimension\n",
    "    # Using local embedder checkpoints\n",
    "    if use_local:\n",
    "        embedder_name = f'/srv/app/model/data/{embedder_name}'\n",
    "        print(\"Using local embedding model checkpoints\")\n",
    "        \n",
    "    # To support pptx files, huggingface extractor needs to be downloaded. Skipping support for this version\n",
    "    # Special parser for CSV data\n",
    "    parser = CSVReader()\n",
    "    file_extractor = {\".csv\": parser}\n",
    "    try:\n",
    "        # Create document dataloader - recursively find data from sub-directories\n",
    "        # Add desired file extensions in required_exts. For example: required_exts=[\".csv\", \".xml\", \".pdf\", \".docx\", \".ipynb\"]\n",
    "        documents = SimpleDirectoryReader(\n",
    "            input_dir=data_path, recursive=True, file_extractor=file_extractor, required_exts=[\".ipynb\", \".csv\", \".xml\", \".pdf\", \".txt\", \".docx\"]\n",
    "        ).load_data()\n",
    "    except:\n",
    "        documents = None\n",
    "        message = \"ERROR: No data in the directory specified. Check if the directory exists and contains files.\"\n",
    "    # Create Transformers embedding model \n",
    "    ## TODO: add local loading option\n",
    "    try:\n",
    "        transformer_embedder = HuggingFaceEmbedding(model_name=embedder_name)\n",
    "        print(f'Loaded embedder from {embedder_name}')\n",
    "    except:\n",
    "        transformer_embedder = None\n",
    "        message = \"ERROR: embedding model is not loaded. Check if the model name is correct. For local loading, check if the path exists\"\n",
    "\n",
    "    if (documents is not None) & (transformer_embedder is not None):\n",
    "        print(\"Start encoding\")\n",
    "        service_context = ServiceContext.from_defaults(\n",
    "            llm=None, embed_model=transformer_embedder, chunk_size=1024\n",
    "        )\n",
    "        vector_store = MilvusVectorStore(uri=MILVUS_ENDPOINT, token=\"\", collection_name=collection_name, dim=embedder_dimension, overwrite=False)\n",
    "        print(\"Vector store ok\")\n",
    "        storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "        # Index document data\n",
    "        index = VectorStoreIndex.from_documents(\n",
    "            documents, storage_context=storage_context, service_context=service_context\n",
    "        )\n",
    "        print(\"Index store ok\")\n",
    "        # Prepare output dataframe\n",
    "        embedder = str(transformer_embedder)\n",
    "        vector_store = str(vector_store)\n",
    "        document = []\n",
    "        for d in documents:\n",
    "            document.append(str(d.metadata['file_path']))\n",
    "        print(\"Finished encoding\")\n",
    "        document = str(list(dict.fromkeys(document)))\n",
    "        cols = {\"Embedder_Info\": embedder, \"Vector_Store_Info\": vector_store, \"Documents_Info\": document, \"Message\": \"Success\"}\n",
    "    else:\n",
    "        cols = {\"Embedder_Info\": \"No Result\", \"Vector_Store_Info\": \"No Result\", \"Documents_Info\": \"No Result\", \"Message\": message}\n",
    "    result = [cols]\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of Stages\n",
    "All subsequent cells are not tagged and can be used for further freeform code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

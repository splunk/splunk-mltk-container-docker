{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Summarization fine-tuning script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script contains the source code of how to fine-tune a T5 model on **End-to-End automatic summarization task** in both English and Japanese.\n",
    "\n",
    "## Sample usage of SPL for fine-tuning:\n",
    "\n",
    "| inputlookup summarization_jp\n",
    "| fields body_text summary_text \n",
    "| rename summary_text as summary body_text as text\n",
    "| head 10\n",
    "| fit MLTKContainer algo=transformers_summarization max_epochs=1 lang=jp base_model=t5_summarization_jp batch_size=4 summary from text into app:t5_summarization_jp_finetuned_final as extracted_summary\n",
    "\n",
    "## Sample usage of SPL for applying:\n",
    "\n",
    "| inputlookup summarization_jp\n",
    "| fields body_text summary_text \n",
    "| rename summary_text as summary body_text as text\n",
    "| head 10\n",
    "| apply t5_summarization_jp_finetuned_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 0 - import libraries\n",
    "At stage 0 we define all imports necessary to run our subsequent code depending on various libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "name": "mltkc_import"
   },
   "outputs": [],
   "source": [
    "# this definition exposes all python module imports that should be available in all subsequent commands\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import re\n",
    "import math\n",
    "import time\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import tarfile\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "# from torchmetrics.text.rouge import ROUGEScore\n",
    "# tensorboard related\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import tensorboard\n",
    "import datetime\n",
    "import logging\n",
    "import sys\n",
    "import io\n",
    "import os\n",
    "import psutil\n",
    "import shutil\n",
    "# Fine-tune parameters initialization\n",
    "MODEL_NAME = \"/srv/app/model/data\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "max_length_src = 400\n",
    "max_length_target = 200\n",
    "\n",
    "batch_size_train = 4\n",
    "batch_size_valid = 4\n",
    "\n",
    "epochs = 100\n",
    "patience = 20\n",
    "\n",
    "MODEL_DIRECTORY = \"/\"\n",
    "\n",
    "class T5FineTuner(nn.Module):\n",
    "    \n",
    "    def __init__(self, MODEL_NAME):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME, local_files_only=True)\n",
    "\n",
    "    def forward(\n",
    "        self, input_ids, attention_mask=None, decoder_input_ids=None,\n",
    "        decoder_attention_mask=None, labels=None\n",
    "    ):\n",
    "        return self.model(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            decoder_input_ids=decoder_input_ids,\n",
    "            decoder_attention_mask=decoder_attention_mask,\n",
    "            labels=labels\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1 - get a data sample from Splunk\n",
    "In Splunk run a search to pipe a prepared dataset into this environment. (internal testing only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| inputlookup summarization_en\n",
    "| fields text summary\n",
    "| head 5\n",
    "| fit MLTKContainer algo=transformers_summarization mode=stage max_epochs=1 lang=en base_model=t5_summarization_en summary from text into app:t5_summarization_en_finetuned_final as extracted_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "name": "mltkc_stage"
   },
   "outputs": [],
   "source": [
    "# this cell is not executed from MLTK and should only be used for staging data into the notebook environment\n",
    "def stage(name):\n",
    "    print(\"DEBUG stage call\")\n",
    "    print(\"DEBUG \" + name)\n",
    "    with open(\"/srv/notebooks/data/\"+name+\".csv\", 'r') as f:\n",
    "        df = pd.read_csv(f)\n",
    "    with open(\"/srv/notebooks/data/\"+name+\".json\", 'r') as f:\n",
    "        param = json.load(f) \n",
    "    return df, param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG stage call\n",
      "DEBUG t5_summarization_jp_finetuned_final\n"
     ]
    }
   ],
   "source": [
    "df, param = stage(\"t5_summarization_jp_finetuned_final\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 2 - create and initialize a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "name": "mltkc_init"
   },
   "outputs": [],
   "source": [
    "def init(df,param):\n",
    "    tag = \"-- process=fine_tuning_progress model={} max_epoch={} -- \".format(param['options']['model_name'], param['options']['params']['max_epochs'])\n",
    "\n",
    "    print(tag + \"Training data loaded with shape: \" + str(df.shape))\n",
    "    print(tag + \"Input parameters: \", param['options']['params'])\n",
    "    print(tag + \"Epoch number: \" + param['options']['params']['max_epochs'])\n",
    "    print(tag + \"Base model: \" + param['options']['params']['base_model'])\n",
    "    \n",
    "    print(tag + \"Model Initialization: started\")\n",
    "    MODEL_NAME = \"/srv/app/model/data/summarization\"\n",
    "    MODEL_NAME = os.path.join(MODEL_NAME, param['options']['params']['lang'], param['options']['params']['base_model'])\n",
    "    print(tag + \"Model file in \" + MODEL_NAME)\n",
    "    model = T5FineTuner(MODEL_NAME)\n",
    "    model = model.to(device)\n",
    "    print(tag + \"Model Initialization: successfully finished\")\n",
    "    # GPU memory calculation\n",
    "    t = torch.cuda.get_device_properties(0).total_memory\n",
    "    r = torch.cuda.memory_reserved(0)\n",
    "    a = torch.cuda.memory_allocated(0)\n",
    "    f = r-a  # free inside reserved\n",
    "    load1, load5, load15 = psutil.getloadavg()\n",
    "    cpu_usage = (load15/os.cpu_count()) * 100\n",
    "    stat = shutil.disk_usage(\"/\")\n",
    "    \n",
    "    print(tag + \"#GPU memory --Total memory: {}, --Memory reserved: {}, --Memory allocated: {}. #CPU: {}% occupied. #disk {}\".format(t,r,a,cpu_usage,stat))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- process=fine_tuning_progress model=t5_summarization_jp max_epoch=1 -- Training data loaded with shape: (10, 2)\n",
      "-- process=fine_tuning_progress model=t5_summarization_jp max_epoch=1 -- Input parameters:  {'algo': 'appNLP_summarization_final', 'mode': 'stage', 'max_epochs': '1', 'lang': 'jp', 'base_model': 't5_summarization_jp', 'batch_size': '4'}\n",
      "-- process=fine_tuning_progress model=t5_summarization_jp max_epoch=1 -- Epoch number: 1\n",
      "-- process=fine_tuning_progress model=t5_summarization_jp max_epoch=1 -- Base model: t5_summarization_jp\n",
      "-- process=fine_tuning_progress model=t5_summarization_jp max_epoch=1 -- Model Initialization: started\n",
      "-- process=fine_tuning_progress model=t5_summarization_jp max_epoch=1 -- Model file in /srv/app/model/data/summarization/jp/t5_summarization_jp\n",
      "-- process=fine_tuning_progress model=t5_summarization_jp max_epoch=1 -- Model Initialization: successfully finished\n",
      "-- process=fine_tuning_progress model=t5_summarization_jp max_epoch=1 -- #GPU memory --Total memory: 15634661376, --Memory reserved: 983564288, --Memory allocated: 891614208. #CPU: 0.25% occupied. #disk usage(total=156052275200, used=154578489344, free=1457008640)\n"
     ]
    }
   ],
   "source": [
    "model = init(df,param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 3 - fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "name": "mltkc_fit"
   },
   "outputs": [],
   "source": [
    "def fit(model,df,param):  \n",
    "    tag = \"-- process=fine_tuning_progress model={} max_epoch={} -- \".format(param['options']['model_name'], param['options']['params']['max_epochs'])\n",
    "    if \"batch_size\" in param['options']['params']:\n",
    "        print(tag + \"setting batch size to \", param['options']['params']['batch_size'])\n",
    "        batch_size_train = int(param['options']['params']['batch_size'])\n",
    "        batch_size_valid = int(param['options']['params']['batch_size'])\n",
    "\n",
    "    def preprocess_text(text):\n",
    "        text = re.sub(r'[\\r\\t\\n\\u3000]', '', text)\n",
    "        text = text.lower()\n",
    "        text = text.strip()\n",
    "        return text\n",
    "\n",
    "    data = df.query('text.notnull()', engine='python').query('summary.notnull()', engine='python')\n",
    "    data = data.assign(\n",
    "        text=lambda x: x.text.map(lambda y: preprocess_text(y)),\n",
    "        summary=lambda x: x.summary.map(lambda y: preprocess_text(y)))\n",
    "    # Data conversion\n",
    "    def convert_batch_data(train_data, valid_data, tokenizer):\n",
    "\n",
    "        def generate_batch(data):\n",
    "\n",
    "            batch_src, batch_tgt = [], []\n",
    "            for src, tgt in data:\n",
    "                batch_src.append(src)\n",
    "                batch_tgt.append(tgt)\n",
    "\n",
    "            batch_src = tokenizer(\n",
    "                batch_src, max_length=max_length_src, truncation=True, padding=\"max_length\", return_tensors=\"pt\"\n",
    "            )\n",
    "            batch_tgt = tokenizer(\n",
    "                batch_tgt, max_length=max_length_target, truncation=True, padding=\"max_length\", return_tensors=\"pt\"\n",
    "            )\n",
    "\n",
    "            return batch_src, batch_tgt\n",
    "\n",
    "        train_iter = DataLoader(train_data, batch_size=batch_size_train, shuffle=True, collate_fn=generate_batch)\n",
    "        valid_iter = DataLoader(valid_data, batch_size=batch_size_valid, shuffle=True, collate_fn=generate_batch)\n",
    "\n",
    "        return train_iter, valid_iter\n",
    "    MODEL_NAME = \"/srv/app/model/data/summarization\"\n",
    "    MODEL_NAME = os.path.join(MODEL_NAME, param['options']['params']['lang'], param['options']['params']['base_model'])\n",
    "    tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME, is_fast=True)\n",
    "    print(tag + \"tokenizer intialized\")\n",
    "    print(tag + \"Data vectorization: started\")\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        data['text'], data['summary'], test_size=0.15, random_state=42, shuffle=True\n",
    "    )\n",
    "\n",
    "    train_data = [(src, tgt) for src, tgt in zip(X_train, y_train)]\n",
    "    valid_data = [(src, tgt) for src, tgt in zip(X_test, y_test)]\n",
    "\n",
    "    train_iter, valid_iter = convert_batch_data(train_data, valid_data, tokenizer)\n",
    "    print(tag + \"Data vectorization: finished.\")\n",
    "    print(tag + \"#Training data: \" + str(len(train_data)) + \", #Test data: \" + str(len(valid_data)))\n",
    "\n",
    "    # Training function\n",
    "    def train(model, data, optimizer, PAD_IDX, i):\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        loop = 1\n",
    "        total = len(data)\n",
    "        losses = 0\n",
    "        for src, tgt in data:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            labels = tgt['input_ids'].to(device)\n",
    "            labels[labels[:, :] == PAD_IDX] = -100\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=src['input_ids'].to(device),\n",
    "                attention_mask=src['attention_mask'].to(device),\n",
    "                decoder_attention_mask=tgt['attention_mask'].to(device),\n",
    "                labels=labels\n",
    "            )\n",
    "            loss = outputs['loss']\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses += loss.item()\n",
    "\n",
    "            print(tag + \"Processed {}% of the {}-th epoch. Finished {} out of {} batches. Loss: {} \".format(round(loop/total*100), i, loop, total, round(losses / loop,2)), flush=True)\n",
    "            loop += 1\n",
    "\n",
    "        return losses / len(data)\n",
    "\n",
    "    # Loss function\n",
    "    def evaluate(model, data, PAD_IDX):\n",
    "\n",
    "        model.eval()\n",
    "        losses = 0\n",
    "        with torch.no_grad():\n",
    "            for src, tgt in data:\n",
    "\n",
    "                labels = tgt['input_ids'].to(device)\n",
    "                labels[labels[:, :] == PAD_IDX] = -100\n",
    "\n",
    "                outputs = model(\n",
    "                    input_ids=src['input_ids'].to(device),\n",
    "                    attention_mask=src['attention_mask'].to(device),\n",
    "                    decoder_attention_mask=tgt['attention_mask'].to(device),\n",
    "                    labels=labels\n",
    "                )\n",
    "                loss = outputs['loss']\n",
    "                losses += loss.item()\n",
    "\n",
    "        return losses / len(data)\n",
    "\n",
    "    epochs = int(param['options']['params']['max_epochs'])\n",
    "    MODEL_DIRECTORY = \"/srv/app/model/data/summarization\"\n",
    "    MODEL_DIRECTORY = os.path.join(MODEL_DIRECTORY, param['options']['params']['lang'], param['options']['model_name'])\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "    PAD_IDX = tokenizer.pad_token_id\n",
    "    best_loss = float('Inf')\n",
    "    best_model = None\n",
    "    counter = 1\n",
    "\n",
    "    print(tag + 'Model fine-tuning started with {} epochs'.format(epochs))\n",
    "\n",
    "    for loop in range(1, epochs + 1):\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        loss_train = train(model=model, data=train_iter, optimizer=optimizer, PAD_IDX=PAD_IDX, i=loop)\n",
    "\n",
    "        elapsed_time = time.time() - start_time\n",
    "\n",
    "        loss_valid = evaluate(model=model, data=valid_iter, PAD_IDX=PAD_IDX)\n",
    "        \n",
    "        t = torch.cuda.get_device_properties(0).total_memory\n",
    "        r = torch.cuda.memory_reserved(0)\n",
    "        a = torch.cuda.memory_allocated(0)\n",
    "        f = r-a  # free inside reserved\n",
    "        load1, load5, load15 = psutil.getloadavg()\n",
    "        cpu_usage = (load15/os.cpu_count()) * 100\n",
    "        stat = shutil.disk_usage(\"/\")\n",
    "        print(tag + \"#GPU memory --Total memory: {}, --Memory reserved: {}, --Memory allocated: {}. #CPU: {}% occupied. #disk {}\".format(t,r,a,cpu_usage,stat), flush=True)\n",
    "\n",
    "        print(tag + '[{}/{}] train loss: {:.4f}, valid loss: {:.4f} [{}{:.0f}s] counter: {} {}'.format(\n",
    "            loop, epochs, loss_train, loss_valid,\n",
    "            str(int(math.floor(elapsed_time / 60))) + 'm' if math.floor(elapsed_time / 60) > 0 else '',\n",
    "            elapsed_time % 60,\n",
    "            counter,\n",
    "            '**' if best_loss > loss_valid else ''\n",
    "        ),flush=True)\n",
    "\n",
    "        if best_loss > loss_valid:\n",
    "            best_loss = loss_valid\n",
    "            best_model = copy.deepcopy(model)\n",
    "            counter = 1\n",
    "        else:\n",
    "            if counter > patience:\n",
    "                break\n",
    "\n",
    "            counter += 1\n",
    "\n",
    "        tokenizer.save_pretrained(MODEL_DIRECTORY)\n",
    "        print(tag + \"tokenizer saved in \" + MODEL_DIRECTORY, flush=True)\n",
    "        best_model.model.save_pretrained(MODEL_DIRECTORY)\n",
    "        print(tag + \"model saved in \" + MODEL_DIRECTORY, flush=True)\n",
    "\n",
    "    print(tag + \"Model fine-tuning successfully finished\")\n",
    "    returns = {}\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- process=fine_tuning_progress model=t5_summarization_jp max_epoch=1 -- setting batch size to  4\n",
      "-- process=fine_tuning_progress model=t5_summarization_jp max_epoch=1 -- tokenizer intialized\n",
      "-- process=fine_tuning_progress model=t5_summarization_jp max_epoch=1 -- Data vectorization: started\n",
      "-- process=fine_tuning_progress model=t5_summarization_jp max_epoch=1 -- Data vectorization: finished.\n",
      "-- process=fine_tuning_progress model=t5_summarization_jp max_epoch=1 -- #Training data: 8, #Test data: 2\n",
      "-- process=fine_tuning_progress model=t5_summarization_jp max_epoch=1 -- Model fine-tuning started with 1 epochs\n",
      "-- process=fine_tuning_progress model=t5_summarization_jp max_epoch=1 -- Processed 50% of the 1-th epoch. Finished 1 out of 2 batches. Loss: 1.39 \n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 100.00 MiB (GPU 0; 14.56 GiB total capacity; 7.10 GiB already allocated; 23.50 MiB free; 7.65 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-e0fe780e5f4e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-f317d4b98458>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(model, df, param)\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mloss_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPAD_IDX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPAD_IDX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-f317d4b98458>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, data, optimizer, PAD_IDX, i)\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0mlosses\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 100.00 MiB (GPU 0; 14.56 GiB total capacity; 7.10 GiB already allocated; 23.50 MiB free; 7.65 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "fit(model,df,param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 4 - apply the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "name": "mltkc_apply"
   },
   "outputs": [],
   "source": [
    "def apply(model,df,param):\n",
    "    print(\"DEBUG: enter apply\")\n",
    "    print(param)\n",
    "    tag = \"-- process=apply_progress model={} max_epoch={} -- \".format(param['options']['model_name'], param['options']['params']['max_epochs'])\n",
    "    MODEL_DIRECTORY = \"/srv/app/model/data/summarization\"\n",
    "    MODEL_DIRECTORY = os.path.join(MODEL_DIRECTORY, param['options']['params']['lang'], param['options']['model_name'])\n",
    "    model = {}\n",
    "    print(MODEL_DIRECTORY)\n",
    "    model[\"tokenizer\"] = T5Tokenizer.from_pretrained(MODEL_DIRECTORY)\n",
    "    model[\"summarizer\"] = T5ForConditionalGeneration.from_pretrained(MODEL_DIRECTORY)\n",
    "    print(\"DEBUG: model inited\")\n",
    "    X = df[param['feature_variables'][0]].values.tolist()\n",
    "    temp_data=list()\n",
    "    print(tag + \"apply function read inputs\")\n",
    "    for i in range(len(X)):\n",
    "        batch = model[\"tokenizer\"](str(X[i]), max_length=400, truncation=True, return_tensors=\"pt\")\n",
    "        outputs = model[\"summarizer\"].generate(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'], max_length=400,repetition_penalty=8.0,num_beams=15)\n",
    "        summary = [model[\"tokenizer\"].decode(ids, skip_special_tokens=True, clean_up_tokenization_spaces=False) for ids in outputs]\n",
    "        temp_data += summary\n",
    "    cols={\"summary\": temp_data}\n",
    "    returns=pd.DataFrame(data=cols)\n",
    "    print(tag + \"apply function successfully finished\")\n",
    "        \n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "debug: start apply\n",
      "debug: read input\n",
      "numpy version: 1.22.1\n",
      "debug: start tokenizing\n",
      "debug: finish tok and start summarizing\n",
      "debug: finish summarizing and start decoding\n",
      "debug: finish decoding\n",
      "debug: finished\n",
      "無線のプレミアムバージョンに加入しているユーザー番号は100145。通常加入よりも高速であるはずなのに、とても遅いという。お客様が契約したパッケージの説明とまったく同じだそう\n"
     ]
    }
   ],
   "source": [
    "returns = apply(model,df,param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 5 - save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "name": "mltkc_save"
   },
   "outputs": [],
   "source": [
    "# save model to name in expected convention \"<algo_name>_<model_name>.h5\"\n",
    "def save(model, name):\n",
    "    return {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 6 - load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "name": "mltkc_load"
   },
   "outputs": [],
   "source": [
    "# load model from name in expected convention \"<algo_name>_<model_name>.h5\"\n",
    "def load(path):\n",
    "    print(\"DEBUG: load\")\n",
    "    model = {}\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 7 - provide a summary of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "name": "mltkc_summary"
   },
   "outputs": [],
   "source": [
    "# return model summary\n",
    "def summary(model=None):\n",
    "    returns = {}\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of Stages\n",
    "All subsequent cells are not tagged and can be used for further freeform code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG using SaaS LLM - This notebook contains sample code for using AzureAI, Bedrock or Gemeni as LLM module. Please customize based on your specific use case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example Usage: | makeresults\n",
    "| eval query = \"How to create an ivestigation on Splunk ES?\"\n",
    "| fit MLTKContainer algo=llm_rag_script_saas_llm embedder_name=\"all-MiniLM-L6-v2\" embedder_dimension=384 collection_name=\"document_collection_splunk\" top_k=4 rag_type=Documents query into app:llm_rag_script_saas_llm as RAG\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 0 - import libraries\n",
    "At stage 0 we define all imports necessary to run our subsequent code depending on various libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "name": "mltkc_import"
   },
   "outputs": [],
   "source": [
    "# this definition exposes all python module imports that should be available in all subsequent commands\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pymilvus\n",
    "from pymilvus import (\n",
    "    connections,\n",
    "    utility,\n",
    "    FieldSchema, CollectionSchema, DataType,\n",
    "    Collection,\n",
    ")\n",
    "import llama_index\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Document, StorageContext, ServiceContext\n",
    "from llama_index.vector_stores.milvus import MilvusVectorStore\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.llms.azure_openai import AzureOpenAI\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.llms.bedrock import Bedrock\n",
    "from llama_index.llms.gemini import Gemini\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "import textwrap\n",
    "from llama_index.core.llms import ChatMessage, MessageRole\n",
    "from llama_index.core import ChatPromptTemplate\n",
    "\n",
    "\n",
    "# ...\n",
    "# global constants\n",
    "MODEL_DIRECTORY = \"/srv/app/model/data/\"\n",
    "LLM_ENDPOINT = \"http://ollama:11434\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some logging settings \n",
    "import logging\n",
    "import sys\n",
    "import llama_index.core\n",
    "from llama_index.core.callbacks import (\n",
    "    CallbackManager,\n",
    "    LlamaDebugHandler,\n",
    "    CBEventType,\n",
    ")\n",
    "\n",
    "llama_index.core.set_global_handler(\"simple\")\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "llama_debug = LlamaDebugHandler(print_trace_on_end=True)\n",
    "callback_manager = CallbackManager([llama_debug])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy version: 1.26.4\n",
      "pandas version: 2.2.0\n"
     ]
    }
   ],
   "source": [
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",
    "print(\"numpy version: \" + np.__version__)\n",
    "print(\"pandas version: \" + pd.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "name": "mltkc_stage"
   },
   "outputs": [],
   "source": [
    "# this cell is not executed from MLTK and should only be used for staging data into the notebook environment\n",
    "def stage(name):\n",
    "    with open(\"data/\"+name+\".csv\", 'r') as f:\n",
    "        df = pd.read_csv(f)\n",
    "    with open(\"data/\"+name+\".json\", 'r') as f:\n",
    "        param = json.load(f)\n",
    "    return df, param"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 2 - create and initialize a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "name": "mltkc_init"
   },
   "outputs": [],
   "source": [
    "# initialize your model\n",
    "# available inputs: data and parameters\n",
    "# returns the model object which will be used as a reference to call fit, apply and summary subsequently\n",
    "def init(df,param):\n",
    "    model = {}\n",
    "    model['hyperparameter'] = 42.0\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 3 - fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "name": "mltkc_fit"
   },
   "outputs": [],
   "source": [
    "# train your model\n",
    "# returns a fit info json object and may modify the model object\n",
    "def fit(model,df,param):\n",
    "    info = {\"message\": \"model trained\"}\n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'message': 'model trained'}\n"
     ]
    }
   ],
   "source": [
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",
    "print(fit(model,df,param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 4 - apply the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "name": "mltkc_apply"
   },
   "outputs": [],
   "source": [
    "# apply your model\n",
    "# returns the calculated results\n",
    "def apply(model,df,param):\n",
    "    # \"Documents\" or \"Logs\"\n",
    "    try:\n",
    "        d_type = param['options']['params']['rag_type'].strip('\\\"')\n",
    "    except:\n",
    "        d_type = \"Documents\"\n",
    "    \n",
    "    X = df[\"query\"].values.tolist()\n",
    "    use_local= int(param['options']['params']['use_local'])\n",
    "    try:\n",
    "        embedder_name = param['options']['params']['embedder_name'].strip('\\\"')\n",
    "    except:\n",
    "        embedder_name = 'all-MiniLM-L6-v2'\n",
    "\n",
    "    try:\n",
    "        collection_name = param['options']['params']['collection_name'].strip('\\\"')\n",
    "    except:\n",
    "        collection_name = \"default-doc-collection\"\n",
    "\n",
    "    if embedder_name == 'intfloat/multilingual-e5-large':\n",
    "        embedder_dimension = 1024\n",
    "    elif embedder_name == 'all-MiniLM-L6-v2':\n",
    "        embedder_dimension = 384\n",
    "    else:\n",
    "        try:\n",
    "            embedder_dimension = int(param['options']['params']['embedder_dimension'])\n",
    "        except:\n",
    "            embedder_dimension = 384\n",
    "    if use_local:\n",
    "        embedder_name = f'/srv/app/model/data/{embedder_name}'\n",
    "        print(\"Using local embedding model checkpoints\")\n",
    "    try:\n",
    "        top_k = int(param['options']['params']['top_k'])\n",
    "    except:\n",
    "        top_k = 5\n",
    "        \n",
    "    if d_type == \"Documents\":\n",
    "        qa_prompt_str = (\n",
    "            \"Below are the context information.\\n\"\n",
    "            \"---------------------\\n\"\n",
    "            \"{context_str}\\n\"\n",
    "            \"---------------------\\n\"\n",
    "            \"Given the context information as well as necessary prior knowledge, \"\n",
    "            \"answer the question: {query_str}\\n\"\n",
    "        )\n",
    "        chat_text_qa_msgs = [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"You are an expert Q&A system that is trusted around the world. Always answer the query using the provided context information and reasoning as detailed as possible\",\n",
    "            ),\n",
    "            (\"user\", qa_prompt_str),\n",
    "        ]\n",
    "    else:\n",
    "        qa_prompt_str = (\n",
    "            \"Past log messages below are given as context information.\\n\"\n",
    "            \"---------------------\\n\"\n",
    "            \"{context_str}\\n\"\n",
    "            \"---------------------\\n\"\n",
    "            \"Given the context information as well as necessary prior knowledge, \"\n",
    "            \"answer the question: {query_str}\\n\"\n",
    "        )\n",
    "        chat_text_qa_msgs = [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"You are an expert Q&A system that is trusted around the world. Always answer the query using the provided context information and reasoning as detailed as possible\",\n",
    "            ),\n",
    "            (\"user\", qa_prompt_str),\n",
    "        ]\n",
    "    \n",
    "    text_qa_template = ChatPromptTemplate.from_messages(chat_text_qa_msgs)\n",
    "\n",
    "    # Example for SaaS LLM service integration\n",
    "    llm_service = \"\"\n",
    "    if llm_service == \"azure\":\n",
    "        ## Azure AI Example\n",
    "        api_key = \"XXXXXXXXXXXXXXXX\"\n",
    "        azure_endpoint = \"https://XXXX.openai.azure.com\"\n",
    "        api_version = \"API-VERSION\"\n",
    "        \n",
    "        llm = AzureOpenAI(\n",
    "            model=\"MODEL_NAME\",\n",
    "            deployment_name=\"DEPLOYMENT_NAME\",\n",
    "            api_key=api_key,\n",
    "            azure_endpoint=azure_endpoint,\n",
    "            api_version=api_version,\n",
    "        )\n",
    "    elif llm_service == \"bedrock\":\n",
    "        ## Bedrock Example\n",
    "        llm = Bedrock(\n",
    "            model=\"amazon.titan-text-express-v1\",\n",
    "            aws_access_key_id=\"AWS Access Key ID to use\",\n",
    "            aws_secret_access_key=\"AWS Secret Access Key to use\",\n",
    "            aws_session_token=\"AWS Session Token to use\",\n",
    "            aws_region_name=\"AWS Region to use, eg. us-east-1\",\n",
    "        )\n",
    "    elif llm_service == \"gemini\":\n",
    "        ## Gemeni Example\n",
    "        GOOGLE_API_KEY = \"YOUR GOOGLE API KEY\"  \n",
    "        os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
    "        llm = Gemini()\n",
    "    else:\n",
    "        llm = None\n",
    "\n",
    "    try:\n",
    "        transformer_embedder = HuggingFaceEmbedding(model_name=embedder_name)\n",
    "        service_context = ServiceContext.from_defaults(\n",
    "            llm=llm, embed_model=transformer_embedder, chunk_size=1024\n",
    "        )\n",
    "    except:\n",
    "        cols = {\"Response\": [\"ERROR: Could not load embedder\"], \"References\": [\"ERROR\"]}\n",
    "        result = pd.DataFrame(data=cols)\n",
    "        return result\n",
    "    try:\n",
    "        if d_type == \"Documents\":\n",
    "            vector_store = MilvusVectorStore(uri=\"http://milvus-standalone:19530\", token=\"\", collection_name=collection_name, dim=embedder_dimension, overwrite=False)\n",
    "        else:\n",
    "            vector_store = MilvusVectorStore(uri=\"http://milvus-standalone:19530\", token=\"\", collection_name=collection_name, embedding_field='embeddings', text_key='label', dim=embedder_dimension, overwrite=False)\n",
    "        index = VectorStoreIndex.from_vector_store(\n",
    "           vector_store=vector_store, service_context=service_context\n",
    "        )\n",
    "        query_engine = index.as_query_engine(similarity_top_k=top_k, text_qa_template=text_qa_template)\n",
    "    except:\n",
    "        cols = {\"Response\": [\"ERROR: Could not load collection\"], \"References\": [\"ERROR\"]}\n",
    "        result = pd.DataFrame(data=cols)\n",
    "        return result\n",
    "        \n",
    "    l = []\n",
    "    f = []\n",
    "    for i in range(len(X)):\n",
    "        r = query_engine.query(X[i])\n",
    "        l.append(r.response)\n",
    "        if d_type == \"Documents\":\n",
    "            files = \"\"\n",
    "            for node in r.source_nodes:\n",
    "                files += node.node.metadata['file_path']\n",
    "                files += \"\\n\"\n",
    "            f.append(files)\n",
    "        else:\n",
    "            logs = \"\"\n",
    "            for i in range(len(r.source_nodes)):\n",
    "                logs += r.source_nodes[0].text\n",
    "                logs += \"\\n\"\n",
    "            f.append(logs)       \n",
    "    \n",
    "    cols = {\"Response\": l, \"References\": f}\n",
    "    result = pd.DataFrame(data=cols)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",
    "print(apply(None,None,None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 5 - save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "name": "mltkc_save"
   },
   "outputs": [],
   "source": [
    "# save model to name in expected convention \"<algo_name>_<model_name>\"\n",
    "def save(model,name):\n",
    "    with open(MODEL_DIRECTORY + name + \".json\", 'w') as file:\n",
    "        json.dump(model, file)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 6 - load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "name": "mltkc_load"
   },
   "outputs": [],
   "source": [
    "# load model from name in expected convention \"<algo_name>_<model_name>\"\n",
    "def load(name):\n",
    "    model = {}\n",
    "    with open(MODEL_DIRECTORY + name + \".json\", 'r') as file:\n",
    "        model = json.load(file)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 7 - provide a summary of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "name": "mltkc_summary"
   },
   "outputs": [],
   "source": [
    "# return a model summary\n",
    "def summary(model=None):\n",
    "    returns = {\"version\": {\"numpy\": np.__version__, \"pandas\": pd.__version__} }\n",
    "    return returns\n",
    "\n",
    "def compute(model,df,param):\n",
    "    # \"Documents\" or \"Logs\"\n",
    "    try:\n",
    "        d_type = param['params']['rag_type'].strip('\\\"')\n",
    "    except:\n",
    "        d_type = \"Documents\"\n",
    "    \n",
    "    X = df[0][\"query\"]\n",
    "    use_local= int(param['params']['use_local'])\n",
    "    try:\n",
    "        embedder_name = param['params']['embedder_name'].strip('\\\"')\n",
    "    except:\n",
    "        embedder_name = 'all-MiniLM-L6-v2'\n",
    "\n",
    "    try:\n",
    "        collection_name = param['params']['collection_name'].strip('\\\"')\n",
    "    except:\n",
    "        collection_name = \"default-doc-collection\"\n",
    "\n",
    "    if embedder_name == 'intfloat/multilingual-e5-large':\n",
    "        embedder_dimension = 1024\n",
    "    elif embedder_name == 'all-MiniLM-L6-v2':\n",
    "        embedder_dimension = 384\n",
    "    else:\n",
    "        try:\n",
    "            embedder_dimension = int(param['params']['embedder_dimension'])\n",
    "        except:\n",
    "            embedder_dimension = 384\n",
    "    if use_local:\n",
    "        embedder_name = f'/srv/app/model/data/{embedder_name}'\n",
    "        print(\"Using local embedding model checkpoints\")\n",
    "    try:\n",
    "        top_k = int(param['params']['top_k'])\n",
    "    except:\n",
    "        top_k = 5\n",
    "        \n",
    "    if d_type == \"Documents\":\n",
    "        qa_prompt_str = (\n",
    "            \"Below are the context information.\\n\"\n",
    "            \"---------------------\\n\"\n",
    "            \"{context_str}\\n\"\n",
    "            \"---------------------\\n\"\n",
    "            \"Given the context information as well as necessary prior knowledge, \"\n",
    "            \"answer the question: {query_str}\\n\"\n",
    "        )\n",
    "        chat_text_qa_msgs = [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"You are an expert Q&A system that is trusted around the world. Always answer the query using the provided context information and reasoning as detailed as possible\",\n",
    "            ),\n",
    "            (\"user\", qa_prompt_str),\n",
    "        ]\n",
    "    else:\n",
    "        qa_prompt_str = (\n",
    "            \"Past log messages below are given as context information.\\n\"\n",
    "            \"---------------------\\n\"\n",
    "            \"{context_str}\\n\"\n",
    "            \"---------------------\\n\"\n",
    "            \"Given the context information as well as necessary prior knowledge, \"\n",
    "            \"answer the question: {query_str}\\n\"\n",
    "        )\n",
    "        chat_text_qa_msgs = [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"You are an expert Q&A system that is trusted around the world. Always answer the query using the provided context information and reasoning as detailed as possible\",\n",
    "            ),\n",
    "            (\"user\", qa_prompt_str),\n",
    "        ]\n",
    "    \n",
    "    text_qa_template = ChatPromptTemplate.from_messages(chat_text_qa_msgs)\n",
    "\n",
    "    # Example for SaaS LLM service integration\n",
    "    llm_service = \"\"\n",
    "    if llm_service == \"azure\":\n",
    "        ## Azure AI Example\n",
    "        api_key = \"XXXXXXXXXXXXXXXX\"\n",
    "        azure_endpoint = \"https://XXXX.openai.azure.com\"\n",
    "        api_version = \"API-VERSION\"\n",
    "        \n",
    "        llm = AzureOpenAI(\n",
    "            model=\"MODEL_NAME\",\n",
    "            deployment_name=\"DEPLOYMENT_NAME\",\n",
    "            api_key=api_key,\n",
    "            azure_endpoint=azure_endpoint,\n",
    "            api_version=api_version,\n",
    "        )\n",
    "    elif llm_service == \"bedrock\":\n",
    "        ## Bedrock Example\n",
    "        llm = Bedrock(\n",
    "            model=\"amazon.titan-text-express-v1\",\n",
    "            aws_access_key_id=\"AWS Access Key ID to use\",\n",
    "            aws_secret_access_key=\"AWS Secret Access Key to use\",\n",
    "            aws_session_token=\"AWS Session Token to use\",\n",
    "            aws_region_name=\"AWS Region to use, eg. us-east-1\",\n",
    "        )\n",
    "    elif llm_service == \"gemini\":\n",
    "        ## Gemeni Example\n",
    "        GOOGLE_API_KEY = \"YOUR GOOGLE API KEY\"  \n",
    "        os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
    "        llm = Gemini()\n",
    "    else:\n",
    "        llm = None\n",
    "\n",
    "    try:\n",
    "        transformer_embedder = HuggingFaceEmbedding(model_name=embedder_name)\n",
    "        service_context = ServiceContext.from_defaults(\n",
    "            llm=llm, embed_model=transformer_embedder, chunk_size=1024\n",
    "        )\n",
    "    except:\n",
    "        cols = {\"Response\": \"ERROR: Could not load embedder\", \"References\": \"ERROR\"}\n",
    "        result = [cols]\n",
    "        return result\n",
    "    try:\n",
    "        if d_type == \"Documents\":\n",
    "            vector_store = MilvusVectorStore(uri=\"http://milvus-standalone:19530\", token=\"\", collection_name=collection_name, dim=embedder_dimension, overwrite=False)\n",
    "        else:\n",
    "            vector_store = MilvusVectorStore(uri=\"http://milvus-standalone:19530\", token=\"\", collection_name=collection_name, embedding_field='embeddings', text_key='label', dim=embedder_dimension, overwrite=False)\n",
    "        index = VectorStoreIndex.from_vector_store(\n",
    "           vector_store=vector_store, service_context=service_context\n",
    "        )\n",
    "        query_engine = index.as_query_engine(similarity_top_k=top_k, text_qa_template=text_qa_template)\n",
    "    except:\n",
    "        cols = {\"Response\": \"ERROR: Could not load collection\", \"References\": \"ERROR\"}\n",
    "        result = [cols]\n",
    "        return result\n",
    "        \n",
    "\n",
    "    \n",
    "    r = query_engine.query(X)\n",
    "    l = r.response\n",
    "    if d_type == \"Documents\":\n",
    "        files = \"\"\n",
    "        for node in r.source_nodes:\n",
    "            files += node.node.metadata['file_path']\n",
    "            files += \"\\n\"\n",
    "    else:\n",
    "        files = \"\"\n",
    "        for i in range(len(r.source_nodes)):\n",
    "            files += r.source_nodes[0].text\n",
    "            files += \"\\n\"     \n",
    "    \n",
    "    cols = {\"Response\": l, \"References\": files}\n",
    "    result = [cols]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After implementing your fit, apply, save and load you can train your model:<br>\n",
    "| makeresults count=10<br>\n",
    "| streamstats c as i<br>\n",
    "| eval s = i%3<br>\n",
    "| eval feature_{s}=0<br>\n",
    "| foreach feature_* [eval &lt;&lt;FIELD&gt;&gt;=random()/pow(2,31)]<br>\n",
    "| fit MLTKContainer algo=barebone s from feature_* into app:barebone_model<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or apply your model:<br>\n",
    "| makeresults count=10<br>\n",
    "| streamstats c as i<br>\n",
    "| eval s = i%3<br>\n",
    "| eval feature_{s}=0<br>\n",
    "| foreach feature_* [eval &lt;&lt;FIELD&gt;&gt;=random()/pow(2,31)]<br>\n",
    "| apply barebone_model as the_meaning_of_life"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Send data back to Splunk HEC\n",
    "When you configured the Splunk HEC Settings in the DSDL app you can easily send back data to an index with [Splunk's HTTP Event Collector (HEC)](https://docs.splunk.com/Documentation/Splunk/latest/Data/UsetheHTTPEventCollector). Read more about data formats and options in the [documentation](https://docs.splunk.com/Documentation/Splunk/latest/Data/FormateventsforHTTPEventCollector#Event_metadata).\n",
    "\n",
    "### Use cases\n",
    "- you want to offload longer running, possibly distributed computations that need to deliver results asynchroneously back into Splunk. \n",
    "- you might not want to present results back into the search pipeline after your `| fit` or `| apply` command. \n",
    "- you can easily utilize this approach for any logging purposes or other profiling tasks in your ML code so you can actively monitor and analyze your processes.\n",
    "\n",
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dsdlsupport import SplunkHEC as SplunkHEC\n",
    "hec = SplunkHEC.SplunkHEC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example to send 10 hello world events\n",
    "response = hec.send_hello_world(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HEC endpoint http://host.docker.internal:8088/services/collector/event \n",
      "returned with status code 200 \n",
      "and response message: {\"text\":\"Success\",\"code\":0}\n"
     ]
    }
   ],
   "source": [
    "print(\"HEC endpoint %s \\nreturned with status code %s \\nand response message: %s\" % (response.url, response.status_code, response.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example to send a JSON object, e.g. to log some data\n",
    "from datetime import datetime\n",
    "response = hec.send({'event': {'message': 'operation done', 'log_level': 'INFO' }, 'time': datetime.now().timestamp()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HEC endpoint http://host.docker.internal:8088/services/collector/event \n",
      "returned with status code 200 \n",
      "and response message: {\"text\":\"Success\",\"code\":0}\n"
     ]
    }
   ],
   "source": [
    "print(\"HEC endpoint %s \\nreturned with status code %s \\nand response message: %s\" % (response.url, response.status_code, response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of Stages\n",
    "All subsequent cells are not tagged and can be used for further freeform code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

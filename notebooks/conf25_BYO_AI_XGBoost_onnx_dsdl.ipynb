{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6WHobqZGottS"
   },
   "source": [
    "**Install Required Packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z_t9BAM0LstX",
    "outputId": "0218a9ef-dcec-49a0-f468-c37d576eebe2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in /usr/local/lib64/python3.9/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in /usr/local/lib64/python3.9/site-packages (1.26.4)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib64/python3.9/site-packages (1.6.1)\n",
      "Requirement already satisfied: xgboost in /dltk/.local/lib/python3.9/site-packages (2.1.4)\n",
      "Requirement already satisfied: skl2onnx in /dltk/.local/lib/python3.9/site-packages (1.19.1)\n",
      "Requirement already satisfied: onnx in /dltk/.local/lib/python3.9/site-packages (1.18.0)\n",
      "Requirement already satisfied: onnxmltools in /dltk/.local/lib/python3.9/site-packages (1.14.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.9/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.9/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.9/site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.9/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib64/python3.9/site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12 in /dltk/.local/lib/python3.9/site-packages (from xgboost) (2.27.5)\n",
      "Requirement already satisfied: protobuf>=4.25.1 in /usr/local/lib64/python3.9/site-packages (from onnx) (4.25.7)\n",
      "Requirement already satisfied: typing_extensions>=4.7.1 in /usr/local/lib/python3.9/site-packages (from onnx) (4.13.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 25.1.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas numpy scikit-learn xgboost skl2onnx onnx onnxmltools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y3as3nG3Lqw4"
   },
   "source": [
    "Restart kernal to use updated packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2TCjye3OLotf"
   },
   "source": [
    "***Insert code to restart kernal to use updated packages***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "lL4Wwmn0Lxnw"
   },
   "outputs": [],
   "source": [
    "#restart session\n",
    "exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Gaype-mTLotf"
   },
   "outputs": [],
   "source": [
    "import pandas as pd  # For DataFrame operations (pd.read_csv, pd.cut, pd.merge, etc.)\n",
    "import numpy as np  # For numerical operations and handling NaN (implied by fillna and array operations)\n",
    "from sklearn.model_selection import train_test_split  # For splitting data into training and validation sets\n",
    "from sklearn.model_selection import GridSearchCV  # For hyperparameter tuning with grid search\n",
    "from sklearn.metrics import f1_score, make_scorer, accuracy_score # For F1 score and custom scoring in GridSearchCV\n",
    "import xgboost as xgb  # For XGBoost classifier and DMatrix operations\n",
    "from onnxmltools.convert import convert_xgboost # For converting XGBoost model to ONNX format\n",
    "from skl2onnx.common.data_types import FloatTensorType  # For defining ONNX input types\n",
    "import onnx  # For saving the ONNX model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kbRTfXxWohSh"
   },
   "source": [
    "**Import URL Phishing Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "LtHFHtODLotf"
   },
   "outputs": [],
   "source": [
    "# this cell is not executed from MLTK and should only be used for staging data into the notebook environment\n",
    "def stage(name):\n",
    "    with open(\"data/\"+name+\".csv\", 'r') as f:\n",
    "        df = pd.read_csv(f)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Lp1B61rFLotf",
    "outputId": "480630f5-fc5c-4872-ed62-d0b621d4f23d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       FILENAME                                                URL  URLLength  \\\n",
      "0  mw205674.txt                        http://www.danangluxury.com         26   \n",
      "1    712147.txt                    https://www.leedstownhall.co.uk         30   \n",
      "2    806061.txt                      https://www.mexicancafe.co.nz         28   \n",
      "3    164934.txt                         https://www.usglassmag.com         25   \n",
      "4   8131216.txt  https://cloudflare-ipfs.com/ipfs/bafybeicivf4l...         93   \n",
      "\n",
      "                    Domain  DomainLength  IsDomainIP  TLD  URLSimilarityIndex  \\\n",
      "0     www.danangluxury.com          20.0         0.0  com           75.000000   \n",
      "1  www.leedstownhall.co.uk          23.0         0.0   uk          100.000000   \n",
      "2    www.mexicancafe.co.nz          21.0         0.0   nz          100.000000   \n",
      "3       www.usglassmag.com          18.0         0.0  com          100.000000   \n",
      "4      cloudflare-ipfs.com          19.0         0.0  com           23.030879   \n",
      "\n",
      "   CharContinuationRate  TLDLegitimateProb  ...  Pay  Crypto  \\\n",
      "0              1.000000           0.522907  ...  0.0     0.0   \n",
      "1              0.875000           0.028555  ...  0.0     0.0   \n",
      "2              0.857143           0.001993  ...  1.0     0.0   \n",
      "3              1.000000           0.522907  ...  1.0     0.0   \n",
      "4              0.733333           0.522907  ...  0.0     0.0   \n",
      "\n",
      "   HasCopyrightInfo  NoOfImage  NoOfCSS  NoOfJS  NoOfSelfRef  NoOfEmptyRef  \\\n",
      "0               0.0        0.0      0.0     2.0          0.0           0.0   \n",
      "1               1.0       37.0     41.0    46.0        104.0           3.0   \n",
      "2               0.0       14.0      4.0    14.0         51.0           0.0   \n",
      "3               1.0       40.0     26.0    25.0        106.0           9.0   \n",
      "4               0.0        0.0      0.0     0.0          0.0           0.0   \n",
      "\n",
      "   NoOfExternalRef  label  \n",
      "0              0.0    0.0  \n",
      "1             93.0    1.0  \n",
      "2              7.0    1.0  \n",
      "3            193.0    1.0  \n",
      "4              0.0    0.0  \n",
      "\n",
      "[5 rows x 56 columns]\n",
      "(71742, 56)\n"
     ]
    }
   ],
   "source": [
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing purposes\n",
    "df = stage(\"train\")\n",
    "print(df.head())\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bt972B0Do45W"
   },
   "source": [
    "**Data Preparation of Training and Test Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0kKJRXJ4KwPw",
    "outputId": "6f1a9022-4998-4498-c52b-010204bb68d6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3566/231294607.py:5: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  range_count=df.groupby('URL_LengthRange').apply(lambda x:(x['label']==0).mean())*100\n",
      "/tmp/ipykernel_3566/231294607.py:5: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  range_count=df.groupby('URL_LengthRange').apply(lambda x:(x['label']==0).mean())*100\n"
     ]
    }
   ],
   "source": [
    "#Create bins for URL length\n",
    "labels=['0-25', '25-30', '30-35', '35-40', '40-45', '45-50', '50-100']\n",
    "bins=[0,25,30,35,40,45,50,100]\n",
    "df['URL_LengthRange']=pd.cut(df['URLLength'], bins=bins, labels=labels, right=True)\n",
    "range_count=df.groupby('URL_LengthRange').apply(lambda x:(x['label']==0).mean())*100\n",
    "\n",
    "#Drop isHTTP to avoid overfitting\n",
    "df=df.drop(columns=['IsHTTPS'])\n",
    "\n",
    "#Create feature buckets for the TLD URL Phishing percentage. 4 buckets of 25% each\n",
    "#Count of URLs with each TLD and number of URLs flagged as Phishing\n",
    "tld_dist=df.groupby('TLD')['label'].agg(['count','sum'])\n",
    "\n",
    "#Perc of URLs that are phishing\n",
    "tld_dist['phishing_perc']=tld_dist['sum']/tld_dist['count']*100\n",
    "def categorize(phish_perc):\n",
    "  if phish_perc<=0:\n",
    "    return 1\n",
    "  elif phish_perc<=25:\n",
    "    return 2\n",
    "  elif phish_perc<=50:\n",
    "    return 3\n",
    "  elif phish_perc<=75:\n",
    "    return 4\n",
    "  elif phish_perc<100:\n",
    "    return 5\n",
    "  else:\n",
    "    return 6\n",
    "\n",
    "#Apply function to add phish percent number to tld df and add that to the main df.\n",
    "#Also remove 'TLD' as no longer needed with new feature\n",
    "tld_dist['percent group']=tld_dist['phishing_perc'].apply(categorize)\n",
    "\n",
    "#left outer join (original df) to preseve order\n",
    "df=df.merge(tld_dist[['percent group']], on='TLD', how='left')\n",
    "df=df.drop(columns=['TLD'])\n",
    "\n",
    "#Now we need to assure that any features added to our training set are also reflected in our test set\n",
    "\n",
    "#Import test data\n",
    "#df_test=pd.read_csv('test.csv')\n",
    "df_test = stage(\"test\")\n",
    "\n",
    "#repeat feature engineering for test set\n",
    "#df_test['URL_LengthRange']=pd.cut(df_test['URLLength'], bins=bins, labels=labels, right=True)\n",
    "df_test=df_test.drop(columns=['IsHTTPS'])\n",
    "\n",
    "#Apply function to add TLD percent group to test dataframe and remove 'TLD' column\n",
    "df_test=df_test.merge(tld_dist[['percent group']], on='TLD', how='left')\n",
    "df_test=df_test.drop(columns=['TLD'])\n",
    "\n",
    "#Features associated with characters in the URL\n",
    "df['ObfuscationRatio']=df['ObfuscationRatio']*100\n",
    "df['LetterRatioInURL']=df['LetterRatioInURL']*100\n",
    "df['DigitRatioInURL']=df['DegitRatioInURL']*100\n",
    "df['SpecialCharRatioInURL']=df['SpacialCharRatioInURL']*100\n",
    "\n",
    "#digit and special char fields were spelled wrong so I made a new field for them\n",
    "df=df.drop(columns=['DegitRatioInURL','SpacialCharRatioInURL'])\n",
    "\n",
    "#Do the same fix for Test set to avoid issues\n",
    "df_test['SpecialCharRatioInURL']=df_test['SpacialCharRatioInURL']\n",
    "df_test['DigitRatioInURL']=df_test['DegitRatioInURL']\n",
    "df_test=df_test.drop(columns=['DegitRatioInURL','SpacialCharRatioInURL'])\n",
    "\n",
    "#drop columns with potential leakage\n",
    "X_raw=df.drop(columns=['label','URL','Domain','Title','FILENAME', 'URL_LengthRange'])\n",
    "\n",
    "#y labels\n",
    "y=df['label']\n",
    "\n",
    "#reserve original test df and remove leakage from test df\n",
    "df_test_OG=df_test.copy()\n",
    "\n",
    "#drop columns from test to avoid leakage\n",
    "df_test=df_test.drop(columns=['URL','Domain','Title','FILENAME'])\n",
    "columns_wNANs=X_raw.columns[X_raw.isna().any()].tolist()\n",
    "\n",
    "#fill null columns with nan\n",
    "X_raw=X_raw.fillna(0)\n",
    "df_test=df_test.fillna(0)\n",
    "X=X_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JUykO8GjSd6Z"
   },
   "source": [
    "All of the cells above up until this point represent the following:\n",
    "\n",
    "\n",
    "*   Importing of the data\n",
    "*   Importing the necessary libraries and functions\n",
    "*   Data Analysis\n",
    "*   Feature Engineering\n",
    "\n",
    "Data Analysis: Taught us that the length of the URL was extremely indicative of whether the URL was safe or not. We also found that all URLs with HTTP protocal was 100% classified as phishing. We chose to drop this field to avoid overfitting.\n",
    "\n",
    "Feature Engineering: We created a feature to present the likelihood a URLs TLD (.com, .org, etc.) was phishing or not. This allowed each data point to be placed in a phishing percent bucket based on their TLD. We know that characters within a URL have a strong correlation to it's safety, so we scaled our URL character features (those obfuscated, digits, letter and special characters). We did that by mutliplying them by 100 and making them more impactful and prominent in the algorithm. We dropped characters with potential data leakage and null values and we made sure to mimic all feature engineering done to the training set also on our test set for later use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hHwERu7hStTW"
   },
   "source": [
    "**Create Cross Validation Set**\n",
    "\n",
    "We split our training data into a training and a cross validation set. This will help us evaluate the model and parameters during the training set and also help prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "tKez01vWSuiX"
   },
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val= train_test_split(X, y, test_size=.2, random_state=14)\n",
    "X_train.head()\n",
    "#reorder columns of test to ensure they match order or X_train\n",
    "df_test=df_test[X_train.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UKR8FkymWMfB"
   },
   "source": [
    "**Grid Search for Optimal Parameters**\n",
    "\n",
    "We are going to search for our model's most optimal parameters via a cross validated grid search over our specified grid.\n",
    "\n",
    "Some notes on our parameters:\n",
    "* loss: Default='log_loss' logistic regression good for classification\n",
    "* learning_rate: shrinks contribution of each tree by rate designated\n",
    "* n_estimators: Number of boosting stages to perform. Gradient boosting tends to be more. As mentioned this algoritgm is quire resilient to overfitting so large number of stages may lead to better performance.\n",
    "* min_samples_split: Minimum number of samples required to split an internal node\n",
    "* max_depth: Max depth of individual regression estimators which limits the number of nodes in a single tree. Value of this is dependent on number of input variables. If none, than all nodes expanded until all leaves are pure.\n",
    "* subsample: Fraction of samples to be used for fitting the individual base learners.\n",
    "* For scoring we will use F1 as a measure of our models predictive power. F1 being a harmonic mean between accurate positive predicted cases out of all positive predicted cases (Precision) and the positive predicted cases out of all positive cases\n",
    "* CV: Number of folds used for cross validation\n",
    "* n-jobs: Number of jobs to run in parallel. 1 for none and -1 for all processors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OyvF8hOBXtHE",
    "outputId": "3dda9092-757b-4667-b66e-f138913759b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "Optimal parameters for model: {'learning_rate': 0.03, 'max_depth': 5, 'n_estimators': 30} \n",
      "Best CV F1 score on training set: 0.9999\n"
     ]
    }
   ],
   "source": [
    "#Testing various parameters grids to assure the model is not overfitting\n",
    "param_grid={'n_estimators':[5, 30, 50], 'max_depth':[1, 5, 12], 'learning_rate': [0.03, 0.02, 0.05]}\n",
    "\n",
    "# Fill NaN with the most frequent value in y_train\n",
    "y_train = y_train.fillna(y_train.mode()[0])\n",
    "\n",
    "grid_search=GridSearchCV(estimator=xgb.XGBClassifier(eval_metric='logloss', enable_categorical=True),\n",
    "                         param_grid=param_grid,\n",
    "                         scoring= make_scorer(f1_score),\n",
    "                         cv=5,\n",
    "                         n_jobs=-1,\n",
    "                         verbose=3,\n",
    "                         error_score=\"raise\"\n",
    "                         )\n",
    "\n",
    "#Start grid search\n",
    "grid_search.fit(X_train, y_train)\n",
    "#get most optimal model params\n",
    "best_params=grid_search.best_estimator_\n",
    "\n",
    "print(f\"Optimal parameters for model: {grid_search.best_params_} \")\n",
    "print(f\"Best CV F1 score on training set: {grid_search.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "brXQSosHYJxv"
   },
   "source": [
    "**Validate Optimal Parameters**\n",
    "\n",
    "Next we will assess how our validation set performs using our optimal parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "XZGK2MORYVgp",
    "outputId": "4a4d95a4-05db-4d86-ee6d-676be8df538a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score for Validation set leveraging best model found: 0.9999\n"
     ]
    }
   ],
   "source": [
    "y_val_predictions=best_params.predict(X_val)\n",
    "val_score=f1_score(y_val, y_val_predictions)\n",
    "\n",
    "print(f\"f1 score for Validation set leveraging best model found: {val_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MfqqwGknYlwL"
   },
   "source": [
    "As you can see the model performed very well so if we are comfortable with it we can now test the model and parameters on our real world data in Splunk.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dm314vio5crP"
   },
   "source": [
    "**Train XGBoost Model**\n",
    "\n",
    "In order to convert the model to XGboost we need it to be in a specific format.\n",
    "\n",
    "We will leverage the best parameters we found earlier and we will set the number of rounds to an arbitrary 100. We need to convert our dataframe to a Dmatrix. We can then train XGboost and then convert the model to ONNX format and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "_s5NvTMgYwG7",
    "outputId": "f03d3863-f888-471e-bc46-2ed70dad7cc1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dltk/.local/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [17:13:34] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"n_estimators\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on val set: 99.97%\n"
     ]
    }
   ],
   "source": [
    "# Rename columns in X_train and X_val (need f%d format for convert xgboost)\n",
    "X_train.columns = [f'f{i}' for i in range(X_train.shape[1])]\n",
    "X_val.columns = [f'f{i}' for i in range(X_val.shape[1])]\n",
    "#set paramters found earlier and number of rounds\n",
    "param=grid_search.best_params_\n",
    "num_rounds=100\n",
    "\n",
    "#convert df to Dmatrix\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_val, label=y_val)\n",
    "\n",
    "#train XGboost\n",
    "bst=xgb.train(param, dtrain, num_rounds)\n",
    "\n",
    "#Make predictions\n",
    "preds=bst.predict(dtest)\n",
    "predictions=[round(value) for value in preds]\n",
    "\n",
    "#Calculate Accuracy\n",
    "accuracy=accuracy_score(y_val, predictions)\n",
    "print(\"Accuracy on val set: %.2f%%\" % (accuracy*100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y0Ow3iaxtTYy"
   },
   "source": [
    "**Convert XGBoost Model to ONNX**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Mht7pe80KhmB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END learning_rate=0.03, max_depth=1, n_estimators=5;, score=0.737 total time=   0.5s\n",
      "[CV 5/5] END learning_rate=0.03, max_depth=1, n_estimators=30;, score=0.997 total time=   0.6s\n",
      "[CV 3/5] END learning_rate=0.03, max_depth=5, n_estimators=5;, score=0.737 total time=   0.5s\n",
      "[CV 1/5] END learning_rate=0.03, max_depth=5, n_estimators=50;, score=1.000 total time=   0.8s\n",
      "[CV 4/5] END learning_rate=0.03, max_depth=12, n_estimators=5;, score=0.737 total time=   0.5s\n",
      "[CV 2/5] END learning_rate=0.03, max_depth=12, n_estimators=50;, score=1.000 total time=   1.3s\n",
      "[CV 3/5] END learning_rate=0.02, max_depth=1, n_estimators=30;, score=0.996 total time=   0.9s\n",
      "[CV 1/5] END learning_rate=0.02, max_depth=5, n_estimators=5;, score=0.737 total time=   0.6s\n",
      "[CV 2/5] END learning_rate=0.02, max_depth=5, n_estimators=30;, score=1.000 total time=   0.9s\n",
      "[CV 5/5] END learning_rate=0.02, max_depth=5, n_estimators=50;, score=1.000 total time=   1.0s\n",
      "[CV 3/5] END learning_rate=0.02, max_depth=12, n_estimators=30;, score=1.000 total time=   0.7s\n",
      "[CV 1/5] END learning_rate=0.05, max_depth=1, n_estimators=5;, score=0.997 total time=   0.5s\n",
      "[CV 2/5] END learning_rate=0.05, max_depth=1, n_estimators=30;, score=0.995 total time=   0.6s\n",
      "[CV 5/5] END learning_rate=0.05, max_depth=1, n_estimators=50;, score=0.997 total time=   0.7s\n",
      "[CV 3/5] END learning_rate=0.05, max_depth=5, n_estimators=30;, score=1.000 total time=   0.7s\n",
      "[CV 2/5] END learning_rate=0.05, max_depth=12, n_estimators=5;, score=1.000 total time=   0.5s\n",
      "[CV 3/5] END learning_rate=0.05, max_depth=12, n_estimators=30;, score=1.000 total time=   0.7s\n",
      "[CV 2/5] END learning_rate=0.03, max_depth=1, n_estimators=30;, score=0.995 total time=   0.7s\n",
      "[CV 1/5] END learning_rate=0.03, max_depth=5, n_estimators=5;, score=0.737 total time=   0.5s\n",
      "[CV 5/5] END learning_rate=0.03, max_depth=5, n_estimators=5;, score=0.737 total time=   0.5s\n",
      "[CV 2/5] END learning_rate=0.03, max_depth=5, n_estimators=50;, score=1.000 total time=   0.8s\n",
      "[CV 5/5] END learning_rate=0.03, max_depth=12, n_estimators=5;, score=0.737 total time=   0.5s\n",
      "[CV 3/5] END learning_rate=0.03, max_depth=12, n_estimators=50;, score=1.000 total time=   1.1s\n",
      "[CV 1/5] END learning_rate=0.02, max_depth=1, n_estimators=30;, score=0.997 total time=   0.7s\n",
      "[CV 3/5] END learning_rate=0.02, max_depth=1, n_estimators=50;, score=0.996 total time=   0.7s\n",
      "[CV 1/5] END learning_rate=0.02, max_depth=5, n_estimators=30;, score=1.000 total time=   0.8s\n",
      "[CV 4/5] END learning_rate=0.02, max_depth=5, n_estimators=50;, score=1.000 total time=   1.2s\n",
      "[CV 2/5] END learning_rate=0.02, max_depth=12, n_estimators=30;, score=1.000 total time=   0.8s\n",
      "[CV 5/5] END learning_rate=0.02, max_depth=12, n_estimators=50;, score=1.000 total time=   0.9s\n",
      "[CV 4/5] END learning_rate=0.05, max_depth=1, n_estimators=30;, score=0.998 total time=   0.6s\n",
      "[CV 2/5] END learning_rate=0.05, max_depth=5, n_estimators=5;, score=1.000 total time=   0.5s\n",
      "[CV 5/5] END learning_rate=0.05, max_depth=5, n_estimators=30;, score=1.000 total time=   0.7s\n",
      "[CV 3/5] END learning_rate=0.05, max_depth=12, n_estimators=5;, score=1.000 total time=   0.5s\n",
      "[CV 4/5] END learning_rate=0.05, max_depth=12, n_estimators=30;, score=1.000 total time=   0.7s\n",
      "[CV 1/5] END learning_rate=0.03, max_depth=1, n_estimators=30;, score=0.997 total time=   0.7s\n",
      "[CV 4/5] END learning_rate=0.03, max_depth=1, n_estimators=50;, score=0.998 total time=   0.7s\n",
      "[CV 3/5] END learning_rate=0.03, max_depth=5, n_estimators=30;, score=1.000 total time=   0.7s\n",
      "[CV 1/5] END learning_rate=0.03, max_depth=12, n_estimators=5;, score=0.737 total time=   0.5s\n",
      "[CV 1/5] END learning_rate=0.03, max_depth=12, n_estimators=30;, score=1.000 total time=   0.8s\n",
      "[CV 1/5] END learning_rate=0.02, max_depth=1, n_estimators=5;, score=0.737 total time=   0.5s\n",
      "[CV 4/5] END learning_rate=0.02, max_depth=1, n_estimators=5;, score=0.737 total time=   0.5s\n",
      "[CV 5/5] END learning_rate=0.02, max_depth=1, n_estimators=30;, score=0.997 total time=   0.9s\n",
      "[CV 3/5] END learning_rate=0.02, max_depth=5, n_estimators=5;, score=0.737 total time=   0.9s\n",
      "[CV 3/5] END learning_rate=0.02, max_depth=5, n_estimators=50;, score=1.000 total time=   1.3s\n",
      "[CV 1/5] END learning_rate=0.02, max_depth=12, n_estimators=30;, score=1.000 total time=   0.8s\n",
      "[CV 4/5] END learning_rate=0.02, max_depth=12, n_estimators=50;, score=1.000 total time=   1.1s\n",
      "[CV 3/5] END learning_rate=0.05, max_depth=1, n_estimators=50;, score=0.996 total time=   0.7s\n",
      "[CV 1/5] END learning_rate=0.05, max_depth=5, n_estimators=30;, score=1.000 total time=   0.7s\n",
      "[CV 4/5] END learning_rate=0.05, max_depth=5, n_estimators=50;, score=1.000 total time=   0.8s\n",
      "[CV 5/5] END learning_rate=0.05, max_depth=12, n_estimators=30;, score=1.000 total time=   0.8s\n",
      "[CV 3/5] END learning_rate=0.03, max_depth=1, n_estimators=30;, score=0.996 total time=   0.6s\n",
      "[CV 5/5] END learning_rate=0.03, max_depth=1, n_estimators=50;, score=0.997 total time=   0.7s\n",
      "[CV 4/5] END learning_rate=0.03, max_depth=5, n_estimators=30;, score=1.000 total time=   0.7s\n",
      "[CV 2/5] END learning_rate=0.03, max_depth=12, n_estimators=5;, score=0.737 total time=   0.5s\n",
      "[CV 3/5] END learning_rate=0.03, max_depth=12, n_estimators=30;, score=1.000 total time=   0.8s\n",
      "[CV 5/5] END learning_rate=0.03, max_depth=12, n_estimators=50;, score=1.000 total time=   1.1s\n",
      "[CV 2/5] END learning_rate=0.02, max_depth=1, n_estimators=50;, score=0.995 total time=   0.9s\n",
      "[CV 4/5] END learning_rate=0.02, max_depth=5, n_estimators=5;, score=0.737 total time=   0.7s\n",
      "[CV 1/5] END learning_rate=0.02, max_depth=5, n_estimators=50;, score=1.000 total time=   1.1s\n",
      "[CV 5/5] END learning_rate=0.02, max_depth=12, n_estimators=5;, score=0.737 total time=   0.6s\n",
      "[CV 3/5] END learning_rate=0.02, max_depth=12, n_estimators=50;, score=1.000 total time=   0.8s\n",
      "[CV 3/5] END learning_rate=0.05, max_depth=1, n_estimators=5;, score=0.996 total time=   0.5s\n",
      "[CV 5/5] END learning_rate=0.05, max_depth=1, n_estimators=30;, score=0.997 total time=   0.6s\n",
      "[CV 3/5] END learning_rate=0.05, max_depth=5, n_estimators=5;, score=1.000 total time=   0.5s\n",
      "[CV 1/5] END learning_rate=0.05, max_depth=5, n_estimators=50;, score=1.000 total time=   0.8s\n",
      "[CV 4/5] END learning_rate=0.05, max_depth=12, n_estimators=5;, score=1.000 total time=   0.5s\n",
      "[CV 1/5] END learning_rate=0.05, max_depth=12, n_estimators=50;, score=1.000 total time=   0.8s\n",
      "[CV 5/5] END learning_rate=0.03, max_depth=1, n_estimators=5;, score=0.737 total time=   0.5s\n",
      "[CV 3/5] END learning_rate=0.03, max_depth=1, n_estimators=50;, score=0.996 total time=   0.7s\n",
      "[CV 2/5] END learning_rate=0.03, max_depth=5, n_estimators=30;, score=1.000 total time=   0.7s\n",
      "[CV 5/5] END learning_rate=0.03, max_depth=5, n_estimators=50;, score=1.000 total time=   0.8s\n",
      "[CV 5/5] END learning_rate=0.03, max_depth=12, n_estimators=30;, score=1.000 total time=   0.8s\n",
      "[CV 3/5] END learning_rate=0.02, max_depth=1, n_estimators=5;, score=0.737 total time=   0.9s\n",
      "[CV 1/5] END learning_rate=0.02, max_depth=1, n_estimators=50;, score=0.997 total time=   1.0s\n",
      "[CV 5/5] END learning_rate=0.02, max_depth=5, n_estimators=5;, score=0.737 total time=   0.7s\n",
      "[CV 2/5] END learning_rate=0.02, max_depth=5, n_estimators=50;, score=1.000 total time=   1.0s\n",
      "[CV 3/5] END learning_rate=0.02, max_depth=12, n_estimators=5;, score=0.737 total time=   0.5s\n",
      "[CV 5/5] END learning_rate=0.02, max_depth=12, n_estimators=30;, score=1.000 total time=   1.0s\n",
      "[CV 4/5] END learning_rate=0.05, max_depth=1, n_estimators=5;, score=0.998 total time=   0.5s\n",
      "[CV 1/5] END learning_rate=0.05, max_depth=1, n_estimators=50;, score=0.997 total time=   0.7s\n",
      "[CV 4/5] END learning_rate=0.05, max_depth=5, n_estimators=5;, score=1.000 total time=   0.5s\n",
      "[CV 2/5] END learning_rate=0.05, max_depth=5, n_estimators=50;, score=1.000 total time=   0.8s\n",
      "[CV 5/5] END learning_rate=0.05, max_depth=12, n_estimators=5;, score=1.000 total time=   0.5s\n",
      "[CV 3/5] END learning_rate=0.05, max_depth=12, n_estimators=50;, score=1.000 total time=   0.8s\n",
      "[CV 3/5] END learning_rate=0.03, max_depth=1, n_estimators=5;, score=0.737 total time=   0.5s\n",
      "[CV 1/5] END learning_rate=0.03, max_depth=1, n_estimators=50;, score=0.997 total time=   0.7s\n",
      "[CV 4/5] END learning_rate=0.03, max_depth=5, n_estimators=5;, score=0.737 total time=   0.6s\n",
      "[CV 3/5] END learning_rate=0.03, max_depth=5, n_estimators=50;, score=1.000 total time=   0.8s\n",
      "[CV 2/5] END learning_rate=0.03, max_depth=12, n_estimators=30;, score=1.000 total time=   0.8s\n",
      "[CV 4/5] END learning_rate=0.03, max_depth=12, n_estimators=50;, score=1.000 total time=   1.0s\n",
      "[CV 4/5] END learning_rate=0.02, max_depth=1, n_estimators=30;, score=0.998 total time=   0.9s\n",
      "[CV 2/5] END learning_rate=0.02, max_depth=5, n_estimators=5;, score=0.737 total time=   0.7s\n",
      "[CV 5/5] END learning_rate=0.02, max_depth=5, n_estimators=30;, score=1.000 total time=   1.2s\n",
      "[CV 4/5] END learning_rate=0.02, max_depth=12, n_estimators=5;, score=0.737 total time=   0.5s\n",
      "[CV 1/5] END learning_rate=0.02, max_depth=12, n_estimators=50;, score=1.000 total time=   1.0s\n",
      "[CV 1/5] END learning_rate=0.05, max_depth=1, n_estimators=30;, score=0.997 total time=   0.6s\n",
      "[CV 4/5] END learning_rate=0.05, max_depth=1, n_estimators=50;, score=0.998 total time=   0.8s\n",
      "[CV 2/5] END learning_rate=0.05, max_depth=5, n_estimators=30;, score=1.000 total time=   0.7s\n",
      "[CV 5/5] END learning_rate=0.05, max_depth=5, n_estimators=50;, score=1.000 total time=   0.8s\n",
      "[CV 2/5] END learning_rate=0.05, max_depth=12, n_estimators=50;, score=1.000 total time=   0.8s\n",
      "[CV 2/5] END learning_rate=0.03, max_depth=1, n_estimators=5;, score=0.737 total time=   0.5s\n",
      "[CV 4/5] END learning_rate=0.03, max_depth=1, n_estimators=30;, score=0.998 total time=   0.6s\n",
      "[CV 2/5] END learning_rate=0.03, max_depth=5, n_estimators=5;, score=0.737 total time=   0.5s\n",
      "[CV 5/5] END learning_rate=0.03, max_depth=5, n_estimators=30;, score=1.000 total time=   0.7s\n",
      "[CV 3/5] END learning_rate=0.03, max_depth=12, n_estimators=5;, score=0.737 total time=   0.6s\n",
      "[CV 1/5] END learning_rate=0.03, max_depth=12, n_estimators=50;, score=1.000 total time=   1.3s\n",
      "[CV 2/5] END learning_rate=0.02, max_depth=1, n_estimators=30;, score=0.995 total time=   0.8s\n",
      "[CV 5/5] END learning_rate=0.02, max_depth=1, n_estimators=50;, score=0.997 total time=   0.9s\n",
      "[CV 3/5] END learning_rate=0.02, max_depth=5, n_estimators=30;, score=1.000 total time=   1.2s\n",
      "[CV 2/5] END learning_rate=0.02, max_depth=12, n_estimators=5;, score=0.737 total time=   0.6s\n",
      "[CV 2/5] END learning_rate=0.02, max_depth=12, n_estimators=50;, score=1.000 total time=   0.9s\n",
      "[CV 5/5] END learning_rate=0.05, max_depth=1, n_estimators=5;, score=0.997 total time=   0.7s\n",
      "[CV 2/5] END learning_rate=0.05, max_depth=1, n_estimators=50;, score=0.995 total time=   0.7s\n",
      "[CV 5/5] END learning_rate=0.05, max_depth=5, n_estimators=5;, score=1.000 total time=   0.5s\n",
      "[CV 3/5] END learning_rate=0.05, max_depth=5, n_estimators=50;, score=1.000 total time=   0.8s\n",
      "[CV 2/5] END learning_rate=0.05, max_depth=12, n_estimators=30;, score=1.000 total time=   0.7s\n",
      "[CV 5/5] END learning_rate=0.05, max_depth=12, n_estimators=50;, score=1.000 total time=   0.6s\n",
      "[CV 4/5] END learning_rate=0.03, max_depth=1, n_estimators=5;, score=0.737 total time=   0.6s\n",
      "[CV 2/5] END learning_rate=0.03, max_depth=1, n_estimators=50;, score=0.995 total time=   0.7s\n",
      "[CV 1/5] END learning_rate=0.03, max_depth=5, n_estimators=30;, score=1.000 total time=   0.7s\n",
      "[CV 4/5] END learning_rate=0.03, max_depth=5, n_estimators=50;, score=1.000 total time=   0.8s\n",
      "[CV 4/5] END learning_rate=0.03, max_depth=12, n_estimators=30;, score=1.000 total time=   0.8s\n",
      "[CV 2/5] END learning_rate=0.02, max_depth=1, n_estimators=5;, score=0.737 total time=   0.6s\n",
      "[CV 5/5] END learning_rate=0.02, max_depth=1, n_estimators=5;, score=0.737 total time=   0.7s\n",
      "[CV 4/5] END learning_rate=0.02, max_depth=1, n_estimators=50;, score=0.998 total time=   1.2s\n",
      "[CV 4/5] END learning_rate=0.02, max_depth=5, n_estimators=30;, score=1.000 total time=   1.2s\n",
      "[CV 1/5] END learning_rate=0.02, max_depth=12, n_estimators=5;, score=0.737 total time=   0.6s\n",
      "[CV 4/5] END learning_rate=0.02, max_depth=12, n_estimators=30;, score=1.000 total time=   0.8s\n",
      "[CV 2/5] END learning_rate=0.05, max_depth=1, n_estimators=5;, score=0.995 total time=   0.5s\n",
      "[CV 3/5] END learning_rate=0.05, max_depth=1, n_estimators=30;, score=0.996 total time=   0.7s\n",
      "[CV 1/5] END learning_rate=0.05, max_depth=5, n_estimators=5;, score=1.000 total time=   0.6s\n",
      "[CV 4/5] END learning_rate=0.05, max_depth=5, n_estimators=30;, score=1.000 total time=   0.7s\n",
      "[CV 1/5] END learning_rate=0.05, max_depth=12, n_estimators=5;, score=1.000 total time=   0.5s\n",
      "[CV 1/5] END learning_rate=0.05, max_depth=12, n_estimators=30;, score=1.000 total time=   0.7s\n",
      "[CV 4/5] END learning_rate=0.05, max_depth=12, n_estimators=50;, score=1.000 total time=   0.6s\n"
     ]
    }
   ],
   "source": [
    "#Covert to XGBoost model to ONNX\n",
    "initial_types = [('float_input', FloatTensorType([None, X_train.shape[1]]))]\n",
    "onx = convert_xgboost(bst, initial_types=initial_types, target_opset=12)\n",
    "\n",
    "# Save the ONNX model\n",
    "#import onnx #Adding this line imports the onnx module\n",
    "onnx.save(onx, 'data/xgboost_model.onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ciTuI133Loth"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b954657b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import sagemaker\n",
    "import boto3\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "import gzip\n",
    "#import tldextract\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import random\n",
    "import io\n",
    "import pandas\n",
    "from io import StringIO\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Input \n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from tensorflow.keras.layers import Embedding,LSTM,Dropout,Dense,Activation\n",
    "from tensorflow.keras.activations import sigmoid\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "import datetime\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.preprocessing import text\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b4fa425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\"\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "\n",
    "gpus = tf.config.list_logical_devices('GPU')\n",
    "strategy = tf.distribute.MirroredStrategy(gpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c8e000e",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_vocab = 40\n",
    "max_sequence_length=40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30d6c472",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_input_data(path):\n",
    "    df = pandas.read_csv(path)\n",
    "    return df\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9f83fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(df, train_size=0.98):\n",
    "    X_train, X_rem = train_test_split(df, train_size=train_size)\n",
    "    X_valid, X_test = train_test_split(X_rem, test_size=0.5)\n",
    "    return X_train,X_valid,X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc410cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_in_alexa1m(domain,in_alexa_domains):\n",
    "    return ((domain in in_alexa_domains))\n",
    "\n",
    "def entropy(s):\n",
    "        p, lns = Counter(s), float(len(s))\n",
    "        return -sum( count/lns * math.log(count/lns, 2) for count in p.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f50e37bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(df,path_to_store_processed_file):\n",
    "    vectorizer_non_dga_domains = CountVectorizer(analyzer='char', ngram_range=(1,4), min_df=1e-4, max_df=1.0)\n",
    "    vectorizer_words = CountVectorizer(analyzer='char', ngram_range=(1,4), min_df=1e-4, max_df=1.0)\n",
    "    word_df = pd.read_csv('./data/words.txt', names=['word'], header=None, dtype={'word': np.str}, encoding='utf-8')\n",
    "    word_df = word_df[word_df['word'].map(lambda x: str(x).isalpha())]\n",
    "    word_df = word_df.applymap(lambda x: str(x).strip().lower())\n",
    "    word_df = word_df.dropna()\n",
    "    word_df = word_df.drop_duplicates()\n",
    "    counts_matrix = vectorizer_words.fit_transform(word_df['word'])\n",
    "    weight_words = np.log10(counts_matrix.sum(axis=0).getA1())\n",
    "\n",
    "    alexa1m = pandas.read_csv(\"./data/top-1m.csv\",header=None,usecols=[1], names=['domain'])\n",
    "\n",
    "    alexa_domains = alexa1m['domain']\n",
    "    in_alexa_domains = set(alexa_domains) & set(df['domain'])\n",
    "\n",
    "\n",
    "    non_dga_domains = df[df['is_dga']==0]\n",
    "    t = vectorizer_non_dga_domains.fit_transform(non_dga_domains['domain'])\n",
    "    weight_non_dga_grams = np.log10(t.sum(axis=0).getA1())\n",
    "    \n",
    "    x1= weight_words * vectorizer_words.transform(df['domain']).T \n",
    "    x2= weight_non_dga_grams * vectorizer_non_dga_domains.transform(df['domain']).T \n",
    "\n",
    "\n",
    "\n",
    "    X_train_2_word_grams =x1\n",
    "    X_train_2_non_dga_grams = x2\n",
    "\n",
    "    print (\"1. Done adding ngram features\")\n",
    "    X_train_2_entropy =   df['domain'].map(lambda x: entropy(x)) \n",
    "    print (\"2. Done adding entropy\")\n",
    "    X_train_2_len = df['domain'].map(lambda x: len(x))\n",
    "    print (\"3. Done adding length of domain\")\n",
    "    print(\"Number of training set domains present in alexa domains \",len(in_alexa_domains))\n",
    "    X_train_2_alexa = df['domain'].map(lambda x: is_in_alexa1m(x,in_alexa_domains))\n",
    "    print (\"4. Done adding domain present in alexa domains\")\n",
    "    X_train_2_alexa = X_train_2_alexa.astype(int)\n",
    "    X_train_2 = np.c_[df['domain'],df['is_dga'],X_train_2_word_grams,X_train_2_non_dga_grams,X_train_2_entropy,X_train_2_len,X_train_2_alexa] #\n",
    "    print (\"5. Done appending features\")\n",
    "\n",
    "    print (X_train_2.shape)\n",
    "    processed_df = pd.DataFrame(X_train_2,columns = \n",
    "                 ['domain','is_dga','word_grams','non_dga_grams','entropy','len','in_alexa'])\n",
    "    processed_df.to_csv(path_to_store_processed_file)\n",
    "    print (\"6. Done creating csv file\")\n",
    "    return processed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6c3097d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_text(tokenizer,texts):\n",
    "    text_sequences = tokenizer.texts_to_sequences(texts)\n",
    "    return sequence.pad_sequences(text_sequences, maxlen=max_sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d968179",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_training(df):\n",
    "    X_train, X_valid, X_test = split_dataset(df, train_size=0.98)\n",
    "    train_df = create_features(X_train,\"./data/processed_dga_train_dataset.csv\")\n",
    "    test_df = create_features(X_test,\"./data/processed_dga_test_dataset.csv\")\n",
    "    X_train_input1 = train_df[['domain','is_dga']]\n",
    "    X_train_input2 = train_df[['word_grams','non_dga_grams','entropy','len','in_alexa']]\n",
    "    tokenizer = text.Tokenizer(num_words=max_vocab,char_level=True)\n",
    "    tokenizer.fit_on_texts(X_train_input1[\"domain\"])\n",
    "    X_train_domains = prep_text(tokenizer,X_train_input1['domain'])\n",
    "    X_test_input1 = test_df[['domain','is_dga']]\n",
    "    X_test_input2 = test_df[['word_grams','non_dga_grams','entropy','len','in_alexa']]\n",
    "    X_test_domains = prep_text(tokenizer,X_test_input1['domain'])\n",
    "    return X_train_domains,X_train_input1,X_train_input2,X_test_domains,X_test_input1,X_test_input2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d303e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decay(epoch):\n",
    "      if epoch < 3:\n",
    "        return 1e-3\n",
    "      elif epoch >= 3 and epoch < 7:\n",
    "        return 1e-4\n",
    "      else:\n",
    "        return 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "280a6575",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(df):\n",
    "    checkpoint_dir = './wide_new/training_checkpoints'\n",
    "    checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "    X_train_domains,X_train_input1,X_train_input2,X_test_domains,X_test_input1,X_test_input2 = prepare_for_training(df)\n",
    "\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    returns_wide={}\n",
    "    with strategy.scope():\n",
    "        input1 = tf.keras.Input(shape = [40], name=\"deep_input\")\n",
    "        input2 = tf.keras.Input(shape= [5],name=\"wide_input\")\n",
    "        embedding = Embedding(input_dim=40,output_dim=256, input_length=40)(input1)\n",
    "        L = LSTM(256, dropout=0.5)(embedding)\n",
    "        concat = tf.keras.layers.concatenate([input2,L])\n",
    "        output = Dense(1, name=\"output\",activation = \"sigmoid\")(concat)\n",
    "        model = tf.keras.Model(inputs=[input1,input2],outputs=[output])\n",
    "        model.compile(loss='binary_crossentropy',optimizer='rmsprop',metrics=['accuracy'])\n",
    "\n",
    "        print (model.summary())\n",
    "\n",
    "        class PrintLR(tf.keras.callbacks.Callback):\n",
    "              def on_epoch_end(self, epoch, logs=None):\n",
    "                print('\\nLearning rate for epoch {} is {}'.format(epoch + 1, model.optimizer.lr.numpy()))\n",
    "\n",
    "        callbacks = [\n",
    "        tf.keras.callbacks.TensorBoard(log_dir='./wide_new/logs', histogram_freq=1),\n",
    "        tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix,\n",
    "                                            save_weights_only=True),\n",
    "        tf.keras.callbacks.EarlyStopping(monitor='loss',min_delta=0.0001,restore_best_weights=True),\n",
    "        tf.keras.callbacks.LearningRateScheduler(decay),\n",
    "        PrintLR()\n",
    "        ]\n",
    "        callbacks_ = callbacks\n",
    "        returns_wide = {}\n",
    "\n",
    "        model_epochs = 20\n",
    "        model_batch_size = 1000\n",
    "\n",
    "        print(type(X_train_domains))\n",
    "        print(type(X_train_input2))\n",
    "        print(type(X_test_domains))\n",
    "        print(type(X_test_input2))\n",
    "    returns_wide['fit_history'] = model.fit(x=[X_train_domains,tf.convert_to_tensor(X_train_input2.to_numpy(), dtype=tf.float32)],\n",
    "                                       y=tf.convert_to_tensor(X_train_input1['is_dga'].to_numpy(), dtype=tf.float32), \n",
    "                                       verbose=2, \n",
    "                                       epochs=model_epochs, \n",
    "                                       batch_size=model_batch_size,\n",
    "                                       #validation_data = ([X_test_domains,X_test_input2.to_numpy().astype(np.float32)], test_df['is_dga'].to_numpy().astype(np.float32)),\n",
    "                                       callbacks=callbacks_)\n",
    "\n",
    "    returns_wide['model_epochs'] = model_epochs\n",
    "    returns_wide['model'] = model\n",
    "    returns_wide['model_batch_size'] = model_batch_size\n",
    "    returns_wide['model_loss_acc'] = model.evaluate([X_test_domains,X_test_input2.to_numpy().astype(np.float32)], test_df['is_dga'].to_numpy().astype(np.float32))\n",
    "    lstm_wide_time = (time.time() - start_time)\n",
    "    print(\"--- Training time in %s seconds ---\" % lstm_wide_time)\n",
    "    return returns_wide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14854f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34547/1513208921.py:4: DeprecationWarning: `np.str` is a deprecated alias for the builtin `str`. To silence this warning, use `str` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.str_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  word_df = pd.read_csv('./data/words.txt', names=['word'], header=None, dtype={'word': np.str}, encoding='utf-8')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Done adding ngram features\n",
      "2. Done adding entropy\n",
      "3. Done adding length of domain\n",
      "Number of training set domains present in alexa domains  2\n",
      "4. Done adding domain present in alexa domains\n",
      "5. Done appending features\n",
      "(980, 7)\n",
      "6. Done creating csv file\n",
      "1. Done adding ngram features\n",
      "2. Done adding entropy\n",
      "3. Done adding length of domain\n",
      "Number of training set domains present in alexa domains  0\n",
      "4. Done adding domain present in alexa domains\n",
      "5. Done appending features\n",
      "(10, 7)\n",
      "6. Done creating csv file\n",
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " deep_input (InputLayer)        [(None, 40)]         0           []                               \n",
      "                                                                                                  \n",
      " embedding_3 (Embedding)        (None, 40, 256)      10240       ['deep_input[0][0]']             \n",
      "                                                                                                  \n",
      " wide_input (InputLayer)        [(None, 5)]          0           []                               \n",
      "                                                                                                  \n",
      " lstm_3 (LSTM)                  (None, 256)          525312      ['embedding_3[0][0]']            \n",
      "                                                                                                  \n",
      " concatenate_3 (Concatenate)    (None, 261)          0           ['wide_input[0][0]',             \n",
      "                                                                  'lstm_3[0][0]']                 \n",
      "                                                                                                  \n",
      " output (Dense)                 (None, 1)            262         ['concatenate_3[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 535,814\n",
      "Trainable params: 535,814\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Epoch 1/20\n",
      "INFO:tensorflow:batch_all_reduce: 5 all-reduces with algorithm = nccl, num_packs = 1\n",
      "WARNING:tensorflow:Efficient allreduce is not supported for 1 IndexedSlices\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3').\n",
      "INFO:tensorflow:batch_all_reduce: 5 all-reduces with algorithm = nccl, num_packs = 1\n",
      "WARNING:tensorflow:Efficient allreduce is not supported for 1 IndexedSlices\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3').\n"
     ]
    }
   ],
   "source": [
    "def train_dga():\n",
    "    df = read_input_data(\"./data/dga_valid_data.csv\")[:1000]\n",
    "    returns_wide = train(df)\n",
    "    print (returns_wide['model'])\n",
    "    print (returns_wide['model_loss_acc'])\n",
    "train_dga()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677cf930",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9925e08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902f6c95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2faa12c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4853fdc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b4f02e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ed7485",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf879bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11461a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809499c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (type(X_train_domains))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p38",
   "language": "python",
   "name": "conda_tensorflow2_p38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agentic Workflows "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use this notebook to **design and test** Agentic Workflows using llama-index workflows. \n",
    "- Agentic workflow classes created in this notebook are exported automatically and can be utilized in **agentic_workflow_execution** algorithm via the Fit command."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Please follow the format of each cell. Classes or code pieces from non-exported cells cannot be imported during execution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries\n",
    "In this cell, essential libraaries are imported. Add imports when necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "name": "mltkc_import"
   },
   "outputs": [],
   "source": [
    "# Basic imports\n",
    "import sys\n",
    "import os\n",
    "import json \n",
    "import splunklib.client\n",
    "from typing import Sequence, List, Any\n",
    "from pydantic import Field\n",
    "\n",
    "# llama-index core imports\n",
    "import llama_index\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Document, StorageContext, ServiceContext\n",
    "from llama_index.core import ChatPromptTemplate\n",
    "from llama_index.core.llms import ChatMessage, MessageRole\n",
    "from llama_index.core.llms.function_calling import FunctionCallingLLM\n",
    "from llama_index.core.memory import ChatMemoryBuffer\n",
    "from llama_index.core.tools import BaseTool, FunctionTool, ToolSelection, ToolOutput\n",
    "from llama_index.core.agent import AgentRunner, ReActAgentWorker, FunctionCallingAgentWorker\n",
    "from llama_index.core.agent.workflow import FunctionAgent\n",
    "from llama_index.vector_stores.milvus import MilvusVectorStore\n",
    "from app.model.llm_utils import create_llm, create_embedding_model\n",
    "\n",
    "# llama-index tool imports\n",
    "from llama_index.tools.slack import SlackToolSpec\n",
    "from llama_index.tools.google import GmailToolSpec\n",
    "from llama_index.tools.google import GoogleCalendarToolSpec\n",
    "from llama_index.tools.google import GoogleSearchToolSpec\n",
    "from llama_index.tools.mcp import BasicMCPClient, McpToolSpec\n",
    "from llama_index.tools.jira import JiraToolSpec\n",
    "from llama_index.tools.wikipedia import WikipediaToolSpec\n",
    "\n",
    "# llama-index workflow imports\n",
    "from llama_index.utils.workflow import draw_most_recent_execution\n",
    "from llama_index.core.workflow import (\n",
    "    Event,\n",
    "    StartEvent,\n",
    "    StopEvent,\n",
    "    Workflow,\n",
    "    step,\n",
    "    Context,\n",
    ")\n",
    "\n",
    "print(f\"PYTHON VERSION: {sys.version}\")\n",
    "print(\"Imported agent tools packages: Google, MCP, Jira, Neo4j, Arxiv, Wiki\")\n",
    "\n",
    "import asyncio\n",
    "import concurrent.futures\n",
    "def run_async_in_sync(async_func):\n",
    "    loop = asyncio.get_event_loop()\n",
    "    if loop.is_running():\n",
    "        with concurrent.futures.ThreadPoolExecutor() as pool:\n",
    "            return pool.submit(lambda: asyncio.run(async_func())).result()\n",
    "    else:\n",
    "        return asyncio.run(async_func())\n",
    "# ...\n",
    "# global constants\n",
    "MODEL_DIRECTORY = \"/srv/app/model/data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Workflow\n",
    "Here is a simple example workflowï¼Œwhich is the default workflow for agentic_workflow_execution. Please use the subsequent cells to create custom agentic workflows. You can create multiple workflows within the same cell. Workflows created outside those cells will NOT be exported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "name": "mltkc_stage"
   },
   "outputs": [],
   "source": [
    "# Example chat workflow using Bedrock LLM\n",
    "# Example SPL:\n",
    "## | makeresults\n",
    "## | fit MLTKContainer algo=agentic_workflow_execution workflow_name=SimpleLLMFlow query=\"What does Splunk do?\" * into app:agentic_workflow_execution\n",
    "\n",
    "\n",
    "class SimpleLLMFlow(Workflow):\n",
    "    @step\n",
    "    async def generate(self, ev: StartEvent) -> StopEvent:\n",
    "        llm, _ = create_llm('bedrock')\n",
    "        response = await llm.acomplete(ev.query)\n",
    "        return StopEvent(result=str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing LLM object from bedrock\n",
      "No model specified at the input. Using configured model apac.anthropic.claude-3-5-sonnet-20241022-v2:0.\n",
      "Splunk is a software platform used for searching, monitoring, analyzing, and visualizing machine-generated big data in real-time. It's particularly popular in IT operations, security, and business analytics.\n",
      "\n",
      "Key features and uses of Splunk include:\n",
      "\n",
      "1. Log Management: Collects and indexes log data from any source, including applications, systems, and devices\n",
      "\n",
      "2. Security Information and Event Management (SIEM): Monitors and analyzes security events in real-time\n",
      "\n",
      "3. IT Operations: Helps track system performance, troubleshoot issues, and monitor infrastructure\n",
      "\n",
      "4. Business Analytics: Provides insights into business metrics and customer behavior\n",
      "\n",
      "5. Application Management: Monitors application performance and user experience\n",
      "\n",
      "6. IoT Data Analysis: Processes and analyzes data from Internet of Things devices\n",
      "\n",
      "The platform is known for its powerful search processing language (SPL) and ability to handle large volumes of data. Organizations use Splunk to:\n",
      "\n",
      "- Detect and respond to security threats\n",
      "- Monitor system health\n",
      "- Troubleshoot problems\n",
      "- Generate reports and dashboards\n",
      "- Create real-time alerts\n",
      "- Perform business analytics\n",
      "\n",
      "Splunk is widely used by enterprises, government agencies, and organizations that need to make sense of their machine data.\n"
     ]
    }
   ],
   "source": [
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",
    "w = SimpleLLMFlow(timeout=10, verbose=False)\n",
    "result = await w.run(query=\"What's Splunk?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Workflow 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "name": "mltkc_init"
   },
   "outputs": [],
   "source": [
    "# Example workflow as a Splunk Cloud MCP client\n",
    "# Please fill in the mcp_token and tenant_name variables and change LLM option if needed before execution\n",
    "# Example SPL:\n",
    "## | makeresults\n",
    "## | fit MLTKContainer algo=agentic_workflow_execution workflow_name=SplunkMCPAgent query=\"What savesearches are there?\" * into app:agentic_workflow_execution\n",
    "\n",
    "class InputEvent(Event):\n",
    "    input: list[ChatMessage]\n",
    "\n",
    "class StreamEvent(Event):\n",
    "    delta: str\n",
    "\n",
    "class ToolCallEvent(Event):\n",
    "    tool_calls: list[ToolSelection]\n",
    "\n",
    "class FunctionOutputEvent(Event):\n",
    "    output: ToolOutput\n",
    "\n",
    "\n",
    "async def get_mcp_tools():\n",
    "    mcp_token = \"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\"\n",
    "    tenant_name = \"XXXXXX\"\n",
    "    URL = f\"https://{tenant_name}.api.scs.splunk.com/{tenant_name}/mcp/v1/\"\n",
    "    mcp_client = BasicMCPClient(\n",
    "        URL,\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {mcp_token}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "    )\n",
    "    mcp_tool_spec = McpToolSpec(\n",
    "        client=mcp_client\n",
    "    )\n",
    "    tools = await mcp_tool_spec.to_tool_list_async()\n",
    "    return tools\n",
    "\n",
    "class SplunkMCPAgent(Workflow):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *args: Any,\n",
    "        **kwargs: Any,\n",
    "    ) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        tools = run_async_in_sync(get_mcp_tools)\n",
    "        llm, _ = create_llm('bedrock')\n",
    "        # Initialize tools and LLM\n",
    "        self.tools = tools\n",
    "        self.llm = llm\n",
    "        assert self.llm.metadata.is_function_calling_model\n",
    "\n",
    "\n",
    "    @step\n",
    "    async def prepare_chat_history(\n",
    "        self, ctx: Context, ev: StartEvent\n",
    "    ) -> InputEvent:\n",
    "        # clear sources\n",
    "        await ctx.store.set(\"sources\", [])\n",
    "\n",
    "        # check if memory is setup\n",
    "        memory = await ctx.store.get(\"memory\", default=None)\n",
    "        if not memory:\n",
    "            memory = ChatMemoryBuffer.from_defaults(llm=self.llm)\n",
    "\n",
    "        # get user input\n",
    "        user_input = ev.query\n",
    "        user_msg = ChatMessage(role=\"user\", content=user_input)\n",
    "        memory.put(user_msg)\n",
    "\n",
    "        # get chat history\n",
    "        chat_history = memory.get()\n",
    "\n",
    "        # update context\n",
    "        await ctx.store.set(\"memory\", memory)\n",
    "\n",
    "        return InputEvent(input=chat_history)\n",
    "\n",
    "    @step\n",
    "    async def handle_llm_input(\n",
    "        self, ctx: Context, ev: InputEvent\n",
    "    ) -> ToolCallEvent | StopEvent:\n",
    "        chat_history = ev.input\n",
    "\n",
    "        # stream the response\n",
    "        response_stream = await self.llm.astream_chat_with_tools(\n",
    "            self.tools, chat_history=chat_history\n",
    "        )\n",
    "        async for response in response_stream:\n",
    "            ctx.write_event_to_stream(StreamEvent(delta=response.delta or \"\"))\n",
    "\n",
    "        # save the final response, which should have all content\n",
    "        memory = await ctx.store.get(\"memory\")\n",
    "        memory.put(response.message)\n",
    "        await ctx.store.set(\"memory\", memory)\n",
    "\n",
    "        # get tool calls\n",
    "        tool_calls = self.llm.get_tool_calls_from_response(\n",
    "            response, error_on_no_tool_call=False\n",
    "        )\n",
    "\n",
    "        if not tool_calls:\n",
    "            sources = await ctx.store.get(\"sources\", default=[])\n",
    "            return StopEvent(result=str(response) + \"\\n\\n\\n\\n\" + \"Tool Execution Results:\\n\" + str([*sources]) )\n",
    "            # return StopEvent(\n",
    "            #     result={\"response\": response, \"sources\": [*sources]}\n",
    "            # )\n",
    "        else:\n",
    "            return ToolCallEvent(tool_calls=tool_calls)\n",
    "\n",
    "    @step\n",
    "    async def handle_tool_calls(\n",
    "        self, ctx: Context, ev: ToolCallEvent\n",
    "    ) -> InputEvent:\n",
    "        tool_calls = ev.tool_calls\n",
    "        tools_by_name = {tool.metadata.get_name(): tool for tool in self.tools}\n",
    "\n",
    "        tool_msgs = []\n",
    "        sources = await ctx.store.get(\"sources\", default=[])\n",
    "\n",
    "        # call tools -- safely!\n",
    "        for tool_call in tool_calls:\n",
    "            tool = tools_by_name.get(tool_call.tool_name)\n",
    "            additional_kwargs = {\n",
    "                \"tool_call_id\": tool_call.tool_id,\n",
    "                \"name\": tool.metadata.get_name(),\n",
    "            }\n",
    "            if not tool:\n",
    "                tool_msgs.append(\n",
    "                    ChatMessage(\n",
    "                        role=\"tool\",\n",
    "                        content=f\"Tool {tool_call.tool_name} does not exist\",\n",
    "                        additional_kwargs=additional_kwargs,\n",
    "                    )\n",
    "                )\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                tool_output = tool(**tool_call.tool_kwargs)\n",
    "                sources.append(tool_output)\n",
    "                tool_msgs.append(\n",
    "                    ChatMessage(\n",
    "                        role=\"tool\",\n",
    "                        content=tool_output.content,\n",
    "                        additional_kwargs=additional_kwargs,\n",
    "                    )\n",
    "                )\n",
    "            except Exception as e:\n",
    "                tool_msgs.append(\n",
    "                    ChatMessage(\n",
    "                        role=\"tool\",\n",
    "                        content=f\"Encountered error in tool call: {e}\",\n",
    "                        additional_kwargs=additional_kwargs,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        # update memory\n",
    "        memory = await ctx.store.get(\"memory\")\n",
    "        for msg in tool_msgs:\n",
    "            memory.put(msg)\n",
    "\n",
    "        await ctx.store.set(\"sources\", sources)\n",
    "        await ctx.store.set(\"memory\", memory)\n",
    "\n",
    "        chat_history = memory.get()\n",
    "        return InputEvent(input=chat_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Workflow 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "name": "mltkc_fit"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Workflow 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "name": "mltkc_apply"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Workflow 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "name": "mltkc_save"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Workflow 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "name": "mltkc_load"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Workflow 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "name": "mltkc_summary"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Space\n",
    "All subsequent cells are not exported and can be used for free testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",
    "w = SimpleLLMFlow(timeout=10, verbose=False)\n",
    "result = await w.run(query=\"What's Splunk?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

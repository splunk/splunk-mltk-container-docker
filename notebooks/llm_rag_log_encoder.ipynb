{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding machine data (log) to vector DB using Milvus Push"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example Usage: | search ... | fit MLTKContainer algo=llm_rag_log_encoder collection_name=log_events_example embedder_dimension=384 embedder_name=\"all-MiniLM-L6-v2\" use_local=1 label_field_name=_raw _raw search_name src dest into app:llm_rag_log_encoder"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 0 - import libraries\n",
    "At stage 0 we define all imports necessary to run our subsequent code depending on various libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "name": "mltkc_import",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this definition exposes all python module imports that should be available in all subsequent commands\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import pymilvus\n",
    "from pymilvus import (\n",
    "    connections,\n",
    "    utility,\n",
    "    FieldSchema, CollectionSchema, DataType,\n",
    "    Collection,\n",
    ")\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "# ...\n",
    "# global constants\n",
    "MODEL_DIRECTORY = \"/srv/app/model/data/\"\n",
    "MILVUS_ENDPOINT = \"http://milvus-standalone:19530\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",
    "print(\"numpy version: \" + np.__version__)\n",
    "print(\"pandas version: \" + pd.__version__)\n",
    "print(\"pymilvus version: \" + pymilvus.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "name": "mltkc_stage",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this cell is not executed from MLTK and should only be used for staging data into the notebook environment\n",
    "def stage(name):\n",
    "    with open(\"data/\"+name+\".csv\", 'r') as f:\n",
    "        df = pd.read_csv(f)\n",
    "    with open(\"data/\"+name+\".json\", 'r') as f:\n",
    "        param = json.load(f)\n",
    "    return df, param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",
    "df, param = stage(\"llm_rag_log_encoder\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 2 - create and initialize a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "name": "mltkc_init",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# initialize your model\n",
    "# available inputs: data and parameters\n",
    "# returns the model object which will be used as a reference to call fit, apply and summary subsequently\n",
    "def init(df,param):\n",
    "    model = {}   \n",
    "    pk_type=DataType.VARCHAR        \n",
    "    embedding_type=DataType.FLOAT_VECTOR\n",
    "    # Dimensionality setting of collection\n",
    "    try:\n",
    "        embedder_name = param['options']['params']['embedder_name'].strip('\\\"')\n",
    "    except:\n",
    "        embedder_name = 'all-MiniLM-L6-v2'\n",
    "    # Dimension checking for default embedders\n",
    "    if embedder_name == 'intfloat/multilingual-e5-large':\n",
    "        n_dims = 1024\n",
    "    elif embedder_name == 'all-MiniLM-L6-v2':\n",
    "        n_dims = 384\n",
    "    else:\n",
    "        try:\n",
    "            n_dims=int(param['options']['params']['embedder_dimension'])\n",
    "        except:\n",
    "            n_dims=384\n",
    "    \n",
    "    \n",
    "    # Collection name setting   \n",
    "    try:\n",
    "        collection_name=param['options']['params']['collection_name'].strip('\\\"')\n",
    "    except:\n",
    "        collection_name=\"default_collection\"\n",
    "    # Schema setting\n",
    "    try:\n",
    "        schema_fields=df.columns.tolist()\n",
    "        schema_fields.remove(param['options']['params']['label_field_name'])\n",
    "    except:\n",
    "        schema_fields=[]\n",
    "        \n",
    "    print(\"start connecting to Milvus\")\n",
    "    try:\n",
    "        # this hostname may need changing to a specific local docker network ip address depending on docker configuration\n",
    "        connections.connect(\"default\", host=\"milvus-standalone\", port=\"19530\")\n",
    "        collection_exists = utility.has_collection(collection_name)\n",
    "        \n",
    "        # Basic schema setting\n",
    "        fields = [\n",
    "            FieldSchema(name=\"_key\", is_primary=True, auto_id=True, dtype=DataType.INT64),\n",
    "            FieldSchema(name=\"embeddings\", dtype=embedding_type, dim=n_dims),\n",
    "            FieldSchema(name=\"label\", dtype=DataType.VARCHAR, max_length=15000),\n",
    "        ]\n",
    "        # Additional schema setting\n",
    "        if len(schema_fields) != 0: \n",
    "            for i in range(len(schema_fields)):\n",
    "                fields.append(FieldSchema(name=schema_fields[i], dtype=DataType.VARCHAR, max_length=1000))\n",
    "        # Create schema\n",
    "        \n",
    "        schema = CollectionSchema(fields, f\"dsdl schema for {collection_name}\")\n",
    "        print(fields)\n",
    "        \n",
    "        if collection_exists:\n",
    "            print(f\"The collection {collection_name} already exists\")\n",
    "            collection = Collection(collection_name)\n",
    "            collection.load()\n",
    "        else:\n",
    "            print(f\"The collection {collection_name} does not exist\")\n",
    "            print(f\"creating new collection: {collection_name}\")\n",
    "            collection = Collection(collection_name, schema, consistency_level=\"Strong\")\n",
    "            index = {\n",
    "                \"index_type\": \"IVF_FLAT\",\n",
    "                \"metric_type\": \"L2\",\n",
    "                \"params\": {\"nlist\": 1024},\n",
    "            }\n",
    "            collection.create_index(\"embeddings\", index)\n",
    "    except:\n",
    "        collection = None\n",
    "    \n",
    "    model['collection']=collection\n",
    "    model['collection_name']=collection_name\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start connecting to Milvus\n",
      "[{'name': '_key', 'description': '', 'type': <DataType.INT64: 5>, 'is_primary': True, 'auto_id': True}, {'name': 'embeddings', 'description': '', 'type': <DataType.FLOAT_VECTOR: 101>, 'params': {'dim': 1024}}, {'name': 'label', 'description': '', 'type': <DataType.VARCHAR: 21>, 'params': {'max_length': 5000}}, {'name': 'category', 'description': '', 'type': <DataType.VARCHAR: 21>, 'params': {'max_length': 1000}}]\n",
      "The collection tester_cat does not exist\n",
      "creating new collection: tester_cat\n",
      "{'collection': <Collection>:\n",
      "-------------\n",
      "<name>: tester_cat\n",
      "<description>: dsdl schema for tester_cat\n",
      "<schema>: {'auto_id': True, 'description': 'dsdl schema for tester_cat', 'fields': [{'name': '_key', 'description': '', 'type': <DataType.INT64: 5>, 'is_primary': True, 'auto_id': True}, {'name': 'embeddings', 'description': '', 'type': <DataType.FLOAT_VECTOR: 101>, 'params': {'dim': 1024}}, {'name': 'label', 'description': '', 'type': <DataType.VARCHAR: 21>, 'params': {'max_length': 5000}}, {'name': 'category', 'description': '', 'type': <DataType.VARCHAR: 21>, 'params': {'max_length': 1000}}], 'enable_dynamic_field': False}\n",
      ", 'collection_name': 'tester_cat'}\n"
     ]
    }
   ],
   "source": [
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",
    "model = init(df,param)\n",
    "print(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 3 - fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "name": "mltkc_fit",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train your model\n",
    "# returns a fit info json object and may modify the model object\n",
    "def fit(model,df,param):\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",
    "print(fit(model,df,param))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 4 - apply the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "name": "mltkc_apply",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# apply your model\n",
    "# returns the calculated results\n",
    "def apply(model,df,param):\n",
    "    if model['collection'] is not None:\n",
    "        use_local= int(param['options']['params']['use_local'])\n",
    "        try:\n",
    "            embedder_name = param['options']['params']['embedder_name'].strip('\\\"')\n",
    "        except:\n",
    "            embedder_name = 'all-MiniLM-L6-v2'\n",
    "        if use_local:\n",
    "            embedder_name = f'/srv/app/model/data/{embedder_name}'\n",
    "            print(\"Using local embedding model checkpoints\")  \n",
    "        transformer_embedder = HuggingFaceEmbedding(model_name=embedder_name)\n",
    "\n",
    "        try:\n",
    "            df=df.copy()\n",
    "            label_field_name=param['options']['params']['label_field_name']\n",
    "            label_column = df[label_field_name].astype(str)\n",
    "        \n",
    "            text_column = label_column.tolist()\n",
    "            vector_column = []\n",
    "            for text in text_column:\n",
    "                vector_column.append(transformer_embedder.get_text_embedding(text))\n",
    "            data=[vector_column, label_column.tolist()]\n",
    "        except:\n",
    "            data = None\n",
    "            m = \"Failed. Could not vectorize dataframe. Check your field name.\"\n",
    "            \n",
    "        try:\n",
    "            schema_fields=df.columns.tolist()\n",
    "            schema_fields.remove(label_field_name)\n",
    "        except:\n",
    "            schema_fields=[]\n",
    "        if data is not None:\n",
    "            if len(schema_fields) != 0:\n",
    "                for i in range(len(schema_fields)):  \n",
    "                    data.append(df[schema_fields[i]].astype(str).tolist())\n",
    "            # Cap at 16MB for each insertion, 1/4 of the 64MB limit\n",
    "            data_limit = 16000000\n",
    "            try:\n",
    "                n_dims=int(param['options']['params']['embedder_dimension'])\n",
    "            except:\n",
    "                n_dims=384\n",
    "            print(f\"Size of data is {len(data[0])}\")\n",
    "            num_vectors = int(data_limit / (n_dims * 4))\n",
    "            print(f\"Batch size is {num_vectors}\")\n",
    "            if len(data[0]) > num_vectors:\n",
    "                num_sublists = len(data[0]) // num_vectors\n",
    "                if len(data[0]) % num_vectors != 0:\n",
    "                    num_sublists += 1\n",
    "                print(f\"Number of batches is {num_sublists}\")\n",
    "                # Initialize the sublists\n",
    "                sublists = [[] for _ in range(num_sublists)]\n",
    "                # Iterate over each row in the data\n",
    "                for row in data:\n",
    "                    for i in range(num_sublists-1):\n",
    "                        sublists[i].append(row[i * num_vectors:(i + 1) * num_vectors])\n",
    "                    sublists[num_sublists-1].append(row[(num_sublists-1) * num_vectors:])\n",
    "            else:\n",
    "                sublists = [data]\n",
    "            try:\n",
    "                for sub_data in sublists:\n",
    "                    model['collection'].insert(sub_data, timeout=None)\n",
    "                    print(f\"Inserted data batch with length {len(sub_data[0])}\")\n",
    "                m = \"Success\"\n",
    "            except:\n",
    "                m = \"Failed. Could not insert data to collection.\"\n",
    "    else:\n",
    "        m = \"Failed. Could not create collection. Check collection naming.\"\n",
    "    df['message'] = [m]*df.shape[0]\n",
    "    return df['message']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Success\n",
      "1    Success\n",
      "2    Success\n",
      "3    Success\n",
      "4    Success\n",
      "5    Success\n",
      "6    Success\n",
      "7    Success\n",
      "8    Success\n",
      "9    Success\n",
      "Name: message, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",
    "print(apply(model,df,param))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 5 - save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "name": "mltkc_save",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save model to name in expected convention \"<algo_name>_<model_name>\"\n",
    "def save(model,name):\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 6 - load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "name": "mltkc_load",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load model from name in expected convention \"<algo_name>_<model_name>\"\n",
    "def load(name):\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 7 - provide a summary of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "name": "mltkc_summary"
   },
   "outputs": [],
   "source": [
    "# return a model summary\n",
    "def summary(model=None):\n",
    "    returns = {\"version\": {\"numpy\": np.__version__, \"pandas\": pd.__version__} }\n",
    "    return returns\n",
    "\n",
    "def compute(model,df,param):\n",
    "    model = {}   \n",
    "    pk_type=DataType.VARCHAR        \n",
    "    embedding_type=DataType.FLOAT_VECTOR\n",
    "    # Dimensionality setting of collection\n",
    "    try:\n",
    "        embedder_name = param['params']['embedder_name'].strip('\\\"')\n",
    "    except:\n",
    "        embedder_name = 'all-MiniLM-L6-v2'\n",
    "    # Dimension checking for default embedders\n",
    "    if embedder_name == 'intfloat/multilingual-e5-large':\n",
    "        n_dims = 1024\n",
    "    elif embedder_name == 'all-MiniLM-L6-v2':\n",
    "        n_dims = 384\n",
    "    else:\n",
    "        try:\n",
    "            n_dims=int(param['params']['embedder_dimension'])\n",
    "        except:\n",
    "            n_dims=384\n",
    "    \n",
    "    \n",
    "    # Collection name setting   \n",
    "    try:\n",
    "        collection_name=param['params']['collection_name'].strip('\\\"')\n",
    "    except:\n",
    "        collection_name=\"default_collection\"\n",
    "    # Schema setting\n",
    "    try:\n",
    "        schema_fields=param['fieldnames']\n",
    "        schema_fields.remove(param['params']['label_field_name'])\n",
    "    except:\n",
    "        schema_fields=[]\n",
    "\n",
    "    print(schema_fields)\n",
    "        \n",
    "    print(\"start connecting to Milvus\")\n",
    "    try:\n",
    "        # this hostname may need changing to a specific local docker network ip address depending on docker configuration\n",
    "        connections.connect(\"default\", host=\"milvus-standalone\", port=\"19530\")\n",
    "        collection_exists = utility.has_collection(collection_name)\n",
    "        \n",
    "        # Basic schema setting\n",
    "        fields = [\n",
    "            FieldSchema(name=\"_key\", is_primary=True, auto_id=True, dtype=DataType.INT64),\n",
    "            FieldSchema(name=\"embeddings\", dtype=embedding_type, dim=n_dims),\n",
    "            FieldSchema(name=\"label\", dtype=DataType.VARCHAR, max_length=15000),\n",
    "        ]\n",
    "        # Additional schema setting\n",
    "        if len(schema_fields) != 0: \n",
    "            for i in range(len(schema_fields)):\n",
    "                fields.append(FieldSchema(name=schema_fields[i], dtype=DataType.VARCHAR, max_length=1000))\n",
    "        # Create schema\n",
    "        \n",
    "        schema = CollectionSchema(fields, f\"dsdl schema for {collection_name}\")\n",
    "        print(fields)\n",
    "        \n",
    "        if collection_exists:\n",
    "            print(f\"The collection {collection_name} already exists\")\n",
    "            collection = Collection(collection_name)\n",
    "            collection.load()\n",
    "        else:\n",
    "            print(f\"The collection {collection_name} does not exist\")\n",
    "            print(f\"creating new collection: {collection_name}\")\n",
    "            collection = Collection(collection_name, schema, consistency_level=\"Strong\")\n",
    "            index = {\n",
    "                \"index_type\": \"IVF_FLAT\",\n",
    "                \"metric_type\": \"L2\",\n",
    "                \"params\": {\"nlist\": 1024},\n",
    "            }\n",
    "            collection.create_index(\"embeddings\", index)\n",
    "    except:\n",
    "        collection = None\n",
    "    \n",
    "    model['collection']=collection\n",
    "    model['collection_name']=collection_name\n",
    "\n",
    "    if model['collection'] is not None:\n",
    "        use_local= int(param['params']['use_local'])\n",
    "        try:\n",
    "            embedder_name = param['params']['embedder_name'].strip('\\\"')\n",
    "        except:\n",
    "            embedder_name = 'all-MiniLM-L6-v2'\n",
    "        if use_local:\n",
    "            embedder_name = f'/srv/app/model/data/{embedder_name}'\n",
    "            print(\"Using local embedding model checkpoints\")  \n",
    "        transformer_embedder = HuggingFaceEmbedding(model_name=embedder_name)\n",
    "\n",
    "        try:\n",
    "            label_field_name=param['params']['label_field_name']\n",
    "            print(label_field_name)\n",
    "            texts = []\n",
    "            vectors = []\n",
    "            for i in range(len(df)):\n",
    "                texts.append(df[i][label_field_name])\n",
    "                vectors.append(transformer_embedder.get_text_embedding(df[i][label_field_name]))\n",
    "            data=[vectors, texts]\n",
    "        except:\n",
    "            data = None\n",
    "            m = {\"Message\": \"Failed. Could not vectorize dataframe. Check your field name.\"}\n",
    "            print(m)\n",
    "            \n",
    "        if data is not None:\n",
    "            if len(schema_fields) != 0:\n",
    "                for field in schema_fields:  \n",
    "                    l = []\n",
    "                    for i in range(len(df)):\n",
    "                        l.append(df[i][field])\n",
    "                    data.append(l)\n",
    "            data_limit = 16000000\n",
    "            print(f\"Size of data is {len(data[0])}\")\n",
    "            num_vectors = int(data_limit / (n_dims * 4))\n",
    "            print(f\"Batch size is {num_vectors}\")\n",
    "            if len(data[0]) > num_vectors:\n",
    "                num_sublists = len(data[0]) // num_vectors\n",
    "                if len(data[0]) % num_vectors != 0:\n",
    "                    num_sublists += 1\n",
    "                print(f\"Number of batches is {num_sublists}\")\n",
    "                # Initialize the sublists\n",
    "                sublists = [[] for _ in range(num_sublists)]\n",
    "                # Iterate over each row in the data\n",
    "                for row in data:\n",
    "                    for i in range(num_sublists-1):\n",
    "                        sublists[i].append(row[i * num_vectors:(i + 1) * num_vectors])\n",
    "                    sublists[num_sublists-1].append(row[(num_sublists-1) * num_vectors:])\n",
    "            else:\n",
    "                sublists = [data]\n",
    "            try:\n",
    "                for sub_data in sublists:\n",
    "                    model['collection'].insert(sub_data, timeout=None)\n",
    "                    print(f\"Inserted data batch with length {len(sub_data[0])}\")\n",
    "                m = {\"Message\": \"Success\"}\n",
    "                print(m)\n",
    "            except:\n",
    "                m = {\"Message\": \"Failed. Too much data to insert at once.\"}\n",
    "                print(m)\n",
    "    else:\n",
    "        m = {\"Message\": \"Failed. Could not create collection. Check collection naming.\"}\n",
    "        print(m)\n",
    "    cols =[]\n",
    "    for _ in range(len(df)):\n",
    "        cols.append(m)\n",
    "    return cols"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After implementing your fit, apply, save and load you can train your model:<br>\n",
    "| makeresults count=10<br>\n",
    "| streamstats c as i<br>\n",
    "| eval s = i%3<br>\n",
    "| eval feature_{s}=0<br>\n",
    "| foreach feature_* [eval &lt;&lt;FIELD&gt;&gt;=random()/pow(2,31)]<br>\n",
    "| fit MLTKContainer algo=barebone s from feature_* into app:barebone_model<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or apply your model:<br>\n",
    "| makeresults count=10<br>\n",
    "| streamstats c as i<br>\n",
    "| eval s = i%3<br>\n",
    "| eval feature_{s}=0<br>\n",
    "| foreach feature_* [eval &lt;&lt;FIELD&gt;&gt;=random()/pow(2,31)]<br>\n",
    "| apply barebone_model as the_meaning_of_life"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Send data back to Splunk HEC\n",
    "When you configured the Splunk HEC Settings in the DSDL app you can easily send back data to an index with [Splunk's HTTP Event Collector (HEC)](https://docs.splunk.com/Documentation/Splunk/latest/Data/UsetheHTTPEventCollector). Read more about data formats and options in the [documentation](https://docs.splunk.com/Documentation/Splunk/latest/Data/FormateventsforHTTPEventCollector#Event_metadata).\n",
    "\n",
    "### Use cases\n",
    "- you want to offload longer running, possibly distributed computations that need to deliver results asynchroneously back into Splunk. \n",
    "- you might not want to present results back into the search pipeline after your `| fit` or `| apply` command. \n",
    "- you can easily utilize this approach for any logging purposes or other profiling tasks in your ML code so you can actively monitor and analyze your processes.\n",
    "\n",
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dsdlsupport import SplunkHEC as SplunkHEC\n",
    "hec = SplunkHEC.SplunkHEC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example to send 10 hello world events\n",
    "response = hec.send_hello_world(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HEC endpoint http://host.docker.internal:8088/services/collector/event \n",
      "returned with status code 200 \n",
      "and response message: {\"text\":\"Success\",\"code\":0}\n"
     ]
    }
   ],
   "source": [
    "print(\"HEC endpoint %s \\nreturned with status code %s \\nand response message: %s\" % (response.url, response.status_code, response.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example to send a JSON object, e.g. to log some data\n",
    "from datetime import datetime\n",
    "response = hec.send({'event': {'message': 'operation done', 'log_level': 'INFO' }, 'time': datetime.now().timestamp()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HEC endpoint http://host.docker.internal:8088/services/collector/event \n",
      "returned with status code 200 \n",
      "and response message: {\"text\":\"Success\",\"code\":0}\n"
     ]
    }
   ],
   "source": [
    "print(\"HEC endpoint %s \\nreturned with status code %s \\nand response message: %s\" % (response.url, response.status_code, response.text))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of Stages\n",
    "All subsequent cells are not tagged and can be used for further freeform code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
